Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 57.040, Loss 0.655
 * , robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 77.850, time 0.96
Epoch:1
LR: 0.001
 * Train Acc 77.130, Loss 0.474
 * , robust loss: 0.143 robust error: 0.03000000
 *  Val Acc 82.800, time 0.96
Epoch:2
LR: 0.001
 * Train Acc 82.280, Loss 0.379
 * , robust loss: 0.164 robust error: 0.02000000
 *  Val Acc 86.350, time 0.97
Epoch:3
LR: 0.001
 * Train Acc 84.520, Loss 0.337
 * , robust loss: 0.181 robust error: 0.01000000
 *  Val Acc 86.350, time 0.95
Epoch:4
LR: 0.001
 * Train Acc 86.360, Loss 0.301
 * , robust loss: 0.016 robust error: 0.00000000
 *  Val Acc 86.650, time 0.96
Epoch:5
LR: 0.001
 * Train Acc 87.060, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.300, time 0.99
Epoch:6
LR: 0.001
 * Train Acc 87.690, Loss 0.290
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.400, time 0.96
Epoch:7
LR: 0.001
 * Train Acc 88.720, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.500, time 0.94
Epoch:8
LR: 0.001
 * Train Acc 89.860, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.700, time 0.96
Epoch:9
LR: 0.001
 * Train Acc 91.170, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.750, time 0.92
Epoch:10
LR: 0.001
 * Train Acc 91.410, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.850, time 0.96
Epoch:11
LR: 0.001
 * Train Acc 92.450, Loss 0.155
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.700, time 0.93
Epoch:12
LR: 0.001
 * Train Acc 93.590, Loss 0.162
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.600, time 0.98
Epoch:13
LR: 0.001
 * Train Acc 94.420, Loss 0.127
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.900, time 0.99
Epoch:14
LR: 0.001
 * Train Acc 94.310, Loss 0.120
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.000, time 1.01
Epoch:15
LR: 0.001
 * Train Acc 94.720, Loss 0.107
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.050, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 94.950, Loss 0.093
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.900, time 0.99
Epoch:17
LR: 0.001
 * Train Acc 95.120, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.600, time 0.92
Epoch:18
LR: 0.001
 * Train Acc 95.920, Loss 0.089
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.200, time 0.76
Epoch:19
LR: 0.001
 * Train Acc 96.050, Loss 0.073
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.350, time 0.92
Epoch:20
LR: 0.001
 * Train Acc 95.890, Loss 0.207
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 95.950, time 0.83
Epoch:21
LR: 0.001
 * Train Acc 95.750, Loss 1.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.600, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 96.140, Loss 0.103
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.700, time 0.82
Epoch:23
LR: 0.001
 * Train Acc 96.420, Loss 0.057
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.300, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 96.720, Loss 1.636
 * , robust loss: 1.200 robust error: 0.01000000
 *  Val Acc 96.800, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 96.640, Loss 0.047
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.66
Epoch:26
LR: 0.001
 * Train Acc 96.920, Loss 0.044
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.70
Epoch:27
LR: 0.001
 * Train Acc 97.060, Loss 0.132
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.95
Epoch:28
LR: 0.001
 * Train Acc 97.270, Loss 0.038
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.600, time 0.93
Epoch:29
LR: 0.001
 * Train Acc 97.370, Loss 0.035
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.96
Epoch:30
LR: 0.001
 * Train Acc 97.330, Loss 0.258
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 1.02
Epoch:31
LR: 0.001
 * Train Acc 97.390, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.94
Epoch:32
LR: 0.001
 * Train Acc 97.710, Loss 0.032
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 97.590, Loss 0.033
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.050, time 1.03
Epoch:34
LR: 0.001
 * Train Acc 97.570, Loss 0.109
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.150, time 1.08
Epoch:35
LR: 0.001
 * Train Acc 97.700, Loss 2.971
 * , robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 96.850, time 0.96
Epoch:36
LR: 0.001
 * Train Acc 97.490, Loss 0.604
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.350, time 0.96
Epoch:37
LR: 0.001
 * Train Acc 97.800, Loss 0.043
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.91
Epoch:38
LR: 0.001
 * Train Acc 97.980, Loss 0.029
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.92
Epoch:39
LR: 0.001
 * Train Acc 97.810, Loss 0.741
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 1.13
Epoch:40
LR: 0.001
 * Train Acc 98.020, Loss 4.420
 * , robust loss: 0.333 robust error: 0.00000000
 *  Val Acc 97.050, time 0.95
Epoch:41
LR: 0.001
 * Train Acc 97.840, Loss 0.893
 * , robust loss: 0.451 robust error: 0.00000000
 *  Val Acc 97.500, time 0.95
Epoch:42
LR: 0.001
 * Train Acc 97.990, Loss 0.252
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 1.07
Epoch:43
LR: 0.001
 * Train Acc 98.050, Loss 0.060
 * , robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 97.350, time 0.92
Epoch:44
LR: 0.001
 * Train Acc 97.880, Loss 0.141
 * , robust loss: 0.291 robust error: 0.00000000
 *  Val Acc 97.300, time 0.97
Epoch:45
LR: 0.001
 * Train Acc 98.220, Loss 6.927
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.92
Epoch:46
LR: 0.001
 * Train Acc 98.200, Loss 0.133
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.92
Epoch:47
LR: 0.001
 * Train Acc 98.250, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.87
Epoch:48
LR: 0.001
 * Train Acc 98.380, Loss 0.025
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 95.650, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 98.100, Loss 0.038
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.97
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.41745376586914 - mean: 0.015529460273683071 - std: 0.006415003444999456
 * min 0.009080406278371811, max: 0.03267435356974602
sum: 347.60968017578125 - mean: 0.037718065083026886 - std: 0.02004256471991539
 * min 0.01194006111472845, max: 0.1102142408490181
sum: 183.78163146972656 - mean: 0.009970791637897491 - std: 0.004074812401086092
 * min 0.0031818763818591833, max: 0.026295199990272522
sum: 449.27520751953125 - mean: 0.012187370099127293 - std: 0.003515272866934538
 * min 0.0038219494745135307, max: 0.02966941148042679
sum: 2166.0498046875 - mean: 0.02937893010675907 - std: 0.008529565297067165
 * min 0.00926609430462122, max: 0.07673545181751251
sum: 4444.73046875 - mean: 0.030142758041620255 - std: 0.004467906430363655
 * min 0.010054363869130611, max: 0.07098647952079773
sum: 6189.603515625 - mean: 0.04197593405842781 - std: 0.004810810089111328
 * min 0.011887377128005028, max: 0.10652565211057663
sum: 116.1426773071289 - mean: 0.00014177573029883206 - std: 9.122855431087373e-07
 * min 0.00010807707440108061, max: 0.00014956793165765703
sum: 5.0 - mean: 0.009765625 - std: 0.0030543096363544464
 * min 0.002615619683638215, max: 0.05850693956017494
eps: tensor([0.1398, 0.3395, 0.0897, 0.1097, 0.2644, 0.2713, 0.3778, 0.4537, 0.4542],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.500, time 0.89
 * Lower 1 Val Acc 56.800, time 0.92
 * Upper 1 Val Acc 56.800, time 0.92
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 64.440, Loss 0.656
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.750, time 0.92
Epoch:1
LR: 0.001
 * Train Acc 74.810, Loss 0.508
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.750, time 0.63
Epoch:2
LR: 0.001
 * Train Acc 77.400, Loss 0.457
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 79.210, Loss 0.430
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.700, time 0.94
Epoch:4
LR: 0.001
 * Train Acc 79.180, Loss 0.417
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.950, time 0.87
Epoch:5
LR: 0.001
 * Train Acc 80.610, Loss 0.395
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.95
Epoch:6
LR: 0.001
 * Train Acc 81.000, Loss 0.379
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.91
Epoch:7
LR: 0.001
 * Train Acc 81.570, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.90
Epoch:8
LR: 0.001
 * Train Acc 81.480, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.92
Epoch:9
LR: 0.001
 * Train Acc 81.520, Loss 0.348
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.91
Epoch:10
LR: 0.001
 * Train Acc 81.610, Loss 0.342
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.94
Epoch:11
LR: 0.001
 * Train Acc 81.920, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.95
Epoch:12
LR: 0.001
 * Train Acc 81.380, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.92
Epoch:13
LR: 0.001
 * Train Acc 81.860, Loss 0.363
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.91
Epoch:14
LR: 0.001
 * Train Acc 81.390, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.90
Epoch:15
LR: 0.001
 * Train Acc 82.150, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.92
Epoch:16
LR: 0.001
 * Train Acc 82.930, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 82.760, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.91
Epoch:18
LR: 0.001
 * Train Acc 82.090, Loss 0.273
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.92
Epoch:19
LR: 0.001
 * Train Acc 83.020, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.95
Epoch:20
LR: 0.001
 * Train Acc 82.670, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.94
Epoch:21
LR: 0.001
 * Train Acc 82.280, Loss 0.253
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.90
Epoch:22
LR: 0.001
 * Train Acc 82.920, Loss 0.242
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.91
Epoch:23
LR: 0.001
 * Train Acc 82.320, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.300, time 0.77
Epoch:24
LR: 0.001
 * Train Acc 83.000, Loss 0.229
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 82.740, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.78
Epoch:26
LR: 0.001
 * Train Acc 82.610, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 81.890, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.90
Epoch:28
LR: 0.001
 * Train Acc 82.260, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 1.09
Epoch:29
LR: 0.001
 * Train Acc 82.230, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.91
Epoch:30
LR: 0.001
 * Train Acc 82.530, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 1.00
Epoch:31
LR: 0.001
 * Train Acc 82.780, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 1.07
Epoch:32
LR: 0.001
 * Train Acc 82.210, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.89
Epoch:33
LR: 0.001
 * Train Acc 82.200, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.91
Epoch:34
LR: 0.001
 * Train Acc 82.270, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 1.06
Epoch:35
LR: 0.001
 * Train Acc 82.480, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.95
Epoch:36
LR: 0.001
 * Train Acc 82.320, Loss 0.950
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.91
Epoch:37
LR: 0.001
 * Train Acc 82.490, Loss 0.416
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.73
Epoch:38
LR: 0.001
 * Train Acc 82.190, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.90
Epoch:39
LR: 0.001
 * Train Acc 82.040, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 1.06
Epoch:40
LR: 0.001
 * Train Acc 82.280, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.91
Epoch:41
LR: 0.001
 * Train Acc 81.980, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.93
Epoch:42
LR: 0.001
 * Train Acc 82.100, Loss 0.281
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 80.950, time 0.84
Epoch:43
LR: 0.001
 * Train Acc 81.390, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.66
Epoch:44
LR: 0.001
 * Train Acc 81.990, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.92
Epoch:45
LR: 0.001
 * Train Acc 81.860, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.91
Epoch:46
LR: 0.001
 * Train Acc 82.040, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.95
Epoch:47
LR: 0.001
 * Train Acc 81.740, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.93
Epoch:48
LR: 0.001
 * Train Acc 81.180, Loss 4.757
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.92
Epoch:49
LR: 0.001
 * Train Acc 81.660, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.91
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.6392083168029785 - mean: 0.0053694541566073895 - std: 0.002916307421401143
 * min 0.002571869408711791, max: 0.01373080350458622
sum: 117.4234390258789 - mean: 0.01274125836789608 - std: 0.007993385195732117
 * min 0.003251928836107254, max: 0.04548588767647743
sum: 50.545536041259766 - mean: 0.002742270939052105 - std: 0.0012854381930083036
 * min 0.0006802171119488776, max: 0.00856835301965475
sum: 126.22257232666016 - mean: 0.003424006514251232 - std: 0.00111286377068609
 * min 0.0008774942834861577, max: 0.009690061211585999
sum: 707.1245727539062 - mean: 0.009590990841388702 - std: 0.0032204552553594112
 * min 0.002420536708086729, max: 0.029355237260460854
sum: 1577.920654296875 - mean: 0.010700959712266922 - std: 0.001850897097028792
 * min 0.002982830163091421, max: 0.030695026740431786
sum: 2310.35498046875 - mean: 0.015668097883462906 - std: 0.0020896587520837784
 * min 0.0037793717347085476, max: 0.0455784946680069
sum: 45.77954864501953 - mean: 5.588323620031588e-05 - std: 4.002032483185758e-07
 * min 4.1247498302254826e-05, max: 5.921112096984871e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 9.39424498938024e-05
 * min 0.0026311767287552357, max: 0.004827817436307669
eps: tensor([0.0483, 0.1147, 0.0247, 0.0308, 0.0863, 0.0963, 0.1410, 0.1788, 0.1790],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.550, time 0.92
 * Lower 1 Val Acc 81.850, time 0.94
 * Upper 1 Val Acc 81.850, time 0.92
validation split name: 2
 *  Val Acc 79.950, time 0.93
 * Lower 1 Val Acc 72.700, time 0.91
 * Upper 1 Val Acc 72.700, time 0.95
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 76.570, Loss 0.478
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.82
Epoch:1
LR: 0.001
 * Train Acc 81.140, Loss 0.408
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 81.150, Loss 0.393
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.93
Epoch:3
LR: 0.001
 * Train Acc 82.490, Loss 0.377
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.89
Epoch:4
LR: 0.001
 * Train Acc 82.400, Loss 0.366
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.90
Epoch:5
LR: 0.001
 * Train Acc 82.640, Loss 0.365
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.95
Epoch:6
LR: 0.001
 * Train Acc 82.610, Loss 0.353
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.90
Epoch:7
LR: 0.001
 * Train Acc 82.790, Loss 0.346
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.92
Epoch:8
LR: 0.001
 * Train Acc 82.590, Loss 0.343
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.300, time 0.91
Epoch:9
LR: 0.001
 * Train Acc 82.370, Loss 0.336
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.95
Epoch:10
LR: 0.001
 * Train Acc 82.300, Loss 0.325
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.90
Epoch:11
LR: 0.001
 * Train Acc 82.510, Loss 0.319
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.87
Epoch:12
LR: 0.001
 * Train Acc 82.490, Loss 0.313
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.91
Epoch:13
LR: 0.001
 * Train Acc 83.060, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.93
Epoch:14
LR: 0.001
 * Train Acc 82.630, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.92
Epoch:15
LR: 0.001
 * Train Acc 82.340, Loss 0.294
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.94
Epoch:16
LR: 0.001
 * Train Acc 82.730, Loss 0.290
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.90
Epoch:17
LR: 0.001
 * Train Acc 82.830, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.750, time 0.89
Epoch:18
LR: 0.001
 * Train Acc 82.440, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.800, time 0.91
Epoch:19
LR: 0.001
 * Train Acc 82.830, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.75
Epoch:20
LR: 0.001
 * Train Acc 82.830, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.89
Epoch:21
LR: 0.001
 * Train Acc 82.100, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.83
Epoch:22
LR: 0.001
 * Train Acc 82.540, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.65
Epoch:23
LR: 0.001
 * Train Acc 82.590, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.65
Epoch:24
LR: 0.001
 * Train Acc 81.840, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 1.10
Epoch:25
LR: 0.001
 * Train Acc 82.110, Loss 0.234
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.92
Epoch:26
LR: 0.001
 * Train Acc 82.740, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.93
Epoch:27
LR: 0.001
 * Train Acc 82.540, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 1.05
Epoch:28
LR: 0.001
 * Train Acc 82.640, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.92
Epoch:29
LR: 0.001
 * Train Acc 82.380, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.92
Epoch:30
LR: 0.001
 * Train Acc 82.470, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 1.11
Epoch:31
LR: 0.001
 * Train Acc 82.010, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.90
Epoch:32
LR: 0.001
 * Train Acc 81.670, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.92
Epoch:33
LR: 0.001
 * Train Acc 82.030, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.200, time 1.07
Epoch:34
LR: 0.001
 * Train Acc 81.970, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.90
Epoch:35
LR: 0.001
 * Train Acc 81.260, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.250, time 0.90
Epoch:36
LR: 0.001
 * Train Acc 81.800, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 1.08
Epoch:37
LR: 0.001
 * Train Acc 82.240, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.89
Epoch:38
LR: 0.001
 * Train Acc 81.710, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.000, time 1.03
Epoch:39
LR: 0.001
 * Train Acc 81.460, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.89
Epoch:40
LR: 0.001
 * Train Acc 81.820, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.92
Epoch:41
LR: 0.001
 * Train Acc 81.760, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.94
Epoch:42
LR: 0.001
 * Train Acc 81.440, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.93
Epoch:43
LR: 0.001
 * Train Acc 81.320, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.96
Epoch:44
LR: 0.001
 * Train Acc 81.350, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.91
Epoch:45
LR: 0.001
 * Train Acc 81.220, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.97
Epoch:46
LR: 0.001
 * Train Acc 81.490, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.500, time 0.91
Epoch:47
LR: 0.001
 * Train Acc 81.130, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.94
Epoch:48
LR: 0.001
 * Train Acc 81.130, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.450, time 0.90
Epoch:49
LR: 0.001
 * Train Acc 80.910, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.600, time 0.93
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.5179970264434814 - mean: 0.0017569409683346748 - std: 0.0009621140779927373
 * min 0.0008406243869103491, max: 0.004521129187196493
sum: 39.866737365722656 - mean: 0.004325817804783583 - std: 0.0027330939192324877
 * min 0.0010940044885501266, max: 0.015543547458946705
sum: 16.744394302368164 - mean: 0.0009084415505640209 - std: 0.00042802459211088717
 * min 0.00022360208095051348, max: 0.0028626136481761932
sum: 41.8803596496582 - mean: 0.001136077451519668 - std: 0.00037044298369437456
 * min 0.00029115384677425027, max: 0.003223191015422344
sum: 234.16554260253906 - mean: 0.003176073543727398 - std: 0.0010678587714210153
 * min 0.0008006631396710873, max: 0.009744147770106792
sum: 525.1502685546875 - mean: 0.0035614031367003918 - std: 0.0006162762292660773
 * min 0.000992477172985673, max: 0.010218928568065166
sum: 769.6239013671875 - mean: 0.0052193463779985905 - std: 0.0006963019259274006
 * min 0.001258903299458325, max: 0.015187331475317478
sum: 15.253997802734375 - mean: 1.8620601622387767e-05 - std: 1.333557690941234e-07
 * min 1.3743864656134974e-05, max: 1.9729512132471427e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 2.761075620583142e-06
 * min 0.0011444156989455223, max: 0.0012000025017187
eps: tensor([0.0158, 0.0389, 0.0082, 0.0102, 0.0286, 0.0321, 0.0470, 0.0596, 0.0597],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.850, time 0.95
 * Lower 1 Val Acc 86.550, time 0.92
 * Upper 1 Val Acc 86.550, time 0.91
validation split name: 2
 *  Val Acc 75.450, time 0.93
 * Lower 1 Val Acc 74.950, time 0.94
 * Upper 1 Val Acc 74.950, time 0.66
validation split name: 3
 *  Val Acc 77.600, time 0.95
 * Lower 1 Val Acc 77.050, time 0.92
 * Upper 1 Val Acc 77.050, time 0.93
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.160, Loss 0.581
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.750, time 0.88
Epoch:1
LR: 0.001
 * Train Acc 72.690, Loss 0.525
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.700, time 0.91
Epoch:2
LR: 0.001
 * Train Acc 72.110, Loss 0.517
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.650, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 71.710, Loss 0.514
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.100, time 0.69
Epoch:4
LR: 0.001
 * Train Acc 72.080, Loss 0.502
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.200, time 0.92
Epoch:5
LR: 0.001
 * Train Acc 72.040, Loss 0.493
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.050, time 0.94
Epoch:6
LR: 0.001
 * Train Acc 72.270, Loss 0.484
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.600, time 0.92
Epoch:7
LR: 0.001
 * Train Acc 70.870, Loss 0.486
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.600, time 0.93
Epoch:8
LR: 0.001
 * Train Acc 71.140, Loss 0.474
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.700, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 71.740, Loss 0.462
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.800, time 0.94
Epoch:10
LR: 0.001
 * Train Acc 72.060, Loss 0.451
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.550, time 0.94
Epoch:11
LR: 0.001
 * Train Acc 71.060, Loss 0.449
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.550, time 0.92
Epoch:12
LR: 0.001
 * Train Acc 71.610, Loss 0.437
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.800, time 0.95
Epoch:13
LR: 0.001
 * Train Acc 70.900, Loss 0.432
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.550, time 0.92
Epoch:14
LR: 0.001
 * Train Acc 71.140, Loss 0.425
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.400, time 0.93
Epoch:15
LR: 0.001
 * Train Acc 70.830, Loss 0.416
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.100, time 0.93
Epoch:16
LR: 0.001
 * Train Acc 70.740, Loss 0.408
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.250, time 0.91
Epoch:17
LR: 0.001
 * Train Acc 70.280, Loss 0.402
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.250, time 0.93
Epoch:18
LR: 0.001
 * Train Acc 70.090, Loss 0.395
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.750, time 0.81
Epoch:19
LR: 0.001
 * Train Acc 70.130, Loss 0.386
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.700, time 0.71
Epoch:20
LR: 0.001
 * Train Acc 69.920, Loss 0.376
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.350, time 0.80
Epoch:21
LR: 0.001
 * Train Acc 69.230, Loss 0.368
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.300, time 1.07
Epoch:22
LR: 0.001
 * Train Acc 69.650, Loss 0.361
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.500, time 0.91
Epoch:23
LR: 0.001
 * Train Acc 68.950, Loss 0.354
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.550, time 0.95
Epoch:24
LR: 0.001
 * Train Acc 69.350, Loss 0.344
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.550, time 1.08
Epoch:25
LR: 0.001
 * Train Acc 69.580, Loss 0.334
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.600, time 0.92
Epoch:26
LR: 0.001
 * Train Acc 69.370, Loss 0.327
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 67.250, time 0.97
Epoch:27
LR: 0.001
 * Train Acc 69.000, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.500, time 1.05
Epoch:28
LR: 0.001
 * Train Acc 67.670, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.450, time 0.91
Epoch:29
LR: 0.001
 * Train Acc 67.670, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.050, time 0.91
Epoch:30
LR: 0.001
 * Train Acc 67.580, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.800, time 1.09
Epoch:31
LR: 0.001
 * Train Acc 67.580, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.600, time 0.91
Epoch:32
LR: 0.001
 * Train Acc 67.480, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.350, time 0.76
Epoch:33
LR: 0.001
 * Train Acc 67.570, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.800, time 1.08
Epoch:34
LR: 0.001
 * Train Acc 67.880, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.300, time 0.93
Epoch:35
LR: 0.001
 * Train Acc 67.830, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.050, time 0.82
Epoch:36
LR: 0.001
 * Train Acc 68.320, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.950, time 0.95
Epoch:37
LR: 0.001
 * Train Acc 68.670, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.550, time 0.97
Epoch:38
LR: 0.001
 * Train Acc 68.740, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.500, time 0.91
Epoch:39
LR: 0.001
 * Train Acc 68.600, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.300, time 0.81
Epoch:40
LR: 0.001
 * Train Acc 67.030, Loss 0.303
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 60.150, time 0.94
Epoch:41
LR: 0.001
 * Train Acc 67.040, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 61.400, time 0.89
Epoch:42
LR: 0.001
 * Train Acc 67.300, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 59.300, time 0.91
Epoch:43
LR: 0.001
 * Train Acc 66.760, Loss 0.305
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 58.950, time 0.79
Epoch:44
LR: 0.001
 * Train Acc 66.170, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 61.200, time 0.80
Epoch:45
LR: 0.001
 * Train Acc 66.230, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 59.150, time 0.92
Epoch:46
LR: 0.001
 * Train Acc 65.950, Loss 0.308
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 61.000, time 0.90
Epoch:47
LR: 0.001
 * Train Acc 65.790, Loss 0.308
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.000, time 0.91
Epoch:48
LR: 0.001
 * Train Acc 66.700, Loss 0.305
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 60.400, time 0.90
Epoch:49
LR: 0.001
 * Train Acc 65.810, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 59.200, time 0.94
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 0.9990355968475342 - mean: 0.001156291225925088 - std: 0.0006728593725711107
 * min 0.0005233430420048535, max: 0.0031083289068192244
sum: 27.031097412109375 - mean: 0.0029330619145184755 - std: 0.0019318133126944304
 * min 0.0006932261749170721, max: 0.011083224788308144
sum: 10.679564476013184 - mean: 0.000579403480514884 - std: 0.00027911632787436247
 * min 0.0001345251512248069, max: 0.001874470035545528
sum: 26.714780807495117 - mean: 0.0007246847962960601 - std: 0.00023886376584414393
 * min 0.00018570043903309852, max: 0.0021411417983472347
sum: 145.75369262695312 - mean: 0.0019769109785556793 - std: 0.0006704987608827651
 * min 0.000497479981277138, max: 0.006221375428140163
sum: 349.06048583984375 - mean: 0.0023672180250287056 - std: 0.0004131023015361279
 * min 0.0006459715659730136, max: 0.006901666056364775
sum: 516.1044921875 - mean: 0.003500057617202401 - std: 0.00047227166942320764
 * min 0.0008433610782958567, max: 0.010343754664063454
sum: 10.343920707702637 - mean: 1.2626856005226728e-05 - std: 9.046120652556056e-08
 * min 9.319837772636674e-06, max: 1.337882895313669e-05
sum: 0.3999999761581421 - mean: 0.0007812499534338713 - std: 7.2207817538583186e-06
 * min 0.00070890539791435, max: 0.0008620619773864746
eps: tensor([0.0104, 0.0264, 0.0052, 0.0065, 0.0178, 0.0213, 0.0315, 0.0404, 0.0405],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 80.800, time 0.94
 * Lower 1 Val Acc 80.850, time 0.92
 * Upper 1 Val Acc 80.850, time 0.96
validation split name: 2
 *  Val Acc 71.250, time 0.92
 * Lower 1 Val Acc 70.850, time 0.93
 * Upper 1 Val Acc 70.850, time 0.92
validation split name: 3
 *  Val Acc 78.000, time 0.97
 * Lower 1 Val Acc 76.050, time 0.91
 * Upper 1 Val Acc 76.050, time 0.90
validation split name: 4
 *  Val Acc 59.200, time 0.93
 * Lower 1 Val Acc 61.750, time 0.95
 * Upper 1 Val Acc 61.750, time 0.98
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.400, Loss 0.443
 * , robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 83.600, time 0.96
Epoch:1
LR: 0.001
 * Train Acc 81.920, Loss 0.398
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.91
Epoch:2
LR: 0.001
 * Train Acc 81.900, Loss 0.390
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.90
Epoch:3
LR: 0.001
 * Train Acc 81.790, Loss 0.388
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.92
Epoch:4
LR: 0.001
 * Train Acc 81.790, Loss 0.379
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.93
Epoch:5
LR: 0.001
 * Train Acc 81.270, Loss 0.376
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.93
Epoch:6
LR: 0.001
 * Train Acc 81.110, Loss 0.370
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.94
Epoch:7
LR: 0.001
 * Train Acc 81.410, Loss 0.359
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.93
Epoch:8
LR: 0.001
 * Train Acc 81.160, Loss 0.357
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.92
Epoch:9
LR: 0.001
 * Train Acc 81.580, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.92
Epoch:10
LR: 0.001
 * Train Acc 81.610, Loss 0.338
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.73
Epoch:11
LR: 0.001
 * Train Acc 81.940, Loss 0.331
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.95
Epoch:12
LR: 0.001
 * Train Acc 81.090, Loss 0.329
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.92
Epoch:13
LR: 0.001
 * Train Acc 81.660, Loss 0.322
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.92
Epoch:14
LR: 0.001
 * Train Acc 81.040, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.80
Epoch:15
LR: 0.001
 * Train Acc 81.300, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 81.130, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.83
Epoch:17
LR: 0.001
 * Train Acc 81.030, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 81.000, Loss 0.291
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.66
Epoch:19
LR: 0.001
 * Train Acc 80.810, Loss 0.284
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.65
Epoch:20
LR: 0.001
 * Train Acc 81.030, Loss 0.276
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 81.370, Loss 0.271
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 81.050, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.94
Epoch:23
LR: 0.001
 * Train Acc 80.580, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.96
Epoch:24
LR: 0.001
 * Train Acc 80.690, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.96
Epoch:25
LR: 0.001
 * Train Acc 80.820, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.95
Epoch:26
LR: 0.001
 * Train Acc 80.690, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.96
Epoch:27
LR: 0.001
 * Train Acc 80.710, Loss 0.229
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 1.10
Epoch:28
LR: 0.001
 * Train Acc 80.890, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.93
Epoch:29
LR: 0.001
 * Train Acc 80.030, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 1.00
Epoch:30
LR: 0.001
 * Train Acc 80.060, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 1.12
Epoch:31
LR: 0.001
 * Train Acc 80.620, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 1.01
Epoch:32
LR: 0.001
 * Train Acc 80.610, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.95
Epoch:33
LR: 0.001
 * Train Acc 80.050, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 1.09
Epoch:34
LR: 0.001
 * Train Acc 80.290, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.77
Epoch:35
LR: 0.001
 * Train Acc 80.620, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.93
Epoch:36
LR: 0.001
 * Train Acc 80.040, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.96
Epoch:37
LR: 0.001
 * Train Acc 79.950, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.95
Epoch:38
LR: 0.001
 * Train Acc 79.820, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.88
Epoch:39
LR: 0.001
 * Train Acc 79.620, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.98
Epoch:40
LR: 0.001
 * Train Acc 79.400, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.72
Epoch:41
LR: 0.001
 * Train Acc 79.370, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.91
Epoch:42
LR: 0.001
 * Train Acc 79.650, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.91
Epoch:43
LR: 0.001
 * Train Acc 79.110, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.90
Epoch:44
LR: 0.001
 * Train Acc 79.940, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.91
Epoch:45
LR: 0.001
 * Train Acc 79.460, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.90
Epoch:46
LR: 0.001
 * Train Acc 79.140, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.90
Epoch:47
LR: 0.001
 * Train Acc 79.120, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.93
Epoch:48
LR: 0.001
 * Train Acc 79.130, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.90
Epoch:49
LR: 0.001
 * Train Acc 78.960, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.91
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5379301309585571 - mean: 0.0006226043333299458 - std: 0.0004473355656955391
 * min 0.00023826387769076973, max: 0.001955572748556733
sum: 13.322600364685059 - mean: 0.00144559470936656 - std: 0.0011145812459290028
 * min 0.00025488209212198853, max: 0.006379320286214352
sum: 4.523504257202148 - mean: 0.0002454158093314618 - std: 0.00012869035708718002
 * min 4.7599522076779976e-05, max: 0.0008501425036229193
sum: 10.951044082641602 - mean: 0.0002970660862047225 - std: 0.0001016257083392702
 * min 6.714822666253895e-05, max: 0.0008956809178926051
sum: 62.5965461730957 - mean: 0.000849020027089864 - std: 0.0002947074535768479
 * min 0.0001958159846253693, max: 0.0028456593863666058
sum: 155.0054168701172 - mean: 0.0010511977598071098 - std: 0.00018743390683084726
 * min 0.0002780536306090653, max: 0.0032850310672074556
sum: 264.9721374511719 - mean: 0.0017969573382288218 - std: 0.0002495769294910133
 * min 0.00041415193118155, max: 0.00557802664116025
sum: 5.529756546020508 - mean: 6.75019100526697e-06 - std: 4.845946932618972e-08
 * min 4.982292466593208e-06, max: 7.154339527914999e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 7.4007889452332165e-06
 * min 0.00033247919054701924, max: 0.0004615242942236364
eps: tensor([0.0056, 0.0130, 0.0022, 0.0027, 0.0076, 0.0095, 0.0162, 0.0216, 0.0216],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.150, time 0.91
 * Lower 1 Val Acc 89.500, time 0.91
 * Upper 1 Val Acc 89.500, time 0.90
validation split name: 2
 *  Val Acc 75.850, time 0.94
 * Lower 1 Val Acc 75.650, time 0.89
 * Upper 1 Val Acc 75.650, time 0.94
validation split name: 3
 *  Val Acc 74.300, time 0.71
 * Lower 1 Val Acc 73.700, time 0.67
 * Upper 1 Val Acc 73.700, time 0.76
validation split name: 4
 *  Val Acc 55.550, time 0.91
 * Lower 1 Val Acc 55.550, time 0.93
 * Upper 1 Val Acc 55.550, time 0.90
validation split name: 5
 *  Val Acc 80.750, time 0.93
 * Lower 1 Val Acc 80.600, time 0.93
 * Upper 1 Val Acc 80.600, time 0.93
Task 1 average acc: 97.5
Task 2 average acc: 84.25
Task 3 average acc: 79.96666666666667
Task 4 average acc: 72.3125
Task 5 average acc: 75.32000000000001
===Summary of experiment repeats: 1 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 7.532000000000001 std: 22.596000000000004
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.540, Loss 0.570
 * , robust loss: 0.377 robust error: 0.01000000
 *  Val Acc 82.050, time 0.91
Epoch:1
LR: 0.001
 * Train Acc 80.600, Loss 0.412
 * , robust loss: 0.026 robust error: 0.01000000
 *  Val Acc 84.050, time 0.90
Epoch:2
LR: 0.001
 * Train Acc 84.150, Loss 0.360
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.550, time 0.97
Epoch:3
LR: 0.001
 * Train Acc 87.270, Loss 0.292
 * , robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 87.100, time 0.90
Epoch:4
LR: 0.001
 * Train Acc 88.100, Loss 0.268
 * , robust loss: 0.130 robust error: 0.02000000
 *  Val Acc 90.850, time 0.91
Epoch:5
LR: 0.001
 * Train Acc 89.810, Loss 0.229
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.900, time 0.92
Epoch:6
LR: 0.001
 * Train Acc 90.480, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.850, time 0.93
Epoch:7
LR: 0.001
 * Train Acc 91.300, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.700, time 0.92
Epoch:8
LR: 0.001
 * Train Acc 92.230, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.000, time 0.91
Epoch:9
LR: 0.001
 * Train Acc 93.280, Loss 0.145
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.550, time 0.91
Epoch:10
LR: 0.001
 * Train Acc 93.710, Loss 0.136
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.450, time 0.91
Epoch:11
LR: 0.001
 * Train Acc 94.080, Loss 0.130
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.900, time 0.93
Epoch:12
LR: 0.001
 * Train Acc 95.050, Loss 0.112
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.94
Epoch:13
LR: 0.001
 * Train Acc 94.910, Loss 0.099
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.90
Epoch:14
LR: 0.001
 * Train Acc 95.330, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.900, time 0.88
Epoch:15
LR: 0.001
 * Train Acc 95.910, Loss 0.107
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.400, time 0.98
Epoch:16
LR: 0.001
 * Train Acc 96.170, Loss 0.074
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.82
Epoch:17
LR: 0.001
 * Train Acc 96.440, Loss 0.066
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 96.630, Loss 0.524
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.64
Epoch:19
LR: 0.001
 * Train Acc 96.900, Loss 0.058
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 1.08
Epoch:20
LR: 0.001
 * Train Acc 96.970, Loss 0.052
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.92
Epoch:21
LR: 0.001
 * Train Acc 96.990, Loss 3.774
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.94
Epoch:22
LR: 0.001
 * Train Acc 96.930, Loss 0.054
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.550, time 0.91
Epoch:23
LR: 0.001
 * Train Acc 96.830, Loss 0.047
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.91
Epoch:24
LR: 0.001
 * Train Acc 97.220, Loss 0.042
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 1.09
Epoch:25
LR: 0.001
 * Train Acc 96.960, Loss 0.043
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.750, time 0.93
Epoch:26
LR: 0.001
 * Train Acc 97.520, Loss 1.076
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 97.690, Loss 0.034
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.93
Epoch:28
LR: 0.001
 * Train Acc 97.670, Loss 0.043
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.93
Epoch:29
LR: 0.001
 * Train Acc 97.900, Loss 0.041
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 97.880, Loss 0.029
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.94
Epoch:31
LR: 0.001
 * Train Acc 98.140, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 1.10
Epoch:32
LR: 0.001
 * Train Acc 97.740, Loss 1.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 97.650, Loss 579.685
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 1.05
Epoch:34
LR: 0.001
 * Train Acc 97.720, Loss 0.030
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.89
Epoch:35
LR: 0.001
 * Train Acc 97.490, Loss 0.031
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.95
Epoch:36
LR: 0.001
 * Train Acc 97.870, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.83
Epoch:37
LR: 0.001
 * Train Acc 97.920, Loss 0.029
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.91
Epoch:38
LR: 0.001
 * Train Acc 98.040, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.92
Epoch:39
LR: 0.001
 * Train Acc 97.970, Loss 0.055
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.94
Epoch:40
LR: 0.001
 * Train Acc 97.900, Loss 0.515
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.94
Epoch:41
LR: 0.001
 * Train Acc 98.070, Loss 0.365
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.93
Epoch:42
LR: 0.001
 * Train Acc 98.170, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.92
Epoch:43
LR: 0.001
 * Train Acc 98.040, Loss 0.025
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.92
Epoch:44
LR: 0.001
 * Train Acc 98.420, Loss 0.036
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.94
Epoch:45
LR: 0.001
 * Train Acc 98.010, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.93
Epoch:46
LR: 0.001
 * Train Acc 98.560, Loss 0.020
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.91
Epoch:47
LR: 0.001
 * Train Acc 98.230, Loss 0.314
 * , robust loss: 57.748 robust error: 0.01000000
 *  Val Acc 97.800, time 0.94
Epoch:48
LR: 0.001
 * Train Acc 98.510, Loss 4.404
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.650, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 98.500, Loss 2.875
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.92
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.423025131225586 - mean: 0.015535908751189709 - std: 0.0044372365809977055
 * min 0.009891379624605179, max: 0.027925867587327957
sum: 302.0267639160156 - mean: 0.0327720008790493 - std: 0.014182714745402336
 * min 0.011948204599320889, max: 0.07786396890878677
sum: 188.08811950683594 - mean: 0.010204433463513851 - std: 0.0034066520165652037
 * min 0.003509148256853223, max: 0.02319212444126606
sum: 500.532958984375 - mean: 0.013577825389802456 - std: 0.003598392941057682
 * min 0.004652114585042, max: 0.03365165367722511
sum: 2191.9248046875 - mean: 0.02972988225519657 - std: 0.008436820469796658
 * min 0.008914369158446789, max: 0.07567004859447479
sum: 4903.2275390625 - mean: 0.03325213864445686 - std: 0.006309726275503635
 * min 0.011854407377541065, max: 0.08065582811832428
sum: 6459.2490234375 - mean: 0.0438045859336853 - std: 0.007161916233599186
 * min 0.014650427736341953, max: 0.10256291180849075
sum: 113.91865539550781 - mean: 0.0001390608522342518 - std: 8.862397180564585e-07
 * min 0.00011397214257158339, max: 0.00014425686094909906
sum: 5.0 - mean: 0.009765625 - std: 0.004108348395675421
 * min 0.003316895104944706, max: 0.03379560634493828
eps: tensor([0.1398, 0.2949, 0.0918, 0.1222, 0.2676, 0.2993, 0.3942, 0.4450, 0.4451],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.850, time 0.92
 * Lower 1 Val Acc 59.750, time 0.92
 * Upper 1 Val Acc 59.750, time 0.92
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.740, Loss 0.594
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.350, time 0.93
Epoch:1
LR: 0.001
 * Train Acc 78.030, Loss 0.459
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.94
Epoch:2
LR: 0.001
 * Train Acc 80.590, Loss 0.418
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 81.240, Loss 0.396
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.92
Epoch:4
LR: 0.001
 * Train Acc 81.610, Loss 0.383
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.94
Epoch:5
LR: 0.001
 * Train Acc 82.380, Loss 0.361
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.90
Epoch:6
LR: 0.001
 * Train Acc 82.590, Loss 0.353
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.100, time 0.89
Epoch:7
LR: 0.001
 * Train Acc 83.090, Loss 0.343
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.92
Epoch:8
LR: 0.001
 * Train Acc 83.270, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.94
Epoch:9
LR: 0.001
 * Train Acc 83.860, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.92
Epoch:10
LR: 0.001
 * Train Acc 83.960, Loss 0.303
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.94
Epoch:11
LR: 0.001
 * Train Acc 84.280, Loss 0.290
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.90
Epoch:12
LR: 0.001
 * Train Acc 83.790, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.90
Epoch:13
LR: 0.001
 * Train Acc 83.630, Loss 0.289
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.92
Epoch:14
LR: 0.001
 * Train Acc 84.400, Loss 0.267
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.200, time 0.92
Epoch:15
LR: 0.001
 * Train Acc 84.130, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 84.090, Loss 0.282
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.90
Epoch:17
LR: 0.001
 * Train Acc 83.940, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.80
Epoch:18
LR: 0.001
 * Train Acc 83.890, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.82
Epoch:19
LR: 0.001
 * Train Acc 84.150, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.64
Epoch:20
LR: 0.001
 * Train Acc 84.660, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.93
Epoch:21
LR: 0.001
 * Train Acc 85.200, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 1.09
Epoch:22
LR: 0.001
 * Train Acc 84.470, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.91
Epoch:23
LR: 0.001
 * Train Acc 84.440, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.93
Epoch:24
LR: 0.001
 * Train Acc 84.480, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 1.09
Epoch:25
LR: 0.001
 * Train Acc 84.170, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.92
Epoch:26
LR: 0.001
 * Train Acc 84.780, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.95
Epoch:27
LR: 0.001
 * Train Acc 84.420, Loss 3.428
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 1.08
Epoch:28
LR: 0.001
 * Train Acc 84.580, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.93
Epoch:29
LR: 0.001
 * Train Acc 84.420, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.75
Epoch:30
LR: 0.001
 * Train Acc 84.730, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 1.08
Epoch:31
LR: 0.001
 * Train Acc 85.000, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 84.630, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 1.09
Epoch:33
LR: 0.001
 * Train Acc 84.190, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.84
Epoch:34
LR: 0.001
 * Train Acc 84.060, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.98
Epoch:35
LR: 0.001
 * Train Acc 84.620, Loss 2.005
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.92
Epoch:36
LR: 0.001
 * Train Acc 83.070, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.90
Epoch:37
LR: 0.001
 * Train Acc 84.330, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.97
Epoch:38
LR: 0.001
 * Train Acc 84.110, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.92
Epoch:39
LR: 0.001
 * Train Acc 84.000, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.91
Epoch:40
LR: 0.001
 * Train Acc 83.960, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.95
Epoch:41
LR: 0.001
 * Train Acc 83.170, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.93
Epoch:42
LR: 0.001
 * Train Acc 84.270, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.92
Epoch:43
LR: 0.001
 * Train Acc 83.560, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.90
Epoch:44
LR: 0.001
 * Train Acc 83.870, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.93
Epoch:45
LR: 0.001
 * Train Acc 83.010, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.92
Epoch:46
LR: 0.001
 * Train Acc 84.040, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.93
Epoch:47
LR: 0.001
 * Train Acc 83.940, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.92
Epoch:48
LR: 0.001
 * Train Acc 83.710, Loss 0.185
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.150, time 0.90
Epoch:49
LR: 0.001
 * Train Acc 83.510, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.93
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.68657922744751 - mean: 0.00542428158223629 - std: 0.00208112271502614
 * min 0.00317898066714406, max: 0.011595213785767555
sum: 109.24531555175781 - mean: 0.01185387559235096 - std: 0.006179626099765301
 * min 0.0036216559819877148, max: 0.03257657214999199
sum: 55.25779724121094 - mean: 0.0029979273676872253 - std: 0.0011235272977501154
 * min 0.000900313607417047, max: 0.0078013320453464985
sum: 148.5059814453125 - mean: 0.00402848282828927 - std: 0.0011434857733547688
 * min 0.0012455504620447755, max: 0.011461540125310421
sum: 710.1390380859375 - mean: 0.009631876833736897 - std: 0.002937886631116271
 * min 0.0023794113658368587, max: 0.027350157499313354
sum: 1817.5235595703125 - mean: 0.012325870804488659 - std: 0.0025248771999031305
 * min 0.003952243365347385, max: 0.032633401453495026
sum: 2419.50927734375 - mean: 0.016408348456025124 - std: 0.002915454562753439
 * min 0.004999328404664993, max: 0.03951483964920044
sum: 42.99637985229492 - mean: 5.24858151038643e-05 - std: 3.5600291425907926e-07
 * min 4.25170328526292e-05, max: 5.4525135055882856e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0003186032408848405
 * min 0.002570356708019972, max: 0.004802440293133259
eps: tensor([0.0488, 0.1067, 0.0270, 0.0363, 0.0867, 0.1109, 0.1477, 0.1680, 0.1680],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 92.450, time 0.90
 * Lower 1 Val Acc 83.900, time 0.92
 * Upper 1 Val Acc 83.900, time 0.93
validation split name: 2
 *  Val Acc 83.500, time 0.93
 * Lower 1 Val Acc 68.950, time 0.90
 * Upper 1 Val Acc 68.950, time 0.89
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 73.970, Loss 0.510
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.65
Epoch:1
LR: 0.001
 * Train Acc 82.970, Loss 0.383
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.91
Epoch:2
LR: 0.001
 * Train Acc 84.650, Loss 0.351
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.91
Epoch:3
LR: 0.001
 * Train Acc 84.380, Loss 0.336
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.450, time 0.94
Epoch:4
LR: 0.001
 * Train Acc 84.900, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.700, time 0.95
Epoch:5
LR: 0.001
 * Train Acc 84.910, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.950, time 0.96
Epoch:6
LR: 0.001
 * Train Acc 84.920, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.91
Epoch:7
LR: 0.001
 * Train Acc 85.220, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.300, time 0.97
Epoch:8
LR: 0.001
 * Train Acc 84.890, Loss 0.305
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.91
Epoch:9
LR: 0.001
 * Train Acc 85.000, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.92
Epoch:10
LR: 0.001
 * Train Acc 85.440, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.600, time 0.93
Epoch:11
LR: 0.001
 * Train Acc 85.920, Loss 0.275
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.900, time 0.72
Epoch:12
LR: 0.001
 * Train Acc 85.630, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.500, time 0.80
Epoch:13
LR: 0.001
 * Train Acc 85.280, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.93
Epoch:14
LR: 0.001
 * Train Acc 84.890, Loss 0.264
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.500, time 0.84
Epoch:15
LR: 0.001
 * Train Acc 85.170, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.66
Epoch:16
LR: 0.001
 * Train Acc 85.500, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 85.170, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 1.08
Epoch:18
LR: 0.001
 * Train Acc 85.040, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.79
Epoch:19
LR: 0.001
 * Train Acc 84.960, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.94
Epoch:20
LR: 0.001
 * Train Acc 85.180, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 1.10
Epoch:21
LR: 0.001
 * Train Acc 84.910, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.97
Epoch:22
LR: 0.001
 * Train Acc 84.910, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.850, time 0.92
Epoch:23
LR: 0.001
 * Train Acc 85.420, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 1.06
Epoch:24
LR: 0.001
 * Train Acc 84.930, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.94
Epoch:25
LR: 0.001
 * Train Acc 84.800, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.94
Epoch:26
LR: 0.001
 * Train Acc 84.620, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 1.15
Epoch:27
LR: 0.001
 * Train Acc 85.020, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.950, time 0.94
Epoch:28
LR: 0.001
 * Train Acc 85.230, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.95
Epoch:29
LR: 0.001
 * Train Acc 85.220, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 1.07
Epoch:30
LR: 0.001
 * Train Acc 84.950, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.450, time 0.92
Epoch:31
LR: 0.001
 * Train Acc 84.980, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.650, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 84.670, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 0.75
Epoch:33
LR: 0.001
 * Train Acc 85.360, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.90
Epoch:34
LR: 0.001
 * Train Acc 84.680, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.93
Epoch:35
LR: 0.001
 * Train Acc 85.030, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.93
Epoch:36
LR: 0.001
 * Train Acc 84.390, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.90
Epoch:37
LR: 0.001
 * Train Acc 84.510, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.93
Epoch:38
LR: 0.001
 * Train Acc 84.500, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.96
Epoch:39
LR: 0.001
 * Train Acc 84.470, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.90
Epoch:40
LR: 0.001
 * Train Acc 84.530, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.93
Epoch:41
LR: 0.001
 * Train Acc 84.380, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.92
Epoch:42
LR: 0.001
 * Train Acc 84.490, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.90
Epoch:43
LR: 0.001
 * Train Acc 84.300, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.950, time 0.84
Epoch:44
LR: 0.001
 * Train Acc 83.890, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 84.030, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.94
Epoch:46
LR: 0.001
 * Train Acc 83.880, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.91
Epoch:47
LR: 0.001
 * Train Acc 84.460, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.94
Epoch:48
LR: 0.001
 * Train Acc 84.250, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.92
Epoch:49
LR: 0.001
 * Train Acc 83.570, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.94
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.5439553260803223 - mean: 0.0017869853181764483 - std: 0.0007828271482139826
 * min 0.000937627162784338, max: 0.0040846923366189
sum: 36.40464401245117 - mean: 0.003950156737118959 - std: 0.0022696801461279392
 * min 0.001088288496248424, max: 0.011869406327605247
sum: 16.38793182373047 - mean: 0.0008891022298485041 - std: 0.0003533533017616719
 * min 0.0002479813410900533, max: 0.0024167548399418592
sum: 43.50123596191406 - mean: 0.0011800465872511268 - std: 0.00034643139224499464
 * min 0.0003312002227175981, max: 0.003582753473892808
sum: 206.849853515625 - mean: 0.0028055806178599596 - std: 0.0008711873088032007
 * min 0.0006561367190442979, max: 0.008740321733057499
sum: 608.9555053710938 - mean: 0.004129743669182062 - std: 0.0008567370823584497
 * min 0.0012303459225222468, max: 0.012097124010324478
sum: 837.6948852539062 - mean: 0.005680982023477554 - std: 0.0010340330190956593
 * min 0.0015829458134248853, max: 0.01435571163892746
sum: 14.8709716796875 - mean: 1.8153041310142726e-05 - std: 1.241627671788592e-07
 * min 1.4684646885143593e-05, max: 1.886370409920346e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 7.205236033769324e-05
 * min 0.0009055635891854763, max: 0.0015205490635707974
eps: tensor([0.0161, 0.0356, 0.0080, 0.0106, 0.0253, 0.0372, 0.0511, 0.0581, 0.0581],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.950, time 0.93
 * Lower 1 Val Acc 87.050, time 0.89
 * Upper 1 Val Acc 87.050, time 0.91
validation split name: 2
 *  Val Acc 76.050, time 0.93
 * Lower 1 Val Acc 75.900, time 0.90
 * Upper 1 Val Acc 75.900, time 0.90
validation split name: 3
 *  Val Acc 84.300, time 0.92
 * Lower 1 Val Acc 82.950, time 0.90
 * Upper 1 Val Acc 82.950, time 0.92
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.700, Loss 0.563
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.93
Epoch:1
LR: 0.001
 * Train Acc 76.090, Loss 0.475
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.83
Epoch:2
LR: 0.001
 * Train Acc 76.370, Loss 0.456
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.93
Epoch:3
LR: 0.001
 * Train Acc 76.870, Loss 0.448
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.94
Epoch:4
LR: 0.001
 * Train Acc 77.850, Loss 0.430
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.90
Epoch:5
LR: 0.001
 * Train Acc 79.230, Loss 0.410
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.95
Epoch:6
LR: 0.001
 * Train Acc 79.150, Loss 0.400
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.70
Epoch:7
LR: 0.001
 * Train Acc 78.670, Loss 0.394
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.93
Epoch:8
LR: 0.001
 * Train Acc 79.690, Loss 0.378
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.95
Epoch:9
LR: 0.001
 * Train Acc 80.510, Loss 0.366
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.93
Epoch:10
LR: 0.001
 * Train Acc 80.530, Loss 0.359
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.92
Epoch:11
LR: 0.001
 * Train Acc 80.090, Loss 0.357
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.85
Epoch:12
LR: 0.001
 * Train Acc 79.410, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.82
Epoch:13
LR: 0.001
 * Train Acc 79.970, Loss 0.340
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 79.870, Loss 0.334
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 80.010, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 80.120, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.61
Epoch:17
LR: 0.001
 * Train Acc 79.930, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.66
Epoch:18
LR: 0.001
 * Train Acc 79.010, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 79.370, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 79.520, Loss 0.292
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.64
Epoch:21
LR: 0.001
 * Train Acc 79.360, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.62
Epoch:22
LR: 0.001
 * Train Acc 78.840, Loss 0.287
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.70
Epoch:23
LR: 0.001
 * Train Acc 78.680, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.61
Epoch:24
LR: 0.001
 * Train Acc 79.200, Loss 0.271
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 79.050, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 78.610, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.63
Epoch:27
LR: 0.001
 * Train Acc 78.290, Loss 0.329
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 77.570, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.65
Epoch:29
LR: 0.001
 * Train Acc 78.390, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.64
Epoch:30
LR: 0.001
 * Train Acc 77.310, Loss 0.234
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.62
Epoch:31
LR: 0.001
 * Train Acc 77.990, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 77.900, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.64
Epoch:33
LR: 0.001
 * Train Acc 77.350, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.64
Epoch:34
LR: 0.001
 * Train Acc 77.590, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.63
Epoch:35
LR: 0.001
 * Train Acc 77.190, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.66
Epoch:36
LR: 0.001
 * Train Acc 77.310, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 76.420, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 76.530, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.63
Epoch:39
LR: 0.001
 * Train Acc 76.670, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.63
Epoch:40
LR: 0.001
 * Train Acc 76.320, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.66
Epoch:41
LR: 0.001
 * Train Acc 76.460, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.62
Epoch:42
LR: 0.001
 * Train Acc 76.180, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 75.070, Loss 0.250
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.62
Epoch:44
LR: 0.001
 * Train Acc 75.400, Loss 0.252
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.65
Epoch:45
LR: 0.001
 * Train Acc 75.630, Loss 0.252
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.64
Epoch:46
LR: 0.001
 * Train Acc 74.800, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.66
Epoch:47
LR: 0.001
 * Train Acc 75.180, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.65
Epoch:48
LR: 0.001
 * Train Acc 75.070, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.65
Epoch:49
LR: 0.001
 * Train Acc 74.580, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.64
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0430238246917725 - mean: 0.0012072034878656268 - std: 0.0005620275042019784
 * min 0.0006014155223965645, max: 0.002864214824512601
sum: 24.260133743286133 - mean: 0.0026323930360376835 - std: 0.0015800142427906394
 * min 0.0006887653726153076, max: 0.008257035166025162
sum: 10.507309913635254 - mean: 0.0005700580659322441 - std: 0.00023325192159973085
 * min 0.00015103975601959974, max: 0.0015955646522343159
sum: 27.91564178466797 - mean: 0.0007572602480649948 - std: 0.00022504980734083802
 * min 0.0002032307966146618, max: 0.0023943770211189985
sum: 133.2049560546875 - mean: 0.0018067078199237585 - std: 0.00056468351976946
 * min 0.0004090641450602561, max: 0.005813230760395527
sum: 401.0194396972656 - mean: 0.002719587180763483 - std: 0.0005671263788826764
 * min 0.0008090982446447015, max: 0.008030781522393227
sum: 561.3370361328125 - mean: 0.003806810360401869 - std: 0.0006982687045820057
 * min 0.0010524415411055088, max: 0.009678504429757595
sum: 10.046363830566406 - mean: 1.2263627468200866e-05 - std: 8.38851050843914e-08
 * min 9.920488992065657e-06, max: 1.2743727893393952e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 9.264610525860917e-06
 * min 0.0007441389025188982, max: 0.0008207343053072691
eps: tensor([0.0109, 0.0237, 0.0051, 0.0068, 0.0163, 0.0245, 0.0343, 0.0392, 0.0393],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.900, time 0.66
 * Lower 1 Val Acc 86.500, time 0.64
 * Upper 1 Val Acc 86.500, time 0.66
validation split name: 2
 *  Val Acc 78.250, time 0.63
 * Lower 1 Val Acc 77.250, time 0.63
 * Upper 1 Val Acc 77.250, time 0.69
validation split name: 3
 *  Val Acc 85.050, time 0.63
 * Lower 1 Val Acc 84.150, time 0.64
 * Upper 1 Val Acc 84.150, time 0.63
validation split name: 4
 *  Val Acc 78.900, time 0.62
 * Lower 1 Val Acc 77.200, time 0.62
 * Upper 1 Val Acc 77.200, time 0.63
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 83.170, Loss 0.371
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 85.790, Loss 0.339
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.63
Epoch:2
LR: 0.001
 * Train Acc 85.450, Loss 0.328
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.62
Epoch:3
LR: 0.001
 * Train Acc 85.870, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.64
Epoch:4
LR: 0.001
 * Train Acc 85.800, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 85.510, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.65
Epoch:6
LR: 0.001
 * Train Acc 85.410, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.64
Epoch:7
LR: 0.001
 * Train Acc 85.550, Loss 0.294
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.62
Epoch:8
LR: 0.001
 * Train Acc 85.470, Loss 0.292
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 85.280, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 85.680, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.61
Epoch:11
LR: 0.001
 * Train Acc 85.540, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.63
Epoch:12
LR: 0.001
 * Train Acc 85.550, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.64
Epoch:13
LR: 0.001
 * Train Acc 85.130, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 85.420, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.63
Epoch:15
LR: 0.001
 * Train Acc 85.690, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.62
Epoch:16
LR: 0.001
 * Train Acc 85.520, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.63
Epoch:17
LR: 0.001
 * Train Acc 85.290, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.63
Epoch:18
LR: 0.001
 * Train Acc 85.550, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 85.930, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.66
Epoch:20
LR: 0.001
 * Train Acc 85.330, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.63
Epoch:21
LR: 0.001
 * Train Acc 85.060, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.66
Epoch:22
LR: 0.001
 * Train Acc 85.530, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 85.430, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.62
Epoch:24
LR: 0.001
 * Train Acc 85.470, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.64
Epoch:25
LR: 0.001
 * Train Acc 85.130, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.60
Epoch:26
LR: 0.001
 * Train Acc 85.910, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 85.560, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 85.070, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.63
Epoch:29
LR: 0.001
 * Train Acc 84.650, Loss 0.178
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.65
Epoch:30
LR: 0.001
 * Train Acc 84.940, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.62
Epoch:31
LR: 0.001
 * Train Acc 84.600, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.64
Epoch:32
LR: 0.001
 * Train Acc 85.170, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.63
Epoch:33
LR: 0.001
 * Train Acc 84.260, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.61
Epoch:34
LR: 0.001
 * Train Acc 84.430, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.61
Epoch:35
LR: 0.001
 * Train Acc 84.690, Loss 0.178
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.61
Epoch:36
LR: 0.001
 * Train Acc 84.650, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 84.870, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 84.350, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.62
Epoch:39
LR: 0.001
 * Train Acc 84.540, Loss 0.178
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.63
Epoch:40
LR: 0.001
 * Train Acc 84.410, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.63
Epoch:41
LR: 0.001
 * Train Acc 84.390, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.64
Epoch:42
LR: 0.001
 * Train Acc 85.070, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 84.260, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.63
Epoch:44
LR: 0.001
 * Train Acc 84.230, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 84.690, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.63
Epoch:46
LR: 0.001
 * Train Acc 84.220, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.63
Epoch:47
LR: 0.001
 * Train Acc 84.470, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.62
Epoch:48
LR: 0.001
 * Train Acc 84.480, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.64
Epoch:49
LR: 0.001
 * Train Acc 84.180, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.65
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.52427077293396 - mean: 0.0006067948997952044 - std: 0.00028294793446548283
 * min 0.00030219837208278477, max: 0.001442194334231317
sum: 12.200981140136719 - mean: 0.0013238912215456367 - std: 0.000795643893070519
 * min 0.00034581407089717686, max: 0.004156784154474735
sum: 5.276089668273926 - mean: 0.0002862461842596531 - std: 0.00011723572970367968
 * min 7.572136382805184e-05, max: 0.0008013137266971171
sum: 14.005926132202148 - mean: 0.000379935052478686 - std: 0.00011294609430478886
 * min 0.00010194713831879199, max: 0.0012017744593322277
sum: 66.69842529296875 - mean: 0.0009046553168445826 - std: 0.00028277517412789166
 * min 0.0002048163878498599, max: 0.0029113551136106253
sum: 199.71249389648438 - mean: 0.001354387030005455 - std: 0.00028244382701814175
 * min 0.0004029409901704639, max: 0.003999527543783188
sum: 279.9144592285156 - mean: 0.0018982914043590426 - std: 0.0003482016909401864
 * min 0.0005247994558885694, max: 0.004826486576348543
sum: 5.018341541290283 - mean: 6.125905201770365e-06 - std: 4.1902136871385665e-08
 * min 4.95546419188031e-06, max: 6.365724402712658e-06
sum: 0.20000001788139343 - mean: 0.00039062503492459655 - std: 2.434785173477394e-08
 * min 0.00039050649502314627, max: 0.00039074363303370774
eps: tensor([0.0055, 0.0119, 0.0026, 0.0034, 0.0081, 0.0122, 0.0171, 0.0196, 0.0196],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 91.500, time 0.64
 * Lower 1 Val Acc 91.150, time 0.65
 * Upper 1 Val Acc 91.150, time 0.65
validation split name: 2
 *  Val Acc 77.300, time 0.66
 * Lower 1 Val Acc 76.900, time 0.63
 * Upper 1 Val Acc 76.900, time 0.62
validation split name: 3
 *  Val Acc 81.200, time 0.63
 * Lower 1 Val Acc 81.050, time 0.65
 * Upper 1 Val Acc 81.050, time 0.64
validation split name: 4
 *  Val Acc 75.300, time 0.62
 * Lower 1 Val Acc 76.000, time 0.63
 * Upper 1 Val Acc 76.000, time 0.64
validation split name: 5
 *  Val Acc 80.700, time 0.66
 * Lower 1 Val Acc 80.500, time 0.62
 * Upper 1 Val Acc 80.500, time 0.64
Task 1 average acc: 97.85
Task 2 average acc: 87.975
Task 3 average acc: 82.76666666666667
Task 4 average acc: 82.275
Task 5 average acc: 81.2
===Summary of experiment repeats: 2 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2   0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 15.652000000000001 std: 31.33159963998008
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.060, Loss 0.582
 * , robust loss: 0.337 robust error: 0.03000000
 *  Val Acc 79.700, time 0.66
Epoch:1
LR: 0.001
 * Train Acc 79.820, Loss 0.430
 * , robust loss: 0.010 robust error: 0.00000000
 *  Val Acc 83.150, time 0.61
Epoch:2
LR: 0.001
 * Train Acc 84.060, Loss 0.357
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 84.300, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 86.230, Loss 0.310
 * , robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 88.900, time 0.61
Epoch:4
LR: 0.001
 * Train Acc 88.030, Loss 0.276
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.500, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 89.020, Loss 0.256
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.700, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 89.780, Loss 0.221
 * , robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 89.650, time 0.62
Epoch:7
LR: 0.001
 * Train Acc 91.310, Loss 0.190
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 92.500, time 0.64
Epoch:8
LR: 0.001
 * Train Acc 91.830, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.150, time 0.66
Epoch:9
LR: 0.001
 * Train Acc 93.130, Loss 0.162
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.600, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 93.920, Loss 0.154
 * , robust loss: 14.029 robust error: 0.01000000
 *  Val Acc 95.450, time 0.63
Epoch:11
LR: 0.001
 * Train Acc 93.650, Loss 0.162
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.350, time 0.66
Epoch:12
LR: 0.001
 * Train Acc 94.300, Loss 0.111
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.650, time 0.62
Epoch:13
LR: 0.001
 * Train Acc 95.290, Loss 0.997
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.400, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 95.220, Loss 0.149
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.250, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 95.950, Loss 0.082
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.62
Epoch:16
LR: 0.001
 * Train Acc 95.710, Loss 0.271
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 96.700, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 95.990, Loss 0.129
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.62
Epoch:18
LR: 0.001
 * Train Acc 96.350, Loss 0.071
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 96.640, Loss 0.060
 * , robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 97.600, time 0.63
Epoch:20
LR: 0.001
 * Train Acc 96.730, Loss 0.197
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.450, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 96.940, Loss 1.669
 * , robust loss: 0.035 robust error: 0.00000000
 *  Val Acc 96.700, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 96.950, Loss 0.051
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.63
Epoch:23
LR: 0.001
 * Train Acc 97.220, Loss 30.466
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.850, time 0.65
Epoch:24
LR: 0.001
 * Train Acc 97.030, Loss 0.158
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.64
Epoch:25
LR: 0.001
 * Train Acc 97.430, Loss 0.042
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.65
Epoch:26
LR: 0.001
 * Train Acc 97.560, Loss 0.035
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 97.390, Loss 0.037
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.64
Epoch:28
LR: 0.001
 * Train Acc 97.800, Loss 0.054
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.65
Epoch:29
LR: 0.001
 * Train Acc 97.720, Loss 0.489
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.64
Epoch:30
LR: 0.001
 * Train Acc 97.790, Loss 0.030
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.63
Epoch:31
LR: 0.001
 * Train Acc 97.990, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.64
Epoch:32
LR: 0.001
 * Train Acc 97.410, Loss 27.941
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.64
Epoch:33
LR: 0.001
 * Train Acc 97.840, Loss 0.030
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.200, time 0.63
Epoch:34
LR: 0.001
 * Train Acc 97.850, Loss 0.096
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.100, time 0.62
Epoch:35
LR: 0.001
 * Train Acc 98.130, Loss 0.034
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.63
Epoch:36
LR: 0.001
 * Train Acc 98.280, Loss 0.518
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.65
Epoch:37
LR: 0.001
 * Train Acc 98.010, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.69
Epoch:38
LR: 0.001
 * Train Acc 97.980, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 97.970, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.63
Epoch:40
LR: 0.001
 * Train Acc 98.100, Loss 0.025
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.250, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 98.310, Loss 0.026
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 96.700, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 98.210, Loss 134.251
 * , robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 96.850, time 0.66
Epoch:43
LR: 0.001
 * Train Acc 98.200, Loss 1.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.400, time 0.66
Epoch:44
LR: 0.001
 * Train Acc 98.300, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.71
Epoch:45
LR: 0.001
 * Train Acc 98.260, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.64
Epoch:46
LR: 0.001
 * Train Acc 98.130, Loss 1.580
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.65
Epoch:47
LR: 0.001
 * Train Acc 98.160, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 98.260, Loss 0.358
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.65
Epoch:49
LR: 0.001
 * Train Acc 98.230, Loss 0.063
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.64
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.099577903747559 - mean: 0.015161548741161823 - std: 0.005036663264036179
 * min 0.009345410391688347, max: 0.02938232198357582
sum: 304.4278259277344 - mean: 0.033032532781362534 - std: 0.014748101122677326
 * min 0.012064668349921703, max: 0.07989756762981415
sum: 172.5369110107422 - mean: 0.009360726922750473 - std: 0.0031293334905058146
 * min 0.003391219535842538, max: 0.021014370024204254
sum: 471.7438659667969 - mean: 0.012796871364116669 - std: 0.0034213445615023375
 * min 0.004391237162053585, max: 0.02868315763771534
sum: 2351.2080078125 - mean: 0.031890299171209335 - std: 0.00894327275454998
 * min 0.009925050660967827, max: 0.07873344421386719
sum: 5129.68408203125 - mean: 0.034787897020578384 - std: 0.00678815133869648
 * min 0.012351227924227715, max: 0.09142059087753296
sum: 6240.7734375 - mean: 0.042322952300310135 - std: 0.006895826198160648
 * min 0.014322390779852867, max: 0.11038383841514587
sum: 113.36859130859375 - mean: 0.0001383893977617845 - std: 1.0851827028091066e-06
 * min 0.0001059260030160658, max: 0.00014716495934408158
sum: 5.0 - mean: 0.009765625 - std: 0.00490032322704792
 * min 0.0026815447490662336, max: 0.04160017892718315
eps: tensor([0.1365, 0.2973, 0.0842, 0.1152, 0.2870, 0.3131, 0.3809, 0.4428, 0.4430],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.400, time 0.67
 * Lower 1 Val Acc 63.000, time 0.66
 * Upper 1 Val Acc 63.000, time 0.64
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.840, Loss 0.555
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.950, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 78.770, Loss 0.453
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.700, time 0.63
Epoch:2
LR: 0.001
 * Train Acc 80.420, Loss 0.411
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.62
Epoch:3
LR: 0.001
 * Train Acc 81.790, Loss 0.390
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.65
Epoch:4
LR: 0.001
 * Train Acc 82.800, Loss 0.360
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.65
Epoch:5
LR: 0.001
 * Train Acc 83.410, Loss 0.346
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.66
Epoch:6
LR: 0.001
 * Train Acc 83.890, Loss 0.333
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.63
Epoch:7
LR: 0.001
 * Train Acc 83.700, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.65
Epoch:8
LR: 0.001
 * Train Acc 83.640, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.63
Epoch:9
LR: 0.001
 * Train Acc 84.000, Loss 0.337
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.63
Epoch:10
LR: 0.001
 * Train Acc 83.850, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.61
Epoch:11
LR: 0.001
 * Train Acc 84.250, Loss 0.289
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.64
Epoch:12
LR: 0.001
 * Train Acc 84.810, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.63
Epoch:13
LR: 0.001
 * Train Acc 84.930, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.63
Epoch:14
LR: 0.001
 * Train Acc 84.560, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 84.910, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 84.900, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 84.680, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.63
Epoch:18
LR: 0.001
 * Train Acc 84.910, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.64
Epoch:19
LR: 0.001
 * Train Acc 84.490, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.63
Epoch:20
LR: 0.001
 * Train Acc 83.890, Loss 1.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.62
Epoch:21
LR: 0.001
 * Train Acc 84.960, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 84.780, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.63
Epoch:23
LR: 0.001
 * Train Acc 84.540, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.61
Epoch:24
LR: 0.001
 * Train Acc 84.930, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 84.470, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 84.660, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 84.680, Loss 0.191
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 84.350, time 0.62
Epoch:28
LR: 0.001
 * Train Acc 84.400, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.71
Epoch:29
LR: 0.001
 * Train Acc 84.460, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.63
Epoch:30
LR: 0.001
 * Train Acc 84.850, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.63
Epoch:31
LR: 0.001
 * Train Acc 84.510, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 85.140, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.65
Epoch:33
LR: 0.001
 * Train Acc 84.920, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.65
Epoch:34
LR: 0.001
 * Train Acc 84.250, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.66
Epoch:35
LR: 0.001
 * Train Acc 84.610, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.61
Epoch:36
LR: 0.001
 * Train Acc 85.060, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 84.100, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 84.370, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 84.100, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.66
Epoch:40
LR: 0.001
 * Train Acc 84.210, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.66
Epoch:41
LR: 0.001
 * Train Acc 84.910, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 84.270, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 84.210, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.62
Epoch:44
LR: 0.001
 * Train Acc 84.000, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.64
Epoch:45
LR: 0.001
 * Train Acc 83.520, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.65
Epoch:46
LR: 0.001
 * Train Acc 83.530, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 83.600, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.65
Epoch:48
LR: 0.001
 * Train Acc 83.180, Loss 14.146
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.64
Epoch:49
LR: 0.001
 * Train Acc 83.250, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.65
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.7998857498168945 - mean: 0.005555423442274332 - std: 0.002667449414730072
 * min 0.002900822088122368, max: 0.01357777789235115
sum: 112.89944458007812 - mean: 0.01225037407130003 - std: 0.006958359386771917
 * min 0.0034130816347897053, max: 0.036377809941768646
sum: 49.07054138183594 - mean: 0.0026622472796589136 - std: 0.0010555104818195105
 * min 0.0007185477297753096, max: 0.007079402916133404
sum: 133.5132598876953 - mean: 0.003621778916567564 - std: 0.0010791035601869226
 * min 0.00104255392216146, max: 0.009338063187897205
sum: 733.16162109375 - mean: 0.009944140911102295 - std: 0.0030969511717557907
 * min 0.0026858702767640352, max: 0.028083764016628265
sum: 1758.42529296875 - mean: 0.01192508451640606 - std: 0.0025885712821036577
 * min 0.003828697372227907, max: 0.0323159359395504
sum: 2309.390380859375 - mean: 0.015661556273698807 - std: 0.002835544990375638
 * min 0.004773806780576706, max: 0.04371156916022301
sum: 44.202613830566406 - mean: 5.395826883614063e-05 - std: 4.340271857472544e-07
 * min 4.12952431361191e-05, max: 5.7381155784241855e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0003344601718708873
 * min 0.00237402506172657, max: 0.0052602034993469715
eps: tensor([0.0500, 0.1103, 0.0240, 0.0326, 0.0895, 0.1073, 0.1410, 0.1727, 0.1727],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.050, time 0.63
 * Lower 1 Val Acc 83.000, time 0.68
 * Upper 1 Val Acc 83.000, time 0.65
validation split name: 2
 *  Val Acc 82.250, time 0.65
 * Lower 1 Val Acc 69.700, time 0.66
 * Upper 1 Val Acc 69.700, time 0.66
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.650, Loss 0.460
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.66
Epoch:1
LR: 0.001
 * Train Acc 82.600, Loss 0.387
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.63
Epoch:2
LR: 0.001
 * Train Acc 83.030, Loss 0.367
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 83.480, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.63
Epoch:4
LR: 0.001
 * Train Acc 83.840, Loss 0.342
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.63
Epoch:5
LR: 0.001
 * Train Acc 83.620, Loss 0.344
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.66
Epoch:6
LR: 0.001
 * Train Acc 83.920, Loss 0.334
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 84.050, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.61
Epoch:8
LR: 0.001
 * Train Acc 83.820, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.66
Epoch:9
LR: 0.001
 * Train Acc 83.620, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.63
Epoch:10
LR: 0.001
 * Train Acc 83.430, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.65
Epoch:11
LR: 0.001
 * Train Acc 83.490, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 83.630, Loss 0.291
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.65
Epoch:13
LR: 0.001
 * Train Acc 83.700, Loss 0.293
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 83.370, Loss 0.286
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.65
Epoch:15
LR: 0.001
 * Train Acc 83.760, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.66
Epoch:16
LR: 0.001
 * Train Acc 83.940, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.63
Epoch:17
LR: 0.001
 * Train Acc 83.930, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 83.600, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 83.320, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.65
Epoch:20
LR: 0.001
 * Train Acc 84.080, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.65
Epoch:21
LR: 0.001
 * Train Acc 83.590, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 83.230, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 83.260, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 83.280, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.65
Epoch:25
LR: 0.001
 * Train Acc 83.720, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.64
Epoch:26
LR: 0.001
 * Train Acc 83.140, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 83.760, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.64
Epoch:28
LR: 0.001
 * Train Acc 83.080, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.64
Epoch:29
LR: 0.001
 * Train Acc 83.340, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 83.630, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.65
Epoch:31
LR: 0.001
 * Train Acc 83.080, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.64
Epoch:32
LR: 0.001
 * Train Acc 83.040, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.61
Epoch:33
LR: 0.001
 * Train Acc 82.660, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.62
Epoch:34
LR: 0.001
 * Train Acc 82.490, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.67
Epoch:35
LR: 0.001
 * Train Acc 82.670, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.65
Epoch:36
LR: 0.001
 * Train Acc 83.070, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 82.860, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 83.070, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.64
Epoch:39
LR: 0.001
 * Train Acc 82.550, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 82.330, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.63
Epoch:41
LR: 0.001
 * Train Acc 82.500, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.63
Epoch:42
LR: 0.001
 * Train Acc 82.460, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 82.360, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 81.890, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.62
Epoch:45
LR: 0.001
 * Train Acc 82.550, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.63
Epoch:46
LR: 0.001
 * Train Acc 81.860, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 82.390, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.66
Epoch:48
LR: 0.001
 * Train Acc 82.010, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.64
Epoch:49
LR: 0.001
 * Train Acc 81.570, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.68
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.5702602863311768 - mean: 0.0018174309516325593 - std: 0.0008899313397705555
 * min 0.0009441428119316697, max: 0.004464663099497557
sum: 39.21218490600586 - mean: 0.004254794213920832 - std: 0.0024614823050796986
 * min 0.001147473813034594, max: 0.012965945526957512
sum: 16.006763458251953 - mean: 0.0008684225031174719 - std: 0.00034882983891293406
 * min 0.00022400484886020422, max: 0.002352142706513405
sum: 43.41792297363281 - mean: 0.0011777865001931787 - std: 0.00035353429848328233
 * min 0.00033617642475292087, max: 0.003102929564192891
sum: 236.91091918945312 - mean: 0.0032133099157363176 - std: 0.0010064149973914027
 * min 0.0008569418569095433, max: 0.009124096482992172
sum: 579.1959228515625 - mean: 0.003927923738956451 - std: 0.0008550264756195247
 * min 0.001255262759514153, max: 0.010657335631549358
sum: 767.9091796875 - mean: 0.005207717418670654 - std: 0.0009457169217057526
 * min 0.0015834924997761846, max: 0.014590519480407238
sum: 14.817956924438477 - mean: 1.808832530514337e-05 - std: 1.4552742300111277e-07
 * min 1.3843305168848019e-05, max: 1.923577656270936e-05
sum: 0.5999999642372131 - mean: 0.001171874930150807 - std: 7.753059435344767e-06
 * min 0.0011184392496943474, max: 0.001227894565090537
eps: tensor([0.0164, 0.0383, 0.0078, 0.0106, 0.0289, 0.0354, 0.0469, 0.0579, 0.0579],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 84.500, time 0.67
 * Lower 1 Val Acc 84.700, time 0.66
 * Upper 1 Val Acc 84.700, time 0.63
validation split name: 2
 *  Val Acc 74.700, time 0.63
 * Lower 1 Val Acc 75.400, time 0.62
 * Upper 1 Val Acc 75.400, time 0.63
validation split name: 3
 *  Val Acc 80.850, time 0.64
 * Lower 1 Val Acc 79.750, time 0.65
 * Upper 1 Val Acc 79.750, time 0.70
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.400, Loss 0.561
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.63
Epoch:1
LR: 0.001
 * Train Acc 78.580, Loss 0.446
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.65
Epoch:2
LR: 0.001
 * Train Acc 80.170, Loss 0.420
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.64
Epoch:3
LR: 0.001
 * Train Acc 80.640, Loss 0.410
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.65
Epoch:4
LR: 0.001
 * Train Acc 81.430, Loss 0.386
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 81.910, Loss 0.378
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.65
Epoch:6
LR: 0.001
 * Train Acc 81.480, Loss 0.373
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.62
Epoch:7
LR: 0.001
 * Train Acc 82.300, Loss 0.359
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.63
Epoch:8
LR: 0.001
 * Train Acc 81.710, Loss 0.356
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.62
Epoch:9
LR: 0.001
 * Train Acc 82.470, Loss 0.341
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 82.170, Loss 0.332
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.65
Epoch:11
LR: 0.001
 * Train Acc 81.960, Loss 0.333
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.65
Epoch:12
LR: 0.001
 * Train Acc 81.930, Loss 0.324
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 81.630, Loss 0.319
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.64
Epoch:14
LR: 0.001
 * Train Acc 82.120, Loss 0.313
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.63
Epoch:15
LR: 0.001
 * Train Acc 81.350, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.64
Epoch:16
LR: 0.001
 * Train Acc 81.670, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 81.670, Loss 0.293
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.62
Epoch:18
LR: 0.001
 * Train Acc 82.010, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.63
Epoch:19
LR: 0.001
 * Train Acc 81.590, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.62
Epoch:20
LR: 0.001
 * Train Acc 81.840, Loss 0.268
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.64
Epoch:21
LR: 0.001
 * Train Acc 80.930, Loss 0.271
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.63
Epoch:22
LR: 0.001
 * Train Acc 80.740, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.65
Epoch:23
LR: 0.001
 * Train Acc 80.350, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.65
Epoch:24
LR: 0.001
 * Train Acc 80.650, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.65
Epoch:25
LR: 0.001
 * Train Acc 80.180, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.66
Epoch:26
LR: 0.001
 * Train Acc 80.750, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.65
Epoch:27
LR: 0.001
 * Train Acc 80.370, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.66
Epoch:28
LR: 0.001
 * Train Acc 80.540, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.64
Epoch:29
LR: 0.001
 * Train Acc 80.100, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.63
Epoch:30
LR: 0.001
 * Train Acc 81.040, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.65
Epoch:31
LR: 0.001
 * Train Acc 79.770, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.65
Epoch:32
LR: 0.001
 * Train Acc 79.550, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.61
Epoch:33
LR: 0.001
 * Train Acc 79.930, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.63
Epoch:34
LR: 0.001
 * Train Acc 79.460, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.63
Epoch:35
LR: 0.001
 * Train Acc 79.100, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.63
Epoch:36
LR: 0.001
 * Train Acc 79.840, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 79.230, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 78.850, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.66
Epoch:39
LR: 0.001
 * Train Acc 79.030, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 78.530, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.62
Epoch:41
LR: 0.001
 * Train Acc 78.260, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.63
Epoch:42
LR: 0.001
 * Train Acc 78.730, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.65
Epoch:43
LR: 0.001
 * Train Acc 78.370, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.64
Epoch:44
LR: 0.001
 * Train Acc 78.930, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.66
Epoch:45
LR: 0.001
 * Train Acc 78.340, Loss 0.234
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.63
Epoch:46
LR: 0.001
 * Train Acc 78.310, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.63
Epoch:47
LR: 0.001
 * Train Acc 77.450, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.63
Epoch:48
LR: 0.001
 * Train Acc 78.030, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.64
Epoch:49
LR: 0.001
 * Train Acc 77.530, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.65
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0653512477874756 - mean: 0.0012330454774200916 - std: 0.0006064355839043856
 * min 0.0006389427580870688, max: 0.0030372922774404287
sum: 26.189586639404297 - mean: 0.0028417520225048065 - std: 0.0016501775244250894
 * min 0.000762767915148288, max: 0.008692131377756596
sum: 10.75069522857666 - mean: 0.0005832625320181251 - std: 0.00023485813289880753
 * min 0.0001502939558122307, max: 0.0015814750222489238
sum: 29.11814308166504 - mean: 0.0007898801704868674 - std: 0.0002372859453316778
 * min 0.00022544524108525366, max: 0.002081885002553463
sum: 158.34913635253906 - mean: 0.0021477476693689823 - std: 0.0006728940643370152
 * min 0.0005727724055759609, max: 0.006105510052293539
sum: 384.2848815917969 - mean: 0.002606098772957921 - std: 0.0005673352279700339
 * min 0.0008328367257490754, max: 0.007070951163768768
sum: 506.26397705078125 - mean: 0.003433322301134467 - std: 0.0006235131877474487
 * min 0.0010439237812533975, max: 0.009623605757951736
sum: 9.889799118041992 - mean: 1.2072508070559707e-05 - std: 9.712800874694949e-08
 * min 9.239297469321173e-06, max: 1.2838340808229987e-05
sum: 0.3999999761581421 - mean: 0.0007812499534338713 - std: 4.193676375052746e-07
 * min 0.0007782966713421047, max: 0.0007842196500860155
eps: tensor([0.0111, 0.0256, 0.0052, 0.0071, 0.0193, 0.0235, 0.0309, 0.0386, 0.0387],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.950, time 0.66
 * Lower 1 Val Acc 87.050, time 0.65
 * Upper 1 Val Acc 87.050, time 0.61
validation split name: 2
 *  Val Acc 75.400, time 0.62
 * Lower 1 Val Acc 75.700, time 0.64
 * Upper 1 Val Acc 75.700, time 0.67
validation split name: 3
 *  Val Acc 81.650, time 0.64
 * Lower 1 Val Acc 80.350, time 0.65
 * Upper 1 Val Acc 80.350, time 0.62
validation split name: 4
 *  Val Acc 80.200, time 0.63
 * Lower 1 Val Acc 78.750, time 0.65
 * Upper 1 Val Acc 78.750, time 0.64
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.860, Loss 0.432
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 0.66
Epoch:1
LR: 0.001
 * Train Acc 83.620, Loss 0.367
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 83.610, Loss 0.359
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 0.64
Epoch:3
LR: 0.001
 * Train Acc 83.720, Loss 0.352
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 84.210, Loss 0.344
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.62
Epoch:5
LR: 0.001
 * Train Acc 83.470, Loss 0.337
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 83.290, Loss 0.331
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 83.640, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.66
Epoch:8
LR: 0.001
 * Train Acc 83.790, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 83.730, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.61
Epoch:10
LR: 0.001
 * Train Acc 83.990, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.63
Epoch:11
LR: 0.001
 * Train Acc 83.910, Loss 0.300
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.65
Epoch:12
LR: 0.001
 * Train Acc 83.780, Loss 0.294
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.63
Epoch:13
LR: 0.001
 * Train Acc 83.720, Loss 0.288
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 83.920, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.62
Epoch:15
LR: 0.001
 * Train Acc 83.630, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 83.330, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 83.620, Loss 0.263
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.64
Epoch:18
LR: 0.001
 * Train Acc 83.540, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.63
Epoch:19
LR: 0.001
 * Train Acc 83.130, Loss 0.258
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.64
Epoch:20
LR: 0.001
 * Train Acc 82.640, Loss 0.250
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 83.310, Loss 0.242
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.64
Epoch:22
LR: 0.001
 * Train Acc 83.220, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.64
Epoch:23
LR: 0.001
 * Train Acc 83.300, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.64
Epoch:24
LR: 0.001
 * Train Acc 82.940, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 83.850, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 83.010, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 82.790, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 82.610, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 82.410, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 82.830, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.65
Epoch:31
LR: 0.001
 * Train Acc 82.580, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 82.770, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.62
Epoch:33
LR: 0.001
 * Train Acc 82.620, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.65
Epoch:34
LR: 0.001
 * Train Acc 82.060, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 82.330, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.64
Epoch:36
LR: 0.001
 * Train Acc 82.650, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 82.100, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 81.700, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.65
Epoch:39
LR: 0.001
 * Train Acc 82.180, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.62
Epoch:40
LR: 0.001
 * Train Acc 82.180, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.63
Epoch:41
LR: 0.001
 * Train Acc 82.450, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.64
Epoch:42
LR: 0.001
 * Train Acc 81.670, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.63
Epoch:43
LR: 0.001
 * Train Acc 81.760, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 81.750, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.64
Epoch:45
LR: 0.001
 * Train Acc 81.730, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.65
Epoch:46
LR: 0.001
 * Train Acc 81.400, Loss 0.204
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 79.900, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 81.290, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.500, time 0.63
Epoch:48
LR: 0.001
 * Train Acc 81.310, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.63
Epoch:49
LR: 0.001
 * Train Acc 81.310, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.64
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5596791505813599 - mean: 0.0006477768183685839 - std: 0.00035165934241376817
 * min 0.00031686617876403034, max: 0.001733055105432868
sum: 14.596385955810547 - mean: 0.0015838092658668756 - std: 0.000988248735666275
 * min 0.00036797550274059176, max: 0.005326573736965656
sum: 5.174015045166016 - mean: 0.00028070827829651535 - std: 0.00011789517884608358
 * min 6.837278488092124e-05, max: 0.0008172776433639228
sum: 13.754983901977539 - mean: 0.00037312781205400825 - std: 0.00011381678632460535
 * min 9.59739409154281e-05, max: 0.0010252883657813072
sum: 71.97760009765625 - mean: 0.0009762587142176926 - std: 0.00030864274594932795
 * min 0.00024918088456615806, max: 0.0028589514549821615
sum: 179.33895874023438 - mean: 0.0012162202037870884 - std: 0.0002661561011336744
 * min 0.0003852483641821891, max: 0.003308710176497698
sum: 239.9640655517578 - mean: 0.0016273604705929756 - std: 0.00029779798933304846
 * min 0.00048373034223914146, max: 0.004697016905993223
sum: 5.0734992027282715 - mean: 6.193236004037317e-06 - std: 4.983357015930778e-08
 * min 4.739789346785983e-06, max: 6.586109520867467e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 8.276321750599891e-06
 * min 0.0003591959539335221, max: 0.0004252782673574984
eps: tensor([0.0058, 0.0143, 0.0025, 0.0034, 0.0088, 0.0109, 0.0146, 0.0198, 0.0198],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.450, time 0.63
 * Lower 1 Val Acc 90.150, time 0.63
 * Upper 1 Val Acc 90.150, time 0.61
validation split name: 2
 *  Val Acc 75.250, time 0.66
 * Lower 1 Val Acc 75.350, time 0.63
 * Upper 1 Val Acc 75.350, time 0.64
validation split name: 3
 *  Val Acc 75.500, time 0.65
 * Lower 1 Val Acc 75.050, time 0.67
 * Upper 1 Val Acc 75.050, time 0.64
validation split name: 4
 *  Val Acc 75.650, time 0.66
 * Lower 1 Val Acc 74.750, time 0.67
 * Upper 1 Val Acc 74.750, time 0.68
validation split name: 5
 *  Val Acc 79.000, time 0.65
 * Lower 1 Val Acc 79.400, time 0.65
 * Upper 1 Val Acc 79.400, time 0.64
Task 1 average acc: 97.4
Task 2 average acc: 86.15
Task 3 average acc: 80.01666666666667
Task 4 average acc: 81.05000000000001
Task 5 average acc: 79.17
===Summary of experiment repeats: 3 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17  0.    0.    0.    0.    0.    0.    0.  ]
mean: 23.569 std: 36.02700888222613
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 50.590, Loss 0.624
 * , robust loss: 0.024 robust error: 0.03000000
 *  Val Acc 50.000, time 0.64
Epoch:1
LR: 0.001
 * Train Acc 50.000, Loss 0.571
 * , robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 50.000, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 75.670, Loss 0.461
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 83.520, Loss 0.364
 * , robust loss: 0.034 robust error: 0.01000000
 *  Val Acc 87.200, time 0.63
Epoch:4
LR: 0.001
 * Train Acc 85.390, Loss 0.320
 * , robust loss: 1.190 robust error: 0.01000000
 *  Val Acc 85.650, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 87.740, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 89.500, Loss 0.228
 * , robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 91.650, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 90.790, Loss 0.197
 * , robust loss: 0.074 robust error: 0.01000000
 *  Val Acc 93.350, time 0.66
Epoch:8
LR: 0.001
 * Train Acc 92.640, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.500, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 93.550, Loss 0.144
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.000, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 93.630, Loss 0.154
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.050, time 0.64
Epoch:11
LR: 0.001
 * Train Acc 94.570, Loss 0.114
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.350, time 0.64
Epoch:12
LR: 0.001
 * Train Acc 95.120, Loss 0.139
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.350, time 0.65
Epoch:13
LR: 0.001
 * Train Acc 95.900, Loss 0.088
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 0.63
Epoch:14
LR: 0.001
 * Train Acc 95.850, Loss 0.092
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.63
Epoch:15
LR: 0.001
 * Train Acc 96.570, Loss 0.100
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.62
Epoch:16
LR: 0.001
 * Train Acc 96.310, Loss 0.111
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.350, time 0.62
Epoch:17
LR: 0.001
 * Train Acc 96.630, Loss 0.082
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 95.350, time 0.64
Epoch:18
LR: 0.001
 * Train Acc 96.730, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.350, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 96.750, Loss 0.105
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.650, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 97.140, Loss 0.050
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.63
Epoch:21
LR: 0.001
 * Train Acc 97.330, Loss 0.057
 * , robust loss: 2.665 robust error: 0.01000000
 *  Val Acc 97.450, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 97.320, Loss 2.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.65
Epoch:23
LR: 0.001
 * Train Acc 97.350, Loss 0.042
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.65
Epoch:24
LR: 0.001
 * Train Acc 97.320, Loss 0.043
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.050, time 0.65
Epoch:25
LR: 0.001
 * Train Acc 97.470, Loss 6.848
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.64
Epoch:26
LR: 0.001
 * Train Acc 97.620, Loss 1.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 97.850, Loss 0.032
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.64
Epoch:28
LR: 0.001
 * Train Acc 97.940, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.64
Epoch:29
LR: 0.001
 * Train Acc 98.040, Loss 0.117
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 98.120, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 98.040, Loss 0.353
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.65
Epoch:32
LR: 0.001
 * Train Acc 98.220, Loss 64.158
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.63
Epoch:33
LR: 0.001
 * Train Acc 98.140, Loss 0.053
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.900, time 0.63
Epoch:34
LR: 0.001
 * Train Acc 98.160, Loss 0.047
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.63
Epoch:35
LR: 0.001
 * Train Acc 98.250, Loss 0.445
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.64
Epoch:36
LR: 0.001
 * Train Acc 98.020, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.65
Epoch:37
LR: 0.001
 * Train Acc 98.210, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 98.240, Loss 10.487
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.66
Epoch:39
LR: 0.001
 * Train Acc 98.410, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.63
Epoch:40
LR: 0.001
 * Train Acc 98.420, Loss 139.377
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.63
Epoch:41
LR: 0.001
 * Train Acc 98.420, Loss 55.976
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 98.300, Loss 0.055
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.66
Epoch:43
LR: 0.001
 * Train Acc 98.190, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.63
Epoch:44
LR: 0.001
 * Train Acc 98.340, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.64
Epoch:45
LR: 0.001
 * Train Acc 98.380, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.66
Epoch:46
LR: 0.001
 * Train Acc 98.520, Loss 0.021
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.300, time 0.63
Epoch:47
LR: 0.001
 * Train Acc 98.460, Loss 0.021
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 98.440, Loss 0.549
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.66
Epoch:49
LR: 0.001
 * Train Acc 98.580, Loss 45.990
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.66
after batch eps: 2.500000000000002, kappa: 0.5
sum: 15.397504806518555 - mean: 0.017821187153458595 - std: 0.006216895300894976
 * min 0.010725870728492737, max: 0.035009536892175674
sum: 298.5574951171875 - mean: 0.032395560294389725 - std: 0.01504963357001543
 * min 0.01116078719496727, max: 0.08259312808513641
sum: 191.3972930908203 - mean: 0.010383968241512775 - std: 0.0036383061669766903
 * min 0.003910308238118887, max: 0.022125083953142166
sum: 434.118408203125 - mean: 0.011776215396821499 - std: 0.0029998039826750755
 * min 0.004177547991275787, max: 0.02703567035496235
sum: 2427.59326171875 - mean: 0.03292633965611458 - std: 0.008951971307396889
 * min 0.010723544284701347, max: 0.0736771672964096
sum: 5156.43115234375 - mean: 0.03496928885579109 - std: 0.006282298360019922
 * min 0.01273191999644041, max: 0.0855926051735878
sum: 6109.49365234375 - mean: 0.04143265634775162 - std: 0.006582420784980059
 * min 0.014474575407803059, max: 0.09576598554849625
sum: 110.66674041748047 - mean: 0.0001350912352791056 - std: 8.313390367220563e-07
 * min 9.58192758844234e-05, max: 0.00014605569595005363
sum: 5.0 - mean: 0.009765625 - std: 0.00163851969409734
 * min 0.003651090431958437, max: 0.03398188203573227
eps: tensor([0.1604, 0.2916, 0.0935, 0.1060, 0.2963, 0.3147, 0.3729, 0.4323, 0.4324],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.100, time 0.62
 * Lower 1 Val Acc 56.800, time 0.63
 * Upper 1 Val Acc 56.800, time 0.63
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 67.440, Loss 0.611
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.850, time 0.63
Epoch:1
LR: 0.001
 * Train Acc 77.070, Loss 0.474
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.600, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 79.480, Loss 0.426
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 80.580, Loss 0.406
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.65
Epoch:4
LR: 0.001
 * Train Acc 81.560, Loss 0.382
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 81.650, Loss 0.374
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.500, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 82.730, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.64
Epoch:7
LR: 0.001
 * Train Acc 82.350, Loss 0.445
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.950, time 0.65
Epoch:8
LR: 0.001
 * Train Acc 81.910, Loss 0.361
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.65
Epoch:9
LR: 0.001
 * Train Acc 82.640, Loss 0.333
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 82.960, Loss 0.320
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.64
Epoch:11
LR: 0.001
 * Train Acc 83.130, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.63
Epoch:12
LR: 0.001
 * Train Acc 83.720, Loss 0.319
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 83.730, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 84.220, Loss 0.275
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.66
Epoch:15
LR: 0.001
 * Train Acc 83.580, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 83.860, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 83.790, Loss 0.266
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 84.260, Loss 0.250
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.66
Epoch:19
LR: 0.001
 * Train Acc 84.030, Loss 0.501
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.65
Epoch:20
LR: 0.001
 * Train Acc 83.590, Loss 0.741
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 83.550, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 84.670, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.64
Epoch:23
LR: 0.001
 * Train Acc 84.350, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.63
Epoch:24
LR: 0.001
 * Train Acc 84.460, Loss 0.421
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.65
Epoch:25
LR: 0.001
 * Train Acc 84.240, Loss 0.305
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.64
Epoch:26
LR: 0.001
 * Train Acc 84.220, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 84.040, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 84.420, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.64
Epoch:29
LR: 0.001
 * Train Acc 84.180, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 84.270, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.65
Epoch:31
LR: 0.001
 * Train Acc 83.840, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 83.910, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 84.380, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 84.000, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.66
Epoch:35
LR: 0.001
 * Train Acc 83.720, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.63
Epoch:36
LR: 0.001
 * Train Acc 83.240, Loss 1.691
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.63
Epoch:37
LR: 0.001
 * Train Acc 82.790, Loss 4.063
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 82.860, Loss 1.414
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 83.700, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.900, time 0.66
Epoch:40
LR: 0.001
 * Train Acc 83.570, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.65
Epoch:41
LR: 0.001
 * Train Acc 83.530, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 83.620, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.70
Epoch:43
LR: 0.001
 * Train Acc 83.620, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.64
Epoch:44
LR: 0.001
 * Train Acc 83.290, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 83.620, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.64
Epoch:46
LR: 0.001
 * Train Acc 83.030, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 83.770, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 83.030, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 82.240, Loss 0.773
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 5.09205436706543 - mean: 0.005893581546843052 - std: 0.002923183608800173
 * min 0.002731094602495432, max: 0.014221440069377422
sum: 120.95901489257812 - mean: 0.013124893419444561 - std: 0.007763274013996124
 * min 0.003509258385747671, max: 0.04182892665266991
sum: 54.76312255859375 - mean: 0.0029710896778851748 - std: 0.0012334993807598948
 * min 0.0008785338723100722, max: 0.0077701834961771965
sum: 123.29049682617188 - mean: 0.0033444687724113464 - std: 0.0009616186143830419
 * min 0.0010015612933784723, max: 0.00907709077000618
sum: 749.7001342773438 - mean: 0.010168459266424179 - std: 0.003114610444754362
 * min 0.0027464074082672596, max: 0.02780471369624138
sum: 1819.462646484375 - mean: 0.01233902107924223 - std: 0.002501016715541482
 * min 0.004181873984634876, max: 0.03240158408880234
sum: 2290.565185546875 - mean: 0.015533889643847942 - std: 0.002773204818367958
 * min 0.004860248416662216, max: 0.03851110860705376
sum: 42.18596649169922 - mean: 5.149653952685185e-05 - std: 3.2682169148756657e-07
 * min 3.646861296147108e-05, max: 5.5691452871542424e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.00035164906876161695
 * min 0.0025333198718726635, max: 0.005004752893000841
eps: tensor([0.0530, 0.1181, 0.0267, 0.0301, 0.0915, 0.1111, 0.1398, 0.1648, 0.1648],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.550, time 0.67
 * Lower 1 Val Acc 70.800, time 0.67
 * Upper 1 Val Acc 70.800, time 0.67
validation split name: 2
 *  Val Acc 82.350, time 0.69
 * Lower 1 Val Acc 71.100, time 0.68
 * Upper 1 Val Acc 71.100, time 0.68
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 73.240, Loss 0.541
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.150, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 81.230, Loss 0.412
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.69
Epoch:2
LR: 0.001
 * Train Acc 82.060, Loss 0.391
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 83.110, Loss 0.369
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 82.420, Loss 0.369
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 82.610, Loss 0.359
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 82.250, Loss 0.351
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.70
Epoch:7
LR: 0.001
 * Train Acc 83.320, Loss 0.336
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.250, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.66
Epoch:9
LR: 0.001
 * Train Acc 83.150, Loss 0.325
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.71
Epoch:10
LR: 0.001
 * Train Acc 82.870, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 83.180, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 83.340, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 82.860, Loss 0.293
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 82.980, Loss 0.289
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 82.620, Loss 0.286
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 82.780, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 83.420, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.250, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 83.150, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.66
Epoch:19
LR: 0.001
 * Train Acc 82.630, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 82.800, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.450, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 83.210, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.70
Epoch:22
LR: 0.001
 * Train Acc 82.700, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.70
Epoch:23
LR: 0.001
 * Train Acc 83.140, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.66
Epoch:24
LR: 0.001
 * Train Acc 82.800, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 82.430, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 82.210, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 82.560, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 83.150, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 82.980, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.900, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 82.900, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 82.350, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 82.630, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 82.320, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.66
Epoch:34
LR: 0.001
 * Train Acc 82.740, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.67
Epoch:35
LR: 0.001
 * Train Acc 82.460, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.70
Epoch:36
LR: 0.001
 * Train Acc 82.010, Loss 0.383
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.250, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 82.160, Loss 0.972
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.300, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 81.900, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 81.590, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.450, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 81.790, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 81.860, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.850, time 0.70
Epoch:42
LR: 0.001
 * Train Acc 82.000, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 81.470, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 81.750, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.600, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 80.980, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.450, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 81.320, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.350, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 81.520, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 81.970, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 81.510, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.450, time 0.68
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.827091097831726 - mean: 0.0021146887447685003 - std: 0.0011096332455053926
 * min 0.0009477335261180997, max: 0.005298646632581949
sum: 38.499488830566406 - mean: 0.004177461843937635 - std: 0.0025493386201560497
 * min 0.0010801535099744797, max: 0.013525891117751598
sum: 17.9333553314209 - mean: 0.0009729468147270381 - std: 0.00041222653817385435
 * min 0.0002772280713543296, max: 0.002613487420603633
sum: 40.516170501708984 - mean: 0.0010990714654326439 - std: 0.00031993657466955483
 * min 0.00032348179956898093, max: 0.0030147156212478876
sum: 244.52450561523438 - mean: 0.0033165758941322565 - std: 0.0010253931395709515
 * min 0.0008867350406944752, max: 0.009177015163004398
sum: 602.9008178710938 - mean: 0.004088683053851128 - std: 0.0008346643880940974
 * min 0.0013801888562738895, max: 0.010773378424346447
sum: 765.590087890625 - mean: 0.005191990174353123 - std: 0.0009332940680906177
 * min 0.001615967252291739, max: 0.012988532893359661
sum: 14.250299453735352 - mean: 1.7395384929841384e-05 - std: 1.1049560555420612e-07
 * min 1.2318994777160697e-05, max: 1.881241587398108e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 2.0824601961066946e-05
 * min 0.0010870363330468535, max: 0.0012644344242289662
eps: tensor([0.0190, 0.0376, 0.0088, 0.0099, 0.0298, 0.0368, 0.0467, 0.0557, 0.0557],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.000, time 0.68
 * Lower 1 Val Acc 76.150, time 0.69
 * Upper 1 Val Acc 76.150, time 0.66
validation split name: 2
 *  Val Acc 78.900, time 0.68
 * Lower 1 Val Acc 75.550, time 0.65
 * Upper 1 Val Acc 75.550, time 0.64
validation split name: 3
 *  Val Acc 77.450, time 0.71
 * Lower 1 Val Acc 78.250, time 0.65
 * Upper 1 Val Acc 78.250, time 0.64
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 77.390, Loss 0.462
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 80.600, Loss 0.418
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 80.890, Loss 0.405
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.66
Epoch:3
LR: 0.001
 * Train Acc 80.790, Loss 0.394
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.69
Epoch:4
LR: 0.001
 * Train Acc 81.070, Loss 0.388
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 81.780, Loss 0.375
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.71
Epoch:6
LR: 0.001
 * Train Acc 80.790, Loss 0.375
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 81.120, Loss 0.370
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 81.890, Loss 0.355
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 81.550, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 81.230, Loss 0.343
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.66
Epoch:11
LR: 0.001
 * Train Acc 81.840, Loss 0.331
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 81.370, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 81.500, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 81.370, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 81.120, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 80.980, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 80.790, Loss 0.300
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 80.950, Loss 0.294
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 80.930, Loss 0.288
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 80.830, Loss 0.279
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.65
Epoch:21
LR: 0.001
 * Train Acc 80.880, Loss 0.276
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 81.320, Loss 0.266
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 80.420, Loss 0.263
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 80.100, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 79.970, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 79.770, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 79.900, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 79.150, Loss 0.234
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 79.620, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 79.640, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 78.950, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 79.470, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 78.670, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.66
Epoch:34
LR: 0.001
 * Train Acc 79.130, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.66
Epoch:35
LR: 0.001
 * Train Acc 79.100, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 79.020, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 78.130, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 78.620, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 78.480, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 77.810, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 77.650, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 77.910, Loss 0.234
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 77.650, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 77.870, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 78.080, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 77.100, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 77.250, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 76.710, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 77.630, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.67
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.2208755016326904 - mean: 0.001413050340488553 - std: 0.0007420212496072054
 * min 0.0006330872420221567, max: 0.003542539896443486
sum: 25.715803146362305 - mean: 0.0027903432492166758 - std: 0.0017036993522197008
 * min 0.0007210754556581378, max: 0.009038781747221947
sum: 11.974970817565918 - mean: 0.00064968375954777 - std: 0.0002753578592091799
 * min 0.0001850259432103485, max: 0.0017456072382628918
sum: 27.045406341552734 - mean: 0.0007336536073125899 - std: 0.00021358921367209405
 * min 0.00021584649221040308, max: 0.0020126572344452143
sum: 162.89146423339844 - mean: 0.0022093569859862328 - std: 0.0006831171340309083
 * min 0.0005907026352360845, max: 0.00611521303653717
sum: 401.1549072265625 - mean: 0.0027205059304833412 - std: 0.0005553774535655975
 * min 0.0009183391812257469, max: 0.007168407551944256
sum: 509.5279541015625 - mean: 0.0034554575104266405 - std: 0.0006211461150087416
 * min 0.001075484324246645, max: 0.00864481832832098
sum: 9.502740859985352 - mean: 1.160002557298867e-05 - std: 7.368343801772426e-08
 * min 8.21485991764348e-06, max: 1.2544965102279093e-05
sum: 0.40000003576278687 - mean: 0.0007812500698491931 - std: 9.191336403091555e-08
 * min 0.0007805385394021869, max: 0.0007819625898264349
eps: tensor([0.0127, 0.0251, 0.0058, 0.0066, 0.0199, 0.0245, 0.0311, 0.0371, 0.0371],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.350, time 0.68
 * Lower 1 Val Acc 78.550, time 0.67
 * Upper 1 Val Acc 78.550, time 0.66
validation split name: 2
 *  Val Acc 80.400, time 0.68
 * Lower 1 Val Acc 77.300, time 0.69
 * Upper 1 Val Acc 77.300, time 0.65
validation split name: 3
 *  Val Acc 78.050, time 0.67
 * Lower 1 Val Acc 77.500, time 0.67
 * Upper 1 Val Acc 77.500, time 0.67
validation split name: 4
 *  Val Acc 81.400, time 0.69
 * Lower 1 Val Acc 81.650, time 0.70
 * Upper 1 Val Acc 81.650, time 0.67
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.620, Loss 0.441
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 84.060, Loss 0.368
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 84.350, Loss 0.351
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.66
Epoch:3
LR: 0.001
 * Train Acc 83.980, Loss 0.349
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 84.440, Loss 0.336
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 84.710, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.67
Epoch:6
LR: 0.001
 * Train Acc 84.720, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 84.470, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 84.540, Loss 0.313
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 84.570, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 84.470, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 84.770, Loss 0.289
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.780, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 84.930, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 84.410, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 84.390, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 84.820, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 84.570, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 84.070, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 84.450, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 84.320, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 84.450, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 83.990, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 84.130, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 84.610, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 84.200, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.70
Epoch:26
LR: 0.001
 * Train Acc 83.810, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 84.070, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.70
Epoch:28
LR: 0.001
 * Train Acc 84.020, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 83.980, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 83.680, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 84.410, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 84.030, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 83.850, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 83.950, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 83.520, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 83.830, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 84.010, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 84.030, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 83.630, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 83.210, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 83.260, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 83.400, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:43
LR: 0.001
 * Train Acc 83.620, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 83.330, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 83.280, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 83.170, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.66
Epoch:47
LR: 0.001
 * Train Acc 83.020, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 82.980, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 83.100, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.67
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.6453521251678467 - mean: 0.000746935314964503 - std: 0.0004170614993199706
 * min 0.0003379936970304698, max: 0.002022244269028306
sum: 13.27792739868164 - mean: 0.0014407472917810082 - std: 0.0009139156900346279
 * min 0.00035391433630138636, max: 0.004878290928900242
sum: 5.895050525665283 - mean: 0.00031982696964405477 - std: 0.00013880760525353253
 * min 8.37279440020211e-05, max: 0.000884214707184583
sum: 13.184032440185547 - mean: 0.00035763977211900055 - std: 0.00010532963642617688
 * min 9.988541569327936e-05, max: 0.0010036015883088112
sum: 75.49903869628906 - mean: 0.0010240213014185429 - std: 0.0003194051969330758
 * min 0.000272783829132095, max: 0.0028994902968406677
sum: 195.5263671875 - mean: 0.0013259981060400605 - std: 0.00027258999762125313
 * min 0.0004475062305573374, max: 0.003492670366540551
sum: 247.95396423339844 - mean: 0.0016815454000607133 - std: 0.00030553850228898227
 * min 0.0005211772513575852, max: 0.004277112428098917
sum: 4.853848934173584 - mean: 5.925108325754991e-06 - std: 3.7648764106279486e-08
 * min 4.1960774979088455e-06, max: 6.4079649746418e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 8.21361845737556e-06
 * min 0.00036456467933021486, max: 0.0004189654719084501
eps: tensor([0.0067, 0.0130, 0.0029, 0.0032, 0.0092, 0.0119, 0.0151, 0.0190, 0.0190],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 78.600, time 0.66
 * Lower 1 Val Acc 77.850, time 0.68
 * Upper 1 Val Acc 77.850, time 0.68
validation split name: 2
 *  Val Acc 78.600, time 0.68
 * Lower 1 Val Acc 77.000, time 0.66
 * Upper 1 Val Acc 77.000, time 0.66
validation split name: 3
 *  Val Acc 75.450, time 0.66
 * Lower 1 Val Acc 75.000, time 0.70
 * Upper 1 Val Acc 75.000, time 0.67
validation split name: 4
 *  Val Acc 80.600, time 0.67
 * Lower 1 Val Acc 80.600, time 0.68
 * Upper 1 Val Acc 80.600, time 0.71
validation split name: 5
 *  Val Acc 83.500, time 0.67
 * Lower 1 Val Acc 83.850, time 0.66
 * Upper 1 Val Acc 83.850, time 0.69
Task 1 average acc: 98.1
Task 2 average acc: 83.94999999999999
Task 3 average acc: 79.11666666666667
Task 4 average acc: 80.30000000000001
Task 5 average acc: 79.35
===Summary of experiment repeats: 4 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35  0.    0.    0.    0.    0.    0.  ]
mean: 31.503999999999998 std: 38.608073818827066
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.860, Loss 0.554
 * , robust loss: 0.170 robust error: 0.01000000
 *  Val Acc 82.000, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 81.190, Loss 0.406
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 84.730, Loss 0.338
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.69
Epoch:3
LR: 0.001
 * Train Acc 87.130, Loss 0.300
 * , robust loss: 0.209 robust error: 0.01000000
 *  Val Acc 88.900, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 88.470, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.250, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 90.470, Loss 0.214
 * , robust loss: 0.252 robust error: 0.02000000
 *  Val Acc 92.450, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 91.880, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.700, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 92.280, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.600, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 93.890, Loss 0.134
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.200, time 0.72
Epoch:9
LR: 0.001
 * Train Acc 94.370, Loss 0.133
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.100, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 95.270, Loss 0.110
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.66
Epoch:11
LR: 0.001
 * Train Acc 95.120, Loss 0.672
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 94.910, Loss 0.104
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 95.850, Loss 0.092
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.66
Epoch:14
LR: 0.001
 * Train Acc 96.120, Loss 0.093
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 96.530, Loss 0.089
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 96.690, Loss 0.066
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 96.510, Loss 0.097
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 97.210, Loss 0.078
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 97.120, Loss 0.114
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 97.290, Loss 0.046
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 97.330, Loss 0.045
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.66
Epoch:22
LR: 0.001
 * Train Acc 97.460, Loss 0.050
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 97.700, Loss 11.237
 * , robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 97.250, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 97.460, Loss 0.566
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 97.730, Loss 2.041
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 97.800, Loss 10.855
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 97.800, Loss 45.987
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 97.620, Loss 0.033
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.000, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 97.940, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 97.980, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 97.910, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 98.260, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 97.980, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 97.900, Loss 50.195
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.600, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 97.820, Loss 7.884
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 97.650, Loss 0.029
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 98.080, Loss 2.629
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 98.460, Loss 0.021
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 98.350, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 98.280, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.66
Epoch:41
LR: 0.001
 * Train Acc 98.480, Loss 0.482
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 98.000, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 98.500, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 98.350, Loss 197.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.400, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 98.340, Loss 0.363
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 98.330, Loss 0.021
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 98.700, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 98.660, Loss 0.056
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.350, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 98.540, Loss 98.791
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.68
after batch eps: 2.500000000000002, kappa: 0.5
sum: 16.618511199951172 - mean: 0.019234387204051018 - std: 0.006105565000325441
 * min 0.011997583322227001, max: 0.03737737238407135
sum: 314.2496337890625 - mean: 0.034098267555236816 - std: 0.014539786614477634
 * min 0.012521462514996529, max: 0.08391191065311432
sum: 206.94839477539062 - mean: 0.011227669194340706 - std: 0.0037339383270591497
 * min 0.004148550797253847, max: 0.023766761645674706
sum: 519.0517578125 - mean: 0.01408017985522747 - std: 0.003582174424082041
 * min 0.005447109695523977, max: 0.0288167092949152
sum: 2289.3544921875 - mean: 0.031051358208060265 - std: 0.008200448006391525
 * min 0.011440270580351353, max: 0.0696822851896286
sum: 4785.85205078125 - mean: 0.032456137239933014 - std: 0.006185147445648909
 * min 0.013249783776700497, max: 0.07408192753791809
sum: 5987.87646484375 - mean: 0.040607888251543045 - std: 0.006611788645386696
 * min 0.015840845182538033, max: 0.0883902758359909
sum: 109.45240783691406 - mean: 0.00013360888988245279 - std: 8.605200036981842e-07
 * min 0.00011175642430316657, max: 0.00013787960051558912
sum: 5.0 - mean: 0.009765625 - std: 0.0037810232024639845
 * min 0.003249296685680747, max: 0.033350709825754166
eps: tensor([0.1731, 0.3069, 0.1010, 0.1267, 0.2795, 0.2921, 0.3655, 0.4275, 0.4276],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.050, time 0.67
 * Lower 1 Val Acc 54.000, time 0.68
 * Upper 1 Val Acc 54.000, time 0.68
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.630, Loss 0.584
 * , robust loss: 0.474 robust error: 0.01000000
 *  Val Acc 76.900, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 78.370, Loss 0.455
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.400, time 0.69
Epoch:2
LR: 0.001
 * Train Acc 80.010, Loss 0.425
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.350, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 80.400, Loss 0.410
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 81.690, Loss 0.376
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 82.350, Loss 0.361
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 83.200, Loss 0.340
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 82.870, Loss 0.336
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.880, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.69
Epoch:9
LR: 0.001
 * Train Acc 83.470, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 83.700, Loss 0.368
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 83.980, Loss 0.297
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.560, Loss 0.278
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 84.280, Loss 0.335
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 84.630, Loss 0.266
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 84.780, Loss 0.361
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.360, Loss 0.266
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 84.310, Loss 0.587
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 84.310, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 84.540, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.70
Epoch:20
LR: 0.001
 * Train Acc 84.710, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 85.020, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 84.720, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 84.310, Loss 1.084
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 84.640, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 84.900, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.69
Epoch:26
LR: 0.001
 * Train Acc 85.440, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 85.260, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 85.010, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 85.270, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.69
Epoch:30
LR: 0.001
 * Train Acc 84.600, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 84.860, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 84.790, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 85.150, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 84.840, Loss 0.171
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.66
Epoch:35
LR: 0.001
 * Train Acc 85.050, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 84.220, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 84.700, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 84.730, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 84.490, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 84.390, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.71
Epoch:41
LR: 0.001
 * Train Acc 84.710, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 84.710, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.66
Epoch:43
LR: 0.001
 * Train Acc 84.090, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 84.600, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 84.420, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 83.970, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 83.950, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 84.250, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 83.480, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.71
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 5.362374305725098 - mean: 0.006206451915204525 - std: 0.002485601929947734
 * min 0.0033356579951941967, max: 0.013913200236856937
sum: 132.22705078125 - mean: 0.014347553253173828 - std: 0.007256512530148029
 * min 0.004303597845137119, max: 0.042580850422382355
sum: 64.62002563476562 - mean: 0.00350586068816483 - std: 0.001305143116042018
 * min 0.0011425900738686323, max: 0.008501687087118626
sum: 160.70230102539062 - mean: 0.004359329119324684 - std: 0.001191030372865498
 * min 0.0016100022476166487, max: 0.010040873661637306
sum: 730.5089111328125 - mean: 0.009908161126077175 - std: 0.002764811972156167
 * min 0.003409620141610503, max: 0.024558277800679207
sum: 1690.375244140625 - mean: 0.011463590897619724 - std: 0.002304994035512209
 * min 0.00432772608473897, max: 0.026882968842983246
sum: 2195.412841796875 - mean: 0.014888596720993519 - std: 0.002572172088548541
 * min 0.005299657583236694, max: 0.034672994166612625
sum: 40.68426513671875 - mean: 4.9663409299682826e-05 - std: 3.3144220878966735e-07
 * min 4.1540472011547536e-05, max: 5.130064528202638e-05
sum: 1.8000000715255737 - mean: 0.003515625139698386 - std: 0.00042357906932011247
 * min 0.002283582231029868, max: 0.005430209916085005
eps: tensor([0.0559, 0.1291, 0.0316, 0.0392, 0.0892, 0.1032, 0.1340, 0.1589, 0.1590],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.350, time 0.67
 * Lower 1 Val Acc 77.800, time 0.68
 * Upper 1 Val Acc 77.800, time 0.67
validation split name: 2
 *  Val Acc 80.550, time 0.66
 * Lower 1 Val Acc 61.300, time 0.70
 * Upper 1 Val Acc 61.300, time 0.69
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.230, Loss 0.450
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 84.060, Loss 0.360
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 83.750, Loss 0.357
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 84.590, Loss 0.342
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 85.020, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 85.330, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 85.330, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 85.480, Loss 0.301
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 85.670, Loss 0.288
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 85.680, Loss 0.284
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.70
Epoch:10
LR: 0.001
 * Train Acc 85.120, Loss 0.286
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 85.420, Loss 0.273
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 85.900, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 85.970, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 85.100, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 85.230, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 85.640, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 86.020, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 85.960, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 85.480, Loss 0.229
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 85.500, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 85.800, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 85.520, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 86.030, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 85.300, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.70
Epoch:25
LR: 0.001
 * Train Acc 85.320, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 85.850, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.72
Epoch:27
LR: 0.001
 * Train Acc 85.300, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 85.270, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 85.710, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 85.640, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 85.510, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 85.260, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.69
Epoch:33
LR: 0.001
 * Train Acc 85.040, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 84.980, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 84.980, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 84.850, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 84.950, Loss 0.178
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 84.860, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.70
Epoch:39
LR: 0.001
 * Train Acc 84.780, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.66
Epoch:40
LR: 0.001
 * Train Acc 84.710, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 84.310, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 84.560, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.66
Epoch:43
LR: 0.001
 * Train Acc 84.090, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.66
Epoch:44
LR: 0.001
 * Train Acc 84.350, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 84.630, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.66
Epoch:46
LR: 0.001
 * Train Acc 84.180, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 84.050, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 84.300, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.66
Epoch:49
LR: 0.001
 * Train Acc 84.250, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.66
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.8644238710403442 - mean: 0.0021578979212790728 - std: 0.0008887658477760851
 * min 0.0011396799236536026, max: 0.004931468050926924
sum: 43.0833740234375 - mean: 0.00467484537512064 - std: 0.002423948608338833
 * min 0.0013591409660875797, max: 0.014142696745693684
sum: 21.147750854492188 - mean: 0.0011473388876765966 - std: 0.00043360047857277095
 * min 0.00036848356830887496, max: 0.0028834992554038763
sum: 53.326045989990234 - mean: 0.0014465615386143327 - std: 0.00039840745739638805
 * min 0.0005322786164470017, max: 0.0033544248435646296
sum: 242.0777587890625 - mean: 0.0032833898440003395 - std: 0.0009211630676873028
 * min 0.0011220619780942798, max: 0.00824508536607027
sum: 560.3927001953125 - mean: 0.003800406353548169 - std: 0.0007666278979741037
 * min 0.0014321361668407917, max: 0.00893350224941969
sum: 731.2559204101562 - mean: 0.0049591464921832085 - std: 0.0008602673769928515
 * min 0.0017626355402171612, max: 0.01157443318516016
sum: 13.665253639221191 - mean: 1.6681216948200017e-05 - std: 1.1134039112903338e-07
 * min 1.3952840163256042e-05, max: 1.723116474749986e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 8.031519428186584e-06
 * min 0.0011190504301339388, max: 0.0012271995656192303
eps: tensor([0.0194, 0.0421, 0.0103, 0.0130, 0.0296, 0.0342, 0.0446, 0.0534, 0.0534],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.900, time 0.67
 * Lower 1 Val Acc 85.700, time 0.69
 * Upper 1 Val Acc 85.700, time 0.68
validation split name: 2
 *  Val Acc 77.600, time 0.66
 * Lower 1 Val Acc 76.450, time 0.65
 * Upper 1 Val Acc 76.450, time 0.65
validation split name: 3
 *  Val Acc 83.450, time 0.69
 * Lower 1 Val Acc 82.000, time 0.64
 * Upper 1 Val Acc 82.000, time 0.70
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 72.410, Loss 0.512
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.71
Epoch:1
LR: 0.001
 * Train Acc 81.130, Loss 0.408
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 83.650, Loss 0.367
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.750, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 83.970, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 84.960, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 84.190, Loss 0.325
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.350, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 84.840, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 84.520, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.500, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 84.960, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 84.350, Loss 0.308
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 83.980, Loss 0.303
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.750, time 0.66
Epoch:11
LR: 0.001
 * Train Acc 85.230, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.160, Loss 0.286
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.70
Epoch:13
LR: 0.001
 * Train Acc 84.460, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 84.330, Loss 0.273
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 84.270, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.720, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 83.940, Loss 0.263
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.65
Epoch:18
LR: 0.001
 * Train Acc 83.890, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 83.620, Loss 0.252
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 84.360, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 83.170, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.70
Epoch:22
LR: 0.001
 * Train Acc 83.520, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 83.770, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 83.630, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 83.150, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 83.630, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 83.630, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 82.970, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 83.390, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 83.190, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 83.020, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.70
Epoch:32
LR: 0.001
 * Train Acc 83.250, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 82.550, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 82.290, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 81.970, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 82.670, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 82.250, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 81.950, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.70
Epoch:39
LR: 0.001
 * Train Acc 81.970, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 81.770, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 82.070, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 81.780, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 81.410, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 80.880, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 81.380, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 81.260, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 81.360, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 80.800, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 80.150, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.70
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.2484933137893677 - mean: 0.0014450154267251492 - std: 0.0005964575684629381
 * min 0.0007623420679010451, max: 0.0033065034076571465
sum: 28.816265106201172 - mean: 0.003126764902845025 - std: 0.0016238460084423423
 * min 0.0009074751869775355, max: 0.009473242796957493
sum: 14.129405975341797 - mean: 0.0007665693410672247 - std: 0.0002899745886679739
 * min 0.00024586767540313303, max: 0.0019291656790301204
sum: 35.62913513183594 - mean: 0.0009665021789260209 - std: 0.0002662598271854222
 * min 0.00035558309173211455, max: 0.0022419430315494537
sum: 161.43218994140625 - mean: 0.002189564285799861 - std: 0.0006143483333289623
 * min 0.0007482336368411779, max: 0.005501447711139917
sum: 372.1128845214844 - mean: 0.0025235519278794527 - std: 0.0005090687773190439
 * min 0.0009509566589258611, max: 0.005932409316301346
sum: 486.0008544921875 - mean: 0.0032959042582660913 - std: 0.0005717480089515448
 * min 0.0011714666616171598, max: 0.0076927123591303825
sum: 9.109190940856934 - mean: 1.1119617738586385e-05 - std: 7.421896697223929e-08
 * min 9.300894816988148e-06, max: 1.1486210496514104e-05
sum: 0.3999999761581421 - mean: 0.0007812499534338713 - std: 1.2344634114924702e-07
 * min 0.0007805374916642904, max: 0.0007819632883183658
eps: tensor([0.0130, 0.0281, 0.0069, 0.0087, 0.0197, 0.0227, 0.0297, 0.0356, 0.0356],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.550, time 0.68
 * Lower 1 Val Acc 85.600, time 0.66
 * Upper 1 Val Acc 85.600, time 0.65
validation split name: 2
 *  Val Acc 78.250, time 0.67
 * Lower 1 Val Acc 76.150, time 0.64
 * Upper 1 Val Acc 76.150, time 0.67
validation split name: 3
 *  Val Acc 84.550, time 0.65
 * Lower 1 Val Acc 81.350, time 0.67
 * Upper 1 Val Acc 81.350, time 0.68
validation split name: 4
 *  Val Acc 81.650, time 0.69
 * Lower 1 Val Acc 79.550, time 0.69
 * Upper 1 Val Acc 79.550, time 0.68
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.540, Loss 0.462
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 82.060, Loss 0.395
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 82.140, Loss 0.389
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.66
Epoch:3
LR: 0.001
 * Train Acc 82.340, Loss 0.376
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.70
Epoch:4
LR: 0.001
 * Train Acc 83.200, Loss 0.362
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 82.420, Loss 0.359
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:6
LR: 0.001
 * Train Acc 82.230, Loss 0.354
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 82.900, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 82.430, Loss 0.341
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.700, time 0.70
Epoch:9
LR: 0.001
 * Train Acc 83.190, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 82.210, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.69
Epoch:11
LR: 0.001
 * Train Acc 82.420, Loss 0.320
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 82.380, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 83.040, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 82.680, Loss 0.297
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.66
Epoch:15
LR: 0.001
 * Train Acc 81.950, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 82.350, Loss 0.287
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 82.400, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 82.550, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 82.490, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 82.470, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 81.980, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 82.610, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 82.460, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 82.280, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 82.000, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 82.020, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 82.190, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 82.690, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 81.870, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.72
Epoch:30
LR: 0.001
 * Train Acc 81.550, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 81.790, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 81.590, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 82.040, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 81.840, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 81.190, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 81.490, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 81.450, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 81.480, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.70
Epoch:39
LR: 0.001
 * Train Acc 81.460, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 81.240, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 81.050, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.500, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 80.790, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.66
Epoch:43
LR: 0.001
 * Train Acc 81.090, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 80.400, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.66
Epoch:45
LR: 0.001
 * Train Acc 81.140, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 81.180, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 80.700, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 80.570, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 80.510, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.950, time 0.66
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.6583847999572754 - mean: 0.0007620194228366017 - std: 0.00040707437437959015
 * min 0.0003147606330458075, max: 0.002063289051875472
sum: 14.583344459533691 - mean: 0.0015823941212147474 - std: 0.0010158626828342676
 * min 0.00034080882323905826, max: 0.005832837428897619
sum: 5.23853874206543 - mean: 0.00028420891612768173 - std: 0.00012093641998944804
 * min 6.973793642828241e-05, max: 0.0008869953453540802
sum: 13.423295974731445 - mean: 0.00036413021734915674 - std: 0.00010623895650496706
 * min 0.0001142294640885666, max: 0.0009149914258159697
sum: 59.279579162597656 - mean: 0.0008040307438932359 - std: 0.00023404009698424488
 * min 0.00022884660575073212, max: 0.002264769747853279
sum: 172.79449462890625 - mean: 0.0011718376772478223 - std: 0.00024307621060870588
 * min 0.00039491255301982164, max: 0.003044301178306341
sum: 257.7262268066406 - mean: 0.0017478178488090634 - std: 0.0003134195867460221
 * min 0.0005619389121420681, max: 0.004406174644827843
sum: 5.061723232269287 - mean: 6.178861440275796e-06 - std: 4.125702801616171e-08
 * min 5.168247753317701e-06, max: 6.382723313436145e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 2.288894211233128e-05
 * min 0.0003180575731676072, max: 0.0004829904646612704
eps: tensor([0.0069, 0.0142, 0.0026, 0.0033, 0.0072, 0.0105, 0.0157, 0.0198, 0.0198],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.500, time 0.67
 * Lower 1 Val Acc 86.000, time 0.69
 * Upper 1 Val Acc 86.000, time 0.67
validation split name: 2
 *  Val Acc 80.300, time 0.66
 * Lower 1 Val Acc 78.850, time 0.70
 * Upper 1 Val Acc 78.850, time 0.68
validation split name: 3
 *  Val Acc 82.700, time 0.64
 * Lower 1 Val Acc 80.200, time 0.68
 * Upper 1 Val Acc 80.200, time 0.67
validation split name: 4
 *  Val Acc 77.400, time 0.68
 * Lower 1 Val Acc 77.300, time 0.68
 * Upper 1 Val Acc 77.300, time 0.67
validation split name: 5
 *  Val Acc 77.950, time 0.67
 * Lower 1 Val Acc 77.350, time 0.68
 * Upper 1 Val Acc 77.350, time 0.67
Task 1 average acc: 98.05
Task 2 average acc: 84.44999999999999
Task 3 average acc: 83.31666666666666
Task 4 average acc: 83.0
Task 5 average acc: 80.97
===Summary of experiment repeats: 5 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35 80.97  0.    0.    0.    0.    0.  ]
mean: 39.601 std: 39.62903315752227
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 65.210, Loss 0.598
 * , robust loss: 0.122 robust error: 0.02000000
 *  Val Acc 78.800, time 0.66
Epoch:1
LR: 0.001
 * Train Acc 78.790, Loss 0.445
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 83.900, Loss 0.357
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 86.250, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 86.550, Loss 0.306
 * , robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 88.950, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 88.780, Loss 0.258
 * , robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 89.550, time 0.69
Epoch:5
LR: 0.001
 * Train Acc 89.540, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.300, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 90.720, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.300, time 0.71
Epoch:7
LR: 0.001
 * Train Acc 92.130, Loss 0.264
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.950, time 0.70
Epoch:8
LR: 0.001
 * Train Acc 92.210, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.450, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 93.230, Loss 0.145
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.950, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 94.150, Loss 0.126
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.300, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 94.290, Loss 0.142
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.550, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 95.310, Loss 0.165
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 95.800, time 0.71
Epoch:13
LR: 0.001
 * Train Acc 95.430, Loss 0.335
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.400, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 95.730, Loss 0.097
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 96.030, Loss 0.079
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.400, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 95.970, Loss 0.165
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.600, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 96.380, Loss 0.067
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 96.250, Loss 76.371
 * , robust loss: 0.104 robust error: 0.00000000
 *  Val Acc 97.150, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 96.140, Loss 1.242
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.650, time 0.70
Epoch:20
LR: 0.001
 * Train Acc 96.610, Loss 0.061
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.650, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 96.760, Loss 3.006
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.850, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 97.160, Loss 0.046
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.70
Epoch:23
LR: 0.001
 * Train Acc 97.090, Loss 0.048
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 97.290, Loss 0.043
 * , robust loss: 0.381 robust error: 0.01000000
 *  Val Acc 97.050, time 0.70
Epoch:25
LR: 0.001
 * Train Acc 97.340, Loss 0.072
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.000, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 97.360, Loss 0.600
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.70
Epoch:27
LR: 0.001
 * Train Acc 97.930, Loss 0.342
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.400, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 97.900, Loss 0.064
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 97.620, Loss 1.634
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.70
Epoch:30
LR: 0.001
 * Train Acc 97.820, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.300, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 97.810, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.70
Epoch:32
LR: 0.001
 * Train Acc 97.870, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 97.930, Loss 0.135
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.66
Epoch:34
LR: 0.001
 * Train Acc 97.910, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 97.860, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 97.870, Loss 1.230
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.350, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 98.110, Loss 73.866
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.66
Epoch:38
LR: 0.001
 * Train Acc 97.950, Loss 13.561
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 98.140, Loss 0.500
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 98.290, Loss 0.024
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.900, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 98.210, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 98.260, Loss 0.025
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 98.150, Loss 3.756
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 97.830, Loss 2.075
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 98.070, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.400, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 98.200, Loss 1.721
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 98.230, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 98.320, Loss 0.079
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.150, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 98.450, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.68
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.612138748168945 - mean: 0.015754790976643562 - std: 0.005270485766232014
 * min 0.009628773666918278, max: 0.030864715576171875
sum: 308.576904296875 - mean: 0.033482737839221954 - std: 0.014663546346127987
 * min 0.012528135441243649, max: 0.07717686891555786
sum: 193.21009826660156 - mean: 0.010482318699359894 - std: 0.0036758820060640574
 * min 0.00370487361215055, max: 0.023165268823504448
sum: 456.42730712890625 - mean: 0.012381383217871189 - std: 0.002877534832805395
 * min 0.0038720532320439816, max: 0.02742588520050049
sum: 2188.3583984375 - mean: 0.029681511223316193 - std: 0.007318083196878433
 * min 0.00971229001879692, max: 0.06891394406557083
sum: 4673.49658203125 - mean: 0.03169417753815651 - std: 0.006664555985480547
 * min 0.012029014527797699, max: 0.07509665191173553
sum: 5991.16796875 - mean: 0.04063021019101143 - std: 0.007610590662807226
 * min 0.013307824730873108, max: 0.09841766953468323
sum: 119.41249084472656 - mean: 0.00014576720423065126 - std: 1.0642702363838907e-06
 * min 0.00011674269626382738, max: 0.00015322465333156288
sum: 5.0 - mean: 0.009765625 - std: 0.0056168269366025925
 * min 0.002616744488477707, max: 0.040146373212337494
eps: tensor([0.1418, 0.3013, 0.0943, 0.1114, 0.2671, 0.2852, 0.3657, 0.4665, 0.4666],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.000, time 0.68
 * Lower 1 Val Acc 62.550, time 0.70
 * Upper 1 Val Acc 62.550, time 0.64
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.840, Loss 0.586
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.550, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 77.000, Loss 0.476
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.350, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 79.390, Loss 0.433
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 80.770, Loss 0.407
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 81.170, Loss 0.391
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 82.030, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 82.300, Loss 0.363
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.70
Epoch:7
LR: 0.001
 * Train Acc 82.250, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.130, Loss 0.332
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.69
Epoch:9
LR: 0.001
 * Train Acc 83.190, Loss 0.328
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 83.370, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 83.860, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 83.060, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 83.760, Loss 0.578
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 83.600, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 84.000, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.520, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 84.070, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 83.400, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 83.740, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 84.130, Loss 0.267
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 84.480, Loss 0.728
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 83.570, Loss 3.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 84.480, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 84.540, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 84.240, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.66
Epoch:26
LR: 0.001
 * Train Acc 84.160, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 84.520, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 84.700, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 84.090, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 84.720, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 84.570, Loss 0.178
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 84.200, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 83.930, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 83.730, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 83.850, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 84.120, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 84.380, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 84.030, Loss 0.180
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 83.250, time 0.66
Epoch:39
LR: 0.001
 * Train Acc 83.830, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 83.700, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 83.490, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 83.840, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 83.290, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 83.630, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 83.010, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 83.690, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 83.860, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.66
Epoch:48
LR: 0.001
 * Train Acc 83.420, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 82.600, Loss 4.409
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 83.000, time 0.68
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.830190658569336 - mean: 0.005590498447418213 - std: 0.0026058556977659464
 * min 0.0027665963862091303, max: 0.013439464382827282
sum: 119.87944030761719 - mean: 0.013007751666009426 - std: 0.007154910825192928
 * min 0.003928591497242451, max: 0.03601882606744766
sum: 56.05493927001953 - mean: 0.0030411751940846443 - std: 0.0012581973569467664
 * min 0.0009239240898750722, max: 0.007566433399915695
sum: 131.2808380126953 - mean: 0.003561220597475767 - std: 0.0009187526884488761
 * min 0.001050326507538557, max: 0.008999384008347988
sum: 665.6032104492188 - mean: 0.009027821011841297 - std: 0.0023925111163407564
 * min 0.0024575190618634224, max: 0.023050859570503235
sum: 1628.017333984375 - mean: 0.011040699668228626 - std: 0.0024831194896250963
 * min 0.003834846429526806, max: 0.028459232300519943
sum: 2227.36962890625 - mean: 0.01510531734675169 - std: 0.0030628363601863384
 * min 0.004709974396973848, max: 0.03971424326300621
sum: 45.640602111816406 - mean: 5.571362271439284e-05 - std: 4.149698611399799e-07
 * min 4.457376780919731e-05, max: 5.860958117409609e-05
sum: 1.799999713897705 - mean: 0.0035156244412064552 - std: 0.000358476274413988
 * min 0.0025714479852467775, max: 0.004852154292166233
eps: tensor([0.0503, 0.1171, 0.0274, 0.0321, 0.0813, 0.0994, 0.1359, 0.1783, 0.1783],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 91.050, time 0.67
 * Lower 1 Val Acc 77.800, time 0.68
 * Upper 1 Val Acc 77.800, time 0.69
validation split name: 2
 *  Val Acc 83.000, time 0.68
 * Lower 1 Val Acc 71.500, time 0.67
 * Upper 1 Val Acc 71.500, time 0.68
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.050, Loss 0.466
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 82.600, Loss 0.388
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 83.430, Loss 0.370
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 82.930, Loss 0.365
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 83.530, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 83.490, Loss 0.339
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 83.930, Loss 0.344
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 84.010, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 84.520, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.66
Epoch:9
LR: 0.001
 * Train Acc 85.090, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.71
Epoch:10
LR: 0.001
 * Train Acc 84.030, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 84.700, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 83.940, Loss 0.293
 * , robust loss: 0.052 robust error: 0.01000000
 *  Val Acc 84.850, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 84.480, Loss 0.282
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 84.060, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 84.810, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.360, Loss 0.278
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 84.410, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.66
Epoch:18
LR: 0.001
 * Train Acc 83.920, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 84.110, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.70
Epoch:20
LR: 0.001
 * Train Acc 84.790, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 84.500, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 84.530, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 83.920, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 84.110, Loss 0.214
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.200, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 83.920, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 84.070, Loss 0.329
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 83.840, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 83.830, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.70
Epoch:29
LR: 0.001
 * Train Acc 84.210, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 84.130, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 83.490, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 83.600, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 83.780, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.71
Epoch:34
LR: 0.001
 * Train Acc 83.800, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 83.760, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 83.350, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 83.320, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 83.800, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 83.130, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 83.680, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 83.450, Loss 0.192
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 83.630, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 83.370, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 83.010, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 83.310, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.65
Epoch:46
LR: 0.001
 * Train Acc 83.350, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 83.300, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 83.330, Loss 0.239
 * , robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 82.100, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 82.510, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.69
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.4975119829177856 - mean: 0.0017332314746454358 - std: 0.0012347253505140543
 * min 0.0006707591819576919, max: 0.005637423135340214
sum: 38.434532165527344 - mean: 0.0041704135946929455 - std: 0.003105627838522196
 * min 0.0008143690065480769, max: 0.016961533576250076
sum: 12.83404541015625 - mean: 0.0006962915067560971 - std: 0.000365885702194646
 * min 0.0001343899202765897, max: 0.002430291846394539
sum: 32.02486038208008 - mean: 0.0008687299559824169 - std: 0.00026084831915795803
 * min 0.00019801349844783545, max: 0.0030083958990871906
sum: 171.70538330078125 - mean: 0.002328903414309025 - std: 0.0006969206151552498
 * min 0.0005817767232656479, max: 0.007574031129479408
sum: 545.3538818359375 - mean: 0.0036984176840633154 - std: 0.000930187467020005
 * min 0.0009907304774969816, max: 0.011199859902262688
sum: 795.9033203125 - mean: 0.005397564731538296 - std: 0.0012389030307531357
 * min 0.0014499545795843005, max: 0.01660802960395813
sum: 16.631187438964844 - mean: 2.0301742551964708e-05 - std: 1.5652030072033085e-07
 * min 1.614167558727786e-05, max: 2.137145202141255e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0001226416788995266
 * min 0.0007880293414928019, max: 0.0017485985299572349
eps: tensor([0.0156, 0.0375, 0.0063, 0.0078, 0.0210, 0.0333, 0.0486, 0.0650, 0.0650],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.400, time 0.67
 * Lower 1 Val Acc 83.150, time 0.70
 * Upper 1 Val Acc 83.150, time 0.67
validation split name: 2
 *  Val Acc 77.450, time 0.68
 * Lower 1 Val Acc 76.350, time 0.69
 * Upper 1 Val Acc 76.350, time 0.67
validation split name: 3
 *  Val Acc 83.900, time 0.63
 * Lower 1 Val Acc 81.900, time 0.67
 * Upper 1 Val Acc 81.900, time 0.69
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.400, Loss 0.554
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 77.580, Loss 0.454
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 80.610, Loss 0.411
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 81.400, Loss 0.387
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.150, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 82.740, Loss 0.365
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.700, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 82.980, Loss 0.357
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.700, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 83.020, Loss 0.349
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.900, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 82.840, Loss 0.336
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.350, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 82.390, Loss 0.342
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.300, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 82.660, Loss 0.332
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 83.570, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.69
Epoch:11
LR: 0.001
 * Train Acc 83.130, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 83.020, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.750, time 0.70
Epoch:13
LR: 0.001
 * Train Acc 83.110, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 82.960, Loss 0.297
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.650, time 0.66
Epoch:15
LR: 0.001
 * Train Acc 82.300, Loss 0.292
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.900, time 0.63
Epoch:16
LR: 0.001
 * Train Acc 81.950, Loss 0.289
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 82.910, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 81.930, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 82.160, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 0.66
Epoch:20
LR: 0.001
 * Train Acc 82.600, Loss 0.266
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 82.210, Loss 0.256
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.450, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 82.420, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 82.460, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 81.240, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 81.520, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 81.610, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 81.720, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.000, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 81.580, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 81.440, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 81.140, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.150, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 80.850, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 81.240, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 81.130, Loss 0.557
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 81.260, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.66
Epoch:35
LR: 0.001
 * Train Acc 80.550, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.66
Epoch:36
LR: 0.001
 * Train Acc 80.230, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 80.350, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 80.670, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 80.130, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.70
Epoch:40
LR: 0.001
 * Train Acc 80.010, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 80.500, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 79.990, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 79.730, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 79.480, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 79.920, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 80.080, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 79.800, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 79.090, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 79.600, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.67
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 0.9746166467666626 - mean: 0.0011280284961685538 - std: 0.0008571941871196032
 * min 0.00041013205191120505, max: 0.003838759148493409
sum: 26.00653839111328 - mean: 0.002821889938786626 - std: 0.0022019310854375362
 * min 0.0005062706186436117, max: 0.011962144635617733
sum: 7.996471405029297 - mean: 0.0004338363360147923 - std: 0.0002352908777538687
 * min 7.944740355014801e-05, max: 0.0015658370684832335
sum: 20.098962783813477 - mean: 0.0005452192854136229 - std: 0.00016684256843291223
 * min 0.0001176683945232071, max: 0.001968305092304945
sum: 107.41868591308594 - mean: 0.0014569591730833054 - std: 0.0004416327574290335
 * min 0.0003483734035398811, max: 0.004898016341030598
sum: 359.9740295410156 - mean: 0.0024412302300333977 - std: 0.0006194653687998652
 * min 0.0006313890335150063, max: 0.00747210206463933
sum: 536.2037963867188 - mean: 0.0036363648250699043 - std: 0.0008454475901089609
 * min 0.0009683109819889069, max: 0.011314310133457184
sum: 11.239635467529297 - mean: 1.3720257811655756e-05 - std: 1.0591024590667075e-07
 * min 1.0908516742347274e-05, max: 1.44431860462646e-05
sum: 0.3999999761581421 - mean: 0.0007812499534338713 - std: 1.989315205719322e-05
 * min 0.0007135427440516651, max: 0.000855659949593246
eps: tensor([0.0102, 0.0254, 0.0039, 0.0049, 0.0131, 0.0220, 0.0327, 0.0439, 0.0439],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.550, time 0.68
 * Lower 1 Val Acc 80.550, time 0.67
 * Upper 1 Val Acc 80.550, time 0.70
validation split name: 2
 *  Val Acc 76.000, time 0.70
 * Lower 1 Val Acc 76.100, time 0.64
 * Upper 1 Val Acc 76.100, time 0.68
validation split name: 3
 *  Val Acc 81.450, time 0.68
 * Lower 1 Val Acc 78.900, time 0.69
 * Upper 1 Val Acc 78.900, time 0.69
validation split name: 4
 *  Val Acc 82.700, time 0.66
 * Lower 1 Val Acc 82.900, time 0.68
 * Upper 1 Val Acc 82.900, time 0.67
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 75.550, Loss 0.503
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 81.110, Loss 0.411
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 81.230, Loss 0.400
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 80.920, Loss 0.400
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 81.330, Loss 0.387
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 81.510, Loss 0.381
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 81.240, Loss 0.372
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 81.250, Loss 0.371
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 80.890, Loss 0.360
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.70
Epoch:9
LR: 0.001
 * Train Acc 81.500, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 81.130, Loss 0.349
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 81.400, Loss 0.341
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 81.820, Loss 0.328
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 81.580, Loss 0.322
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 81.300, Loss 0.319
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.66
Epoch:15
LR: 0.001
 * Train Acc 81.240, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 80.950, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.65
Epoch:17
LR: 0.001
 * Train Acc 81.400, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.66
Epoch:18
LR: 0.001
 * Train Acc 81.560, Loss 0.288
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 81.380, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 80.640, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 81.560, Loss 0.268
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 80.530, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.71
Epoch:23
LR: 0.001
 * Train Acc 80.600, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.66
Epoch:24
LR: 0.001
 * Train Acc 80.810, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.70
Epoch:25
LR: 0.001
 * Train Acc 80.720, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.69
Epoch:26
LR: 0.001
 * Train Acc 80.570, Loss 0.242
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 81.050, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 80.610, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 80.350, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 80.120, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 79.930, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 80.630, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 80.730, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 80.160, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 79.690, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.550, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 80.430, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 79.960, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.69
Epoch:38
LR: 0.001
 * Train Acc 80.620, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.66
Epoch:39
LR: 0.001
 * Train Acc 80.300, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 79.730, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 80.120, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 80.070, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.69
Epoch:43
LR: 0.001
 * Train Acc 79.900, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.70
Epoch:44
LR: 0.001
 * Train Acc 79.540, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 79.930, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 79.720, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 79.220, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 79.280, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 79.890, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.68
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.4885849952697754 - mean: 0.0005654919077642262 - std: 0.00043062548502348363
 * min 0.0002055327349808067, max: 0.001927583129145205
sum: 13.02676773071289 - mean: 0.0014134948141872883 - std: 0.0011045673163607717
 * min 0.00025307253235951066, max: 0.006001967471092939
sum: 4.002936363220215 - mean: 0.00021717320487368852 - std: 0.00011788798292400315
 * min 3.973926868638955e-05, max: 0.0007843522471375763
sum: 10.058366775512695 - mean: 0.00027285065152682364 - std: 8.353134035132825e-05
 * min 5.8859597629634663e-05, max: 0.0009856815449893475
sum: 53.733375549316406 - mean: 0.0007288055494427681 - std: 0.0002209531085100025
 * min 0.00017423965618945658, max: 0.0024506808258593082
sum: 179.53921508789062 - mean: 0.0012175781885161996 - std: 0.0003089745878241956
 * min 0.00031490225228480995, max: 0.003729279153048992
sum: 267.8895263671875 - mean: 0.0018167421221733093 - std: 0.00042239404865540564
 * min 0.0004837681772187352, max: 0.005652783438563347
sum: 5.619383335113525 - mean: 6.8595986704167444e-06 - std: 5.2951040885318434e-08
 * min 5.453836820379365e-06, max: 7.221035502880113e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 1.0439033104603368e-07
 * min 0.00039010896580293775, max: 0.0003911420644726604
eps: tensor([0.0051, 0.0127, 0.0020, 0.0025, 0.0066, 0.0110, 0.0164, 0.0220, 0.0220],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.650, time 0.68
 * Lower 1 Val Acc 86.050, time 0.68
 * Upper 1 Val Acc 86.050, time 0.69
validation split name: 2
 *  Val Acc 77.500, time 0.69
 * Lower 1 Val Acc 77.200, time 0.66
 * Upper 1 Val Acc 77.200, time 0.68
validation split name: 3
 *  Val Acc 79.750, time 0.68
 * Lower 1 Val Acc 79.200, time 0.68
 * Upper 1 Val Acc 79.200, time 0.68
validation split name: 4
 *  Val Acc 80.650, time 0.68
 * Lower 1 Val Acc 80.600, time 0.69
 * Upper 1 Val Acc 80.600, time 0.68
validation split name: 5
 *  Val Acc 79.350, time 0.67
 * Lower 1 Val Acc 80.100, time 0.68
 * Upper 1 Val Acc 80.100, time 0.71
Task 1 average acc: 98.0
Task 2 average acc: 87.025
Task 3 average acc: 82.58333333333334
Task 4 average acc: 80.425
Task 5 average acc: 80.58
===Summary of experiment repeats: 6 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35 80.97 80.58  0.    0.    0.    0.  ]
mean: 47.659 std: 38.94397038053516
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.050, Loss 0.593
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 78.360, Loss 0.450
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 82.050, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 82.750, Loss 0.380
 * , robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 85.600, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 85.300, Loss 0.325
 * , robust loss: 0.042 robust error: 0.01000000
 *  Val Acc 86.450, time 0.69
Epoch:4
LR: 0.001
 * Train Acc 86.580, Loss 0.295
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.500, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 88.480, Loss 0.255
 * , robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 89.950, time 0.66
Epoch:6
LR: 0.001
 * Train Acc 89.680, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.550, time 0.69
Epoch:7
LR: 0.001
 * Train Acc 90.420, Loss 0.217
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 91.100, time 0.65
Epoch:8
LR: 0.001
 * Train Acc 91.280, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.650, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 92.360, Loss 1.079
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 92.250, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 92.260, Loss 0.159
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.700, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 93.590, Loss 0.142
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.000, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 94.230, Loss 0.121
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.400, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 94.940, Loss 0.101
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.700, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 95.360, Loss 0.093
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 95.620, Loss 0.111
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 96.090, Loss 0.079
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 96.110, Loss 0.261
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 96.800, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 96.220, Loss 0.070
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.300, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 96.750, Loss 0.076
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.850, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 96.540, Loss 0.067
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.050, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 96.590, Loss 1.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.63
Epoch:22
LR: 0.001
 * Train Acc 96.700, Loss 0.054
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 93.850, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 96.710, Loss 0.048
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 96.910, Loss 0.053
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.850, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 97.100, Loss 1.395
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.70
Epoch:26
LR: 0.001
 * Train Acc 97.500, Loss 0.039
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 97.390, Loss 0.037
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 97.460, Loss 0.035
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.72
Epoch:29
LR: 0.001
 * Train Acc 97.740, Loss 0.032
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 97.870, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 97.740, Loss 0.415
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 97.760, Loss 6.961
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.69
Epoch:33
LR: 0.001
 * Train Acc 97.820, Loss 0.029
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 97.990, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 98.070, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.70
Epoch:36
LR: 0.001
 * Train Acc 98.030, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 98.170, Loss 0.520
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.69
Epoch:38
LR: 0.001
 * Train Acc 98.000, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 97.920, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 97.980, Loss 11.381
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 98.260, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 98.090, Loss 0.026
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 98.280, Loss 0.047
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 98.360, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 98.500, Loss 0.049
 * , robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 98.050, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 98.310, Loss 0.025
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.600, time 0.72
Epoch:47
LR: 0.001
 * Train Acc 98.070, Loss 3.029
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.100, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 98.360, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 98.490, Loss 0.021
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.500, time 0.67
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.793680191040039 - mean: 0.01596490852534771 - std: 0.005057961214333773
 * min 0.010168689303100109, max: 0.02983766607940197
sum: 297.8110046386719 - mean: 0.03231456130743027 - std: 0.013584494590759277
 * min 0.012756488285958767, max: 0.07770014554262161
sum: 198.0302734375 - mean: 0.010743830353021622 - std: 0.003386286785826087
 * min 0.004111699294298887, max: 0.023017141968011856
sum: 516.13818359375 - mean: 0.014001144096255302 - std: 0.003665556898340583
 * min 0.0051113334484398365, max: 0.03180750831961632
sum: 2124.338623046875 - mean: 0.028813187032938004 - std: 0.00792376883327961
 * min 0.01081898808479309, max: 0.0645531490445137
sum: 5092.5341796875 - mean: 0.034535959362983704 - std: 0.006508061196655035
 * min 0.011316226795315742, max: 0.08258850872516632
sum: 6145.53759765625 - mean: 0.041677094995975494 - std: 0.007751849014312029
 * min 0.014688156545162201, max: 0.09844738245010376
sum: 114.86778259277344 - mean: 0.00014021946117281914 - std: 9.298340728491894e-07
 * min 0.00010921671491814777, max: 0.00014826617552898824
sum: 5.0 - mean: 0.009765625 - std: 0.004861792083829641
 * min 0.002672740491107106, max: 0.04292288422584534
eps: tensor([0.1437, 0.2908, 0.0967, 0.1260, 0.2593, 0.3108, 0.3751, 0.4487, 0.4488],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.500, time 0.69
 * Lower 1 Val Acc 66.550, time 0.67
 * Upper 1 Val Acc 66.550, time 0.70
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.940, Loss 0.569
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.150, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 79.330, Loss 0.442
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 80.390, Loss 0.408
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.64
Epoch:3
LR: 0.001
 * Train Acc 82.290, Loss 0.381
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 82.050, Loss 0.370
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 83.090, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.67
Epoch:6
LR: 0.001
 * Train Acc 83.120, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 83.720, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.560, Loss 0.369
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 83.540, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 84.000, Loss 0.301
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 84.170, Loss 0.616
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 83.930, Loss 0.291
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 84.530, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 84.590, Loss 0.271
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 84.550, Loss 0.273
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 84.440, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 85.110, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 84.220, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.66
Epoch:19
LR: 0.001
 * Train Acc 84.950, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 85.350, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.69
Epoch:21
LR: 0.001
 * Train Acc 84.670, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.66
Epoch:22
LR: 0.001
 * Train Acc 84.750, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 85.140, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 84.750, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 84.800, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 84.190, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 85.000, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 85.330, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 85.220, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 84.370, Loss 6.013
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 84.430, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 84.460, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 84.670, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.70
Epoch:34
LR: 0.001
 * Train Acc 84.680, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 84.820, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 85.090, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 84.450, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 84.310, Loss 1.163
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 84.370, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 84.350, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 84.330, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 84.020, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 83.710, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.70
Epoch:44
LR: 0.001
 * Train Acc 84.130, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 83.920, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 84.050, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 83.590, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 83.620, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.70
Epoch:49
LR: 0.001
 * Train Acc 84.140, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.71
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.751642227172852 - mean: 0.005499585997313261 - std: 0.0025000914465636015
 * min 0.00312208803370595, max: 0.013073754496872425
sum: 112.0228271484375 - mean: 0.012155254371464252 - std: 0.006434519309550524
 * min 0.0036756861954927444, max: 0.03562585264444351
sum: 56.35963821411133 - mean: 0.003057706169784069 - std: 0.001123029156588018
 * min 0.0009092941763810813, max: 0.00744882645085454
sum: 143.0062255859375 - mean: 0.003879292169585824 - std: 0.0011197106214240193
 * min 0.0012424298329278827, max: 0.010606310330331326
sum: 652.7590942382812 - mean: 0.008853611536324024 - std: 0.002649591537192464
 * min 0.002779274946078658, max: 0.02194146253168583
sum: 1857.3359375 - mean: 0.01259586587548256 - std: 0.0025944060180336237
 * min 0.003632504492998123, max: 0.03239436075091362
sum: 2287.226318359375 - mean: 0.015511246398091316 - std: 0.0031970818527042866
 * min 0.004975512158125639, max: 0.0382159985601902
sum: 44.28109359741211 - mean: 5.405406773206778e-05 - std: 3.695705856898712e-07
 * min 4.18461459048558e-05, max: 5.7203804317396134e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.00026513327611610293
 * min 0.0025341096334159374, max: 0.004910706542432308
eps: tensor([0.0495, 0.1094, 0.0275, 0.0349, 0.0797, 0.1134, 0.1396, 0.1730, 0.1731],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 89.800, time 0.68
 * Lower 1 Val Acc 84.050, time 0.68
 * Upper 1 Val Acc 84.050, time 0.68
validation split name: 2
 *  Val Acc 83.250, time 0.68
 * Lower 1 Val Acc 66.950, time 0.63
 * Upper 1 Val Acc 66.950, time 0.66
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.270, Loss 0.441
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 82.920, Loss 0.380
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 83.530, Loss 0.365
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.71
Epoch:3
LR: 0.001
 * Train Acc 83.130, Loss 0.360
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 84.310, Loss 0.335
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 83.880, Loss 0.339
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 84.080, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 84.710, Loss 0.320
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 84.420, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.70
Epoch:9
LR: 0.001
 * Train Acc 84.430, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 83.710, Loss 0.305
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 84.420, Loss 0.291
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.670, Loss 0.287
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 84.440, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 84.860, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 84.140, Loss 0.270
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.70
Epoch:16
LR: 0.001
 * Train Acc 84.190, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 84.120, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 84.330, Loss 0.252
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 84.510, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 84.070, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 84.480, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 84.190, Loss 0.241
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 84.400, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 83.930, Loss 0.331
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 84.500, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 83.440, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 83.900, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 84.080, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 84.140, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 83.650, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 83.830, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 83.670, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 84.100, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 83.310, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 83.500, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 84.040, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.050, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 83.670, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 83.270, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 83.720, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 82.850, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 83.640, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 82.830, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 83.260, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:43
LR: 0.001
 * Train Acc 83.060, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 83.100, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 82.880, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.66
Epoch:46
LR: 0.001
 * Train Acc 82.770, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.65
Epoch:47
LR: 0.001
 * Train Acc 83.030, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 82.700, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 82.520, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.68
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.5176756381988525 - mean: 0.0017565690213814378 - std: 0.0008524184231646359
 * min 0.0009151435224339366, max: 0.00436013750731945
sum: 39.72257614135742 - mean: 0.004310175310820341 - std: 0.0024318762589246035
 * min 0.0012130317045375705, max: 0.013789212331175804
sum: 17.4891357421875 - mean: 0.0009488463401794434 - std: 0.00036465717130340636
 * min 0.0002670968242455274, max: 0.0023975688964128494
sum: 44.35700988769531 - mean: 0.0012032609665766358 - std: 0.00035511390888132155
 * min 0.0003751089097931981, max: 0.0033627033699303865
sum: 205.73301696777344 - mean: 0.002790432656183839 - std: 0.0008451223256997764
 * min 0.0008262153714895248, max: 0.007088664919137955
sum: 607.2653198242188 - mean: 0.004118281416594982 - std: 0.0008572284714318812
 * min 0.0011637089774012566, max: 0.010812472552061081
sum: 762.007568359375 - mean: 0.005167694762349129 - std: 0.001075361273251474
 * min 0.0016356554115191102, max: 0.012950783595442772
sum: 15.015983581542969 - mean: 1.8330058082938194e-05 - std: 1.253704340342665e-07
 * min 1.4190331967256498e-05, max: 1.939826506713871e-05
sum: 0.6000000834465027 - mean: 0.0011718751629814506 - std: 1.9649109162855893e-05
 * min 0.0010661068372428417, max: 0.001288583967834711
eps: tensor([0.0158, 0.0388, 0.0085, 0.0108, 0.0251, 0.0371, 0.0465, 0.0587, 0.0587],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.900, time 0.68
 * Lower 1 Val Acc 85.700, time 0.69
 * Upper 1 Val Acc 85.700, time 0.70
validation split name: 2
 *  Val Acc 74.550, time 0.70
 * Lower 1 Val Acc 75.050, time 0.69
 * Upper 1 Val Acc 75.050, time 0.68
validation split name: 3
 *  Val Acc 79.600, time 0.68
 * Lower 1 Val Acc 81.500, time 0.67
 * Upper 1 Val Acc 81.500, time 0.69
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 67.430, Loss 0.569
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 78.920, Loss 0.451
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 79.900, Loss 0.424
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 80.460, Loss 0.406
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.69
Epoch:4
LR: 0.001
 * Train Acc 80.560, Loss 0.396
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 80.890, Loss 0.386
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 81.570, Loss 0.372
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 81.350, Loss 0.367
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 81.260, Loss 0.362
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 81.060, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 80.850, Loss 0.352
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 81.700, Loss 0.335
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 81.270, Loss 0.334
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 80.990, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.64
Epoch:14
LR: 0.001
 * Train Acc 80.700, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 81.180, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 80.600, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 80.590, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 80.800, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 80.500, Loss 0.290
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 80.060, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 80.210, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 80.160, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 79.830, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 79.340, Loss 0.264
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 79.620, Loss 0.253
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 79.470, Loss 0.250
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 79.710, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 79.130, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 80.450, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 78.710, Loss 0.229
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 79.520, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 78.930, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.69
Epoch:33
LR: 0.001
 * Train Acc 78.930, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 78.750, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 78.620, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 78.190, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.71
Epoch:37
LR: 0.001
 * Train Acc 77.760, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 77.740, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 78.170, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 77.190, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.050, time 0.71
Epoch:41
LR: 0.001
 * Train Acc 76.820, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 77.430, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 76.820, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 76.720, Loss 0.242
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 76.550, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 77.330, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.70
Epoch:47
LR: 0.001
 * Train Acc 75.810, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.550, time 0.71
Epoch:48
LR: 0.001
 * Train Acc 76.240, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.150, time 0.66
Epoch:49
LR: 0.001
 * Train Acc 76.210, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.69
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0213375091552734 - mean: 0.0011821036459878087 - std: 0.0005758370389230549
 * min 0.0006140891928225756, max: 0.0029419169295579195
sum: 26.68242835998535 - mean: 0.002895228797569871 - std: 0.001638338784687221
 * min 0.000812062353361398, max: 0.009294123388826847
sum: 11.745308876037598 - mean: 0.0006372237694449723 - std: 0.00024529293295927346
 * min 0.000179059206857346, max: 0.001612145104445517
sum: 29.767946243286133 - mean: 0.0008075072546489537 - std: 0.00023840935318730772
 * min 0.00025162476231344044, max: 0.002258619526401162
sum: 137.66114807128906 - mean: 0.0018671487923711538 - std: 0.0005655521526932716
 * min 0.0005528187612071633, max: 0.004743154626339674
sum: 402.97271728515625 - mean: 0.0027328336145728827 - std: 0.0005688642268069088
 * min 0.0007722146110609174, max: 0.007176321931183338
sum: 505.20599365234375 - mean: 0.0034261473920196295 - std: 0.0007129654986783862
 * min 0.0010843907948583364, max: 0.008586377836763859
sum: 9.989803314208984 - mean: 1.2194584087410476e-05 - std: 8.340618506963438e-08
 * min 9.440514986636117e-06, max: 1.2905237781524193e-05
sum: 0.3999999761581421 - mean: 0.0007812499534338713 - std: 2.0376639042751776e-07
 * min 0.0007799708400852978, max: 0.000782532268203795
eps: tensor([0.0106, 0.0261, 0.0057, 0.0073, 0.0168, 0.0246, 0.0308, 0.0390, 0.0390],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 80.000, time 0.69
 * Lower 1 Val Acc 81.850, time 0.64
 * Upper 1 Val Acc 81.850, time 0.64
validation split name: 2
 *  Val Acc 77.600, time 0.64
 * Lower 1 Val Acc 76.750, time 0.68
 * Upper 1 Val Acc 76.750, time 0.64
validation split name: 3
 *  Val Acc 83.150, time 0.68
 * Lower 1 Val Acc 83.100, time 0.67
 * Upper 1 Val Acc 83.100, time 0.68
validation split name: 4
 *  Val Acc 78.100, time 0.66
 * Lower 1 Val Acc 75.750, time 0.70
 * Upper 1 Val Acc 75.750, time 0.69
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.970, Loss 0.431
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 83.400, Loss 0.366
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 83.560, Loss 0.368
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 83.590, Loss 0.358
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 83.900, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 83.620, Loss 0.340
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 83.330, Loss 0.333
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 83.900, Loss 0.324
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 83.400, Loss 0.322
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.66
Epoch:9
LR: 0.001
 * Train Acc 83.450, Loss 0.313
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 82.870, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.69
Epoch:11
LR: 0.001
 * Train Acc 83.650, Loss 0.300
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.65
Epoch:12
LR: 0.001
 * Train Acc 83.280, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.71
Epoch:13
LR: 0.001
 * Train Acc 83.300, Loss 0.294
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.66
Epoch:14
LR: 0.001
 * Train Acc 83.610, Loss 0.287
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 82.960, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 83.040, Loss 0.278
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 82.780, Loss 0.271
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 83.340, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 83.230, Loss 0.253
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 82.650, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 82.810, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 82.880, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 82.880, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 83.010, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 82.830, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.69
Epoch:26
LR: 0.001
 * Train Acc 82.810, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 83.310, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 82.560, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 82.320, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.69
Epoch:30
LR: 0.001
 * Train Acc 82.680, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 82.300, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 82.610, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 82.240, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.450, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 82.090, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 81.840, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 82.230, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.65
Epoch:37
LR: 0.001
 * Train Acc 82.320, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 82.080, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 82.260, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 81.850, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 82.380, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 81.610, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 81.210, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 82.250, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.550, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 81.700, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 81.180, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.70
Epoch:47
LR: 0.001
 * Train Acc 81.760, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 81.480, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 81.720, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.68
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5688930749893188 - mean: 0.0006584410439245403 - std: 0.0003537895099725574
 * min 0.00032328255474567413, max: 0.0018175956793129444
sum: 14.273284912109375 - mean: 0.0015487505588680506 - std: 0.0009589346591383219
 * min 0.00037916458677500486, max: 0.005486926529556513
sum: 5.637742042541504 - mean: 0.0003058670845348388 - std: 0.0001224348961841315
 * min 7.872485730331391e-05, max: 0.0008533857762813568
sum: 14.185332298278809 - mean: 0.00038480176590383053 - std: 0.00011500581604195759
 * min 0.000116820476250723, max: 0.001091623562388122
sum: 62.767791748046875 - mean: 0.0008513426873832941 - std: 0.0002598622813820839
 * min 0.00023819366469979286, max: 0.002252317266538739
sum: 185.6162872314453 - mean: 0.0012587910750880837 - std: 0.0002634785487316549
 * min 0.00034419706207700074, max: 0.0034239008091390133
sum: 249.93850708007812 - mean: 0.0016950039425864816 - std: 0.00035500270314514637
 * min 0.0005255280993878841, max: 0.00431231502443552
sum: 5.076433181762695 - mean: 6.196817594172899e-06 - std: 4.23939923166472e-08
 * min 4.797340807272121e-06, max: 6.558012501045596e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 6.7022174334852025e-06
 * min 0.00036333195748738945, max: 0.0004203326243441552
eps: tensor([0.0059, 0.0139, 0.0028, 0.0035, 0.0077, 0.0113, 0.0153, 0.0198, 0.0198],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.000, time 0.69
 * Lower 1 Val Acc 89.000, time 0.70
 * Upper 1 Val Acc 89.000, time 0.65
validation split name: 2
 *  Val Acc 76.550, time 0.69
 * Lower 1 Val Acc 76.650, time 0.67
 * Upper 1 Val Acc 76.650, time 0.64
validation split name: 3
 *  Val Acc 79.450, time 0.66
 * Lower 1 Val Acc 79.000, time 0.68
 * Upper 1 Val Acc 79.000, time 0.66
validation split name: 4
 *  Val Acc 76.000, time 0.68
 * Lower 1 Val Acc 74.250, time 0.68
 * Upper 1 Val Acc 74.250, time 0.68
validation split name: 5
 *  Val Acc 78.900, time 0.68
 * Lower 1 Val Acc 78.050, time 0.68
 * Upper 1 Val Acc 78.050, time 0.67
Task 1 average acc: 97.5
Task 2 average acc: 86.525
Task 3 average acc: 79.01666666666667
Task 4 average acc: 79.7125
Task 5 average acc: 79.78
===Summary of experiment repeats: 7 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35 80.97 80.58 79.78  0.    0.    0.  ]
mean: 55.637 std: 36.45575648645904
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 50.460, Loss 0.644
 * , robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 50.000, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 50.000, Loss 0.578
 * , robust loss: 0.028 robust error: 0.02000000
 *  Val Acc 50.000, time 0.71
Epoch:2
LR: 0.001
 * Train Acc 70.650, Loss 0.483
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 82.990, Loss 0.368
 * , robust loss: 0.015 robust error: 0.01000000
 *  Val Acc 84.550, time 0.70
Epoch:4
LR: 0.001
 * Train Acc 85.690, Loss 0.307
 * , robust loss: 0.055 robust error: 0.01000000
 *  Val Acc 88.150, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 87.950, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.700, time 0.66
Epoch:6
LR: 0.001
 * Train Acc 89.320, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.550, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 90.380, Loss 0.208
 * , robust loss: 0.623 robust error: 0.01000000
 *  Val Acc 91.900, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 91.810, Loss 0.234
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.150, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 93.030, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.300, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 94.160, Loss 0.131
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.400, time 0.66
Epoch:11
LR: 0.001
 * Train Acc 94.770, Loss 0.124
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.800, time 0.66
Epoch:12
LR: 0.001
 * Train Acc 95.270, Loss 0.148
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.900, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 95.540, Loss 0.090
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.600, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 95.980, Loss 3.053
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.400, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 95.860, Loss 0.107
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 96.800, time 0.66
Epoch:16
LR: 0.001
 * Train Acc 96.650, Loss 0.064
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.100, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 96.420, Loss 0.109
 * , robust loss: 1.465 robust error: 0.01000000
 *  Val Acc 96.850, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 96.630, Loss 0.095
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 97.020, Loss 0.055
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 97.030, Loss 0.777
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 96.830, Loss 0.131
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.200, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 97.220, Loss 6.292
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.100, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 96.880, Loss 0.080
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 97.470, Loss 0.045
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 97.210, Loss 0.043
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.69
Epoch:26
LR: 0.001
 * Train Acc 97.560, Loss 0.037
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.400, time 0.70
Epoch:27
LR: 0.001
 * Train Acc 97.650, Loss 0.108
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 97.680, Loss 0.032
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 97.840, Loss 0.035
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 97.990, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.70
Epoch:31
LR: 0.001
 * Train Acc 98.090, Loss 1.704
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.950, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 97.930, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.66
Epoch:33
LR: 0.001
 * Train Acc 98.090, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 97.860, Loss 0.161
 * , robust loss: 0.055 robust error: 0.00000000
 *  Val Acc 97.250, time 0.66
Epoch:35
LR: 0.001
 * Train Acc 98.160, Loss 1.453
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 98.160, Loss 3.084
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 98.130, Loss 0.025
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 98.180, Loss 8.897
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 98.190, Loss 0.025
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.850, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 98.190, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 97.950, Loss 0.061
 * , robust loss: 0.018 robust error: 0.00000000
 *  Val Acc 97.500, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 98.120, Loss 2.342
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.750, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 98.080, Loss 0.034
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.450, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 98.240, Loss 0.448
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 98.260, Loss 0.316
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 98.390, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 98.380, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 98.360, Loss 12.627
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.450, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 97.940, Loss 0.029
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.66
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.15954875946045 - mean: 0.01523095928132534 - std: 0.006942956708371639
 * min 0.007408355362713337, max: 0.034475721418857574
sum: 335.3326416015625 - mean: 0.0363859198987484 - std: 0.02050674520432949
 * min 0.009968157857656479, max: 0.10757960379123688
sum: 154.67703247070312 - mean: 0.008391765877604485 - std: 0.00349640054628253
 * min 0.0021957638673484325, max: 0.021430237218737602
sum: 399.7369384765625 - mean: 0.010843558236956596 - std: 0.0032387818209826946
 * min 0.0030847375746816397, max: 0.02685355581343174
sum: 1988.80908203125 - mean: 0.02697494998574257 - std: 0.007987271063029766
 * min 0.007595877628773451, max: 0.07762639969587326
sum: 4223.693359375 - mean: 0.02864375337958336 - std: 0.004330517258495092
 * min 0.0077568176202476025, max: 0.08217482268810272
sum: 5973.4794921875 - mean: 0.04051025211811066 - std: 0.0054794419556856155
 * min 0.010179603472352028, max: 0.10657714307308197
sum: 127.59867858886719 - mean: 0.00015576010628137738 - std: 1.101510406442685e-06
 * min 0.00011547472968231887, max: 0.00016908855468500406
sum: 5.0 - mean: 0.009765625 - std: 0.0033636323641985655
 * min 0.0025409571826457977, max: 0.062021445482969284
eps: tensor([0.1371, 0.3275, 0.0755, 0.0976, 0.2428, 0.2578, 0.3646, 0.4984, 0.4987],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.700, time 0.69
 * Lower 1 Val Acc 57.200, time 0.69
 * Upper 1 Val Acc 57.200, time 0.64
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 66.840, Loss 0.606
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.350, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 75.810, Loss 0.489
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.450, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 78.720, Loss 0.439
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.350, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 79.270, Loss 0.419
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.400, time 0.70
Epoch:4
LR: 0.001
 * Train Acc 79.540, Loss 0.407
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 80.260, Loss 0.388
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.66
Epoch:6
LR: 0.001
 * Train Acc 80.860, Loss 0.382
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 80.950, Loss 0.366
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 81.270, Loss 0.353
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 82.040, Loss 0.350
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 81.910, Loss 0.333
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.050, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 82.300, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.66
Epoch:12
LR: 0.001
 * Train Acc 81.880, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.550, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 82.010, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 82.220, Loss 0.325
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 82.210, Loss 0.297
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 82.000, Loss 0.291
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 82.290, Loss 0.279
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 82.800, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 82.740, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 82.640, Loss 0.258
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.69
Epoch:21
LR: 0.001
 * Train Acc 82.780, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 82.610, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 82.890, Loss 0.235
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 82.820, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 82.610, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 82.760, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 82.490, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 83.150, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 82.600, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 82.480, Loss 0.530
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 82.650, Loss 0.487
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 82.250, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.66
Epoch:33
LR: 0.001
 * Train Acc 82.140, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 83.120, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 82.530, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 82.500, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.65
Epoch:37
LR: 0.001
 * Train Acc 82.480, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 82.970, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 82.860, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 82.290, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.70
Epoch:41
LR: 0.001
 * Train Acc 82.520, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.70
Epoch:42
LR: 0.001
 * Train Acc 81.770, Loss 4.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 81.760, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 82.170, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 81.890, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 82.090, Loss 0.284
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 82.210, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.66
Epoch:48
LR: 0.001
 * Train Acc 81.740, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 81.680, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.66
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.756017208099365 - mean: 0.005504649598151445 - std: 0.00324770319275558
 * min 0.002431570552289486, max: 0.014887942932546139
sum: 123.27204895019531 - mean: 0.013375873677432537 - std: 0.008800975978374481
 * min 0.0029965515714138746, max: 0.04866018891334534
sum: 45.93280792236328 - mean: 0.002492014318704605 - std: 0.001166300498880446
 * min 0.0005244596395641565, max: 0.00715212756767869
sum: 119.30923461914062 - mean: 0.0032364700455218554 - std: 0.0010386175708845258
 * min 0.0007902556681074202, max: 0.008811889216303825
sum: 627.071533203125 - mean: 0.008505201898515224 - std: 0.0026903413236141205
 * min 0.0020495119970291853, max: 0.02658260241150856
sum: 1457.5679931640625 - mean: 0.00988476537168026 - std: 0.0015900960424914956
 * min 0.0024259965866804123, max: 0.032526008784770966
sum: 2154.164794921875 - mean: 0.014608864672482014 - std: 0.002140516648069024
 * min 0.00335912243463099, max: 0.042773012071847916
sum: 48.821685791015625 - mean: 5.95967831031885e-05 - std: 4.4728082571054983e-07
 * min 4.360153980087489e-05, max: 6.498517177533358e-05
sum: 1.7999998331069946 - mean: 0.003515624674037099 - std: 0.00016227280138991773
 * min 0.0026015588082373142, max: 0.004825697746127844
eps: tensor([0.0495, 0.1204, 0.0224, 0.0291, 0.0765, 0.0890, 0.1315, 0.1907, 0.1908],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 83.450, time 0.69
 * Lower 1 Val Acc 79.050, time 0.69
 * Upper 1 Val Acc 79.050, time 0.68
validation split name: 2
 *  Val Acc 79.850, time 0.67
 * Lower 1 Val Acc 75.100, time 0.66
 * Upper 1 Val Acc 75.100, time 0.68
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.160, Loss 0.476
 * , robust loss: 0.021 robust error: 0.01000000
 *  Val Acc 82.400, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 82.330, Loss 0.396
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 82.370, Loss 0.382
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 82.700, Loss 0.379
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 82.880, Loss 0.366
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.69
Epoch:5
LR: 0.001
 * Train Acc 83.070, Loss 0.355
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 83.390, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 83.310, Loss 0.340
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 83.380, Loss 0.329
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 82.830, Loss 0.339
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 83.570, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.70
Epoch:11
LR: 0.001
 * Train Acc 83.590, Loss 0.312
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 83.360, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 83.240, Loss 0.300
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 83.130, Loss 0.290
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 82.770, Loss 0.288
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.66
Epoch:16
LR: 0.001
 * Train Acc 83.260, Loss 0.282
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 83.080, Loss 0.278
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 83.230, Loss 0.266
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 83.550, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 82.630, Loss 0.286
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 83.220, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 83.700, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 82.870, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 82.610, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 83.060, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 83.010, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 83.000, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 83.140, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 82.550, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 83.160, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:30
LR: 0.001
 * Train Acc 82.920, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 83.100, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 82.850, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.70
Epoch:33
LR: 0.001
 * Train Acc 82.640, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 83.270, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 83.020, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 82.270, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 82.650, Loss 0.202
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 82.670, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 83.080, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.71
Epoch:40
LR: 0.001
 * Train Acc 82.290, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 82.150, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.71
Epoch:42
LR: 0.001
 * Train Acc 81.950, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.69
Epoch:43
LR: 0.001
 * Train Acc 82.330, Loss 0.201
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 81.830, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 81.660, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 82.090, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 81.430, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 81.490, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 81.820, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.69
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.5874850749969482 - mean: 0.0018373669590801 - std: 0.0011943550780415535
 * min 0.0008084463188424706, max: 0.005407640244811773
sum: 43.929141998291016 - mean: 0.0047666169703006744 - std: 0.0033891303464770317
 * min 0.0009597645257599652, max: 0.019029242917895317
sum: 14.448965072631836 - mean: 0.0007839065510779619 - std: 0.00038958273944444954
 * min 0.0001570906169945374, max: 0.0024962853640317917
sum: 38.056636810302734 - mean: 0.0010323523310944438 - std: 0.00034094517468474805
 * min 0.00024335717898793519, max: 0.0030103360768407583
sum: 197.6143798828125 - mean: 0.0026803165674209595 - std: 0.0008662090986035764
 * min 0.0006437269621528685, max: 0.008689716458320618
sum: 466.1768798828125 - mean: 0.003161464352160692 - std: 0.0005175006808713078
 * min 0.0007498521008528769, max: 0.01081918552517891
sum: 699.9714965820312 - mean: 0.004746985621750355 - std: 0.0007073863525874913
 * min 0.0010808223159983754, max: 0.014428210444748402
sum: 16.49580192565918 - mean: 2.0136476450716145e-05 - std: 1.5147136878113088e-07
 * min 1.4729366739629768e-05, max: 2.1957936041872017e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 2.2615664420300163e-05
 * min 0.0010473910951986909, max: 0.0013158346991986036
eps: tensor([0.0165, 0.0429, 0.0071, 0.0093, 0.0241, 0.0285, 0.0427, 0.0644, 0.0645],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.250, time 0.68
 * Lower 1 Val Acc 80.150, time 0.65
 * Upper 1 Val Acc 80.150, time 0.69
validation split name: 2
 *  Val Acc 73.850, time 0.70
 * Lower 1 Val Acc 75.850, time 0.70
 * Upper 1 Val Acc 75.850, time 0.69
validation split name: 3
 *  Val Acc 78.900, time 0.64
 * Lower 1 Val Acc 78.800, time 0.69
 * Upper 1 Val Acc 78.800, time 0.68
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 71.370, Loss 0.550
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 79.100, Loss 0.436
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 80.230, Loss 0.412
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 80.170, Loss 0.410
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.69
Epoch:4
LR: 0.001
 * Train Acc 79.670, Loss 0.411
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 80.350, Loss 0.393
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 80.500, Loss 0.391
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.69
Epoch:7
LR: 0.001
 * Train Acc 79.940, Loss 0.386
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 79.530, Loss 0.382
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.69
Epoch:9
LR: 0.001
 * Train Acc 80.090, Loss 0.369
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 80.080, Loss 0.363
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 80.180, Loss 0.355
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 80.250, Loss 0.352
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 80.000, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.66
Epoch:14
LR: 0.001
 * Train Acc 79.810, Loss 0.340
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 79.520, Loss 0.332
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 79.250, Loss 0.328
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 79.290, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 78.980, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 78.240, Loss 0.309
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 79.100, Loss 0.301
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.70
Epoch:21
LR: 0.001
 * Train Acc 79.030, Loss 0.293
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 78.970, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 79.040, Loss 0.279
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 78.320, Loss 0.275
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 78.550, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 77.990, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.70
Epoch:27
LR: 0.001
 * Train Acc 79.050, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 78.500, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 78.060, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 77.860, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 77.740, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 77.880, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.70
Epoch:33
LR: 0.001
 * Train Acc 77.710, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.70
Epoch:34
LR: 0.001
 * Train Acc 77.720, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 77.860, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.72
Epoch:36
LR: 0.001
 * Train Acc 77.160, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 77.490, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.69
Epoch:38
LR: 0.001
 * Train Acc 77.110, Loss 0.240
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 77.200, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 77.390, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 76.620, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 76.860, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 75.980, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 76.460, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 77.100, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 76.250, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.500, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 76.130, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 75.940, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 75.370, Loss 0.253
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.72
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0368037223815918 - mean: 0.0012000043643638492 - std: 0.0008115111850202084
 * min 0.0005151501973159611, max: 0.003645613556727767
sum: 29.165416717529297 - mean: 0.003164650173857808 - std: 0.002345171058550477
 * min 0.0006055665435269475, max: 0.013336814008653164
sum: 9.080390930175781 - mean: 0.0004926427500322461 - std: 0.00025136047042906284
 * min 9.376311209052801e-05, max: 0.001634158194065094
sum: 23.916698455810547 - mean: 0.0006487819482572377 - std: 0.00021660896891262382
 * min 0.0001519726065453142, max: 0.00194762391038239
sum: 126.18341064453125 - mean: 0.0017114720540121198 - std: 0.0005566113977693021
 * min 0.0004062191874254495, max: 0.005692408885806799
sum: 309.3240661621094 - mean: 0.0020977382082492113 - std: 0.0003453471581451595
 * min 0.0004908940754830837, max: 0.007356398273259401
sum: 471.09002685546875 - mean: 0.003194783814251423 - std: 0.00047901441575959325
 * min 0.0007249326445162296, max: 0.009902970865368843
sum: 11.184406280517578 - mean: 1.3652839697897434e-05 - std: 1.0272175643422088e-07
 * min 9.986736586142797e-06, max: 1.4887814359099139e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 5.0138437472924124e-06
 * min 0.0007488647243008018, max: 0.0008149991626851261
eps: tensor([0.0108, 0.0285, 0.0044, 0.0058, 0.0154, 0.0189, 0.0288, 0.0437, 0.0437],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.600, time 0.71
 * Lower 1 Val Acc 81.900, time 0.69
 * Upper 1 Val Acc 81.900, time 0.68
validation split name: 2
 *  Val Acc 77.050, time 0.69
 * Lower 1 Val Acc 77.850, time 0.67
 * Upper 1 Val Acc 77.850, time 0.68
validation split name: 3
 *  Val Acc 82.000, time 0.67
 * Lower 1 Val Acc 80.400, time 0.65
 * Upper 1 Val Acc 80.400, time 0.70
validation split name: 4
 *  Val Acc 79.050, time 0.66
 * Lower 1 Val Acc 78.550, time 0.68
 * Upper 1 Val Acc 78.550, time 0.69
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.610, Loss 0.454
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 80.880, Loss 0.411
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.200, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 80.970, Loss 0.401
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.150, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 81.120, Loss 0.394
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.70
Epoch:4
LR: 0.001
 * Train Acc 81.510, Loss 0.380
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.650, time 0.69
Epoch:5
LR: 0.001
 * Train Acc 81.560, Loss 0.372
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 81.690, Loss 0.367
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 80.930, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.750, time 0.72
Epoch:8
LR: 0.001
 * Train Acc 81.820, Loss 0.352
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.100, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 81.270, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 81.230, Loss 0.340
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.450, time 0.76
Epoch:11
LR: 0.001
 * Train Acc 81.380, Loss 0.334
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 81.590, Loss 0.325
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 81.430, Loss 0.319
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.70
Epoch:14
LR: 0.001
 * Train Acc 81.040, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.100, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 81.440, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.650, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 81.240, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 81.630, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.500, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 81.430, Loss 0.289
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 81.350, Loss 0.282
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.700, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 81.350, Loss 0.275
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.800, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 81.320, Loss 0.266
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.700, time 0.66
Epoch:22
LR: 0.001
 * Train Acc 81.560, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 81.050, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.400, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 80.850, Loss 0.250
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.800, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 81.220, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.69
Epoch:26
LR: 0.001
 * Train Acc 81.000, Loss 0.238
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 80.740, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.950, time 0.67
Epoch:28
LR: 0.001
 * Train Acc 80.620, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 81.150, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.750, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 81.010, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.550, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 80.940, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.050, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 80.140, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.550, time 0.70
Epoch:33
LR: 0.001
 * Train Acc 81.120, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.950, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 80.080, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.500, time 0.69
Epoch:35
LR: 0.001
 * Train Acc 80.720, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 80.670, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.550, time 0.70
Epoch:37
LR: 0.001
 * Train Acc 80.780, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 79.930, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.64
Epoch:39
LR: 0.001
 * Train Acc 79.890, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.000, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 80.780, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.100, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 80.180, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.300, time 0.70
Epoch:42
LR: 0.001
 * Train Acc 80.070, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 80.810, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.850, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 80.020, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 80.760, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.600, time 0.70
Epoch:46
LR: 0.001
 * Train Acc 80.190, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.70
Epoch:47
LR: 0.001
 * Train Acc 80.150, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 80.060, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.600, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 79.870, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.900, time 0.67
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5428920388221741 - mean: 0.0006283472757786512 - std: 0.0005068778991699219
 * min 0.00019791520026046783, max: 0.0022248183377087116
sum: 14.047786712646484 - mean: 0.0015242823865264654 - std: 0.001316532026976347
 * min 0.00019996486662421376, max: 0.007677159737795591
sum: 3.444653272628784 - mean: 0.00018688439740799367 - std: 0.00010305236355634406
 * min 2.7915839382330887e-05, max: 0.0007132089813239872
sum: 8.81722640991211 - mean: 0.000239182569202967 - std: 8.301412162836641e-05
 * min 4.924810127704404e-05, max: 0.0007956226472742856
sum: 47.90675735473633 - mean: 0.0006497770082205534 - std: 0.00021776520588900894
 * min 0.00012800563126802444, max: 0.002399267628788948
sum: 142.85292053222656 - mean: 0.0009687833953648806 - std: 0.0001652889040997252
 * min 0.00020785869855899364, max: 0.0038973751943558455
sum: 244.4832763671875 - mean: 0.0016580083174631 - std: 0.00026433850871399045
 * min 0.00032993737841024995, max: 0.005824730731546879
sum: 6.052262306213379 - mean: 7.388015546894167e-06 - std: 5.57215749097395e-08
 * min 5.402974693424767e-06, max: 8.05648414825555e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 2.4757851861068048e-05
 * min 0.00027877112734131515, max: 0.0005584658938460052
eps: tensor([0.0057, 0.0137, 0.0017, 0.0022, 0.0058, 0.0087, 0.0149, 0.0236, 0.0237],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 83.700, time 0.69
 * Lower 1 Val Acc 84.100, time 0.67
 * Upper 1 Val Acc 84.100, time 0.63
validation split name: 2
 *  Val Acc 77.350, time 0.67
 * Lower 1 Val Acc 77.450, time 0.68
 * Upper 1 Val Acc 77.450, time 0.68
validation split name: 3
 *  Val Acc 76.650, time 0.65
 * Lower 1 Val Acc 75.900, time 0.69
 * Upper 1 Val Acc 75.900, time 0.68
validation split name: 4
 *  Val Acc 78.200, time 0.68
 * Lower 1 Val Acc 78.150, time 0.67
 * Upper 1 Val Acc 78.150, time 0.69
validation split name: 5
 *  Val Acc 75.900, time 0.68
 * Lower 1 Val Acc 74.950, time 0.64
 * Upper 1 Val Acc 74.950, time 0.67
Task 1 average acc: 97.7
Task 2 average acc: 81.65
Task 3 average acc: 78.33333333333333
Task 4 average acc: 80.175
Task 5 average acc: 78.36000000000001
===Summary of experiment repeats: 8 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35 80.97 80.58 79.78 78.36  0.    0.  ]
mean: 63.473 std: 31.77585846204631
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.930, Loss 0.556
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 81.540, Loss 0.400
 * , robust loss: 0.135 robust error: 0.01000000
 *  Val Acc 85.750, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 85.140, Loss 0.339
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 88.590, Loss 0.268
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 91.600, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 89.530, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.050, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 90.230, Loss 0.224
 * , robust loss: 0.124 robust error: 0.01000000
 *  Val Acc 91.800, time 0.66
Epoch:6
LR: 0.001
 * Train Acc 91.940, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.800, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 92.980, Loss 0.169
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 92.150, time 0.70
Epoch:8
LR: 0.001
 * Train Acc 93.640, Loss 0.141
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 93.450, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 94.490, Loss 0.127
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.250, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 94.990, Loss 0.836
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.400, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 95.160, Loss 0.103
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.000, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 95.530, Loss 0.130
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.600, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 95.670, Loss 0.085
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.100, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 96.340, Loss 0.072
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.550, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 96.560, Loss 0.170
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.950, time 0.66
Epoch:16
LR: 0.001
 * Train Acc 97.100, Loss 0.116
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.200, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 96.690, Loss 0.062
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 96.360, Loss 60.871
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.150, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 96.760, Loss 0.056
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 97.570, Loss 0.050
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 97.540, Loss 0.042
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.600, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 97.640, Loss 0.040
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 97.700, Loss 0.524
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.850, time 0.66
Epoch:24
LR: 0.001
 * Train Acc 97.500, Loss 0.040
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 97.850, Loss 0.075
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.70
Epoch:26
LR: 0.001
 * Train Acc 97.520, Loss 0.341
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.700, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 98.240, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.500, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 98.140, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 98.090, Loss 0.030
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.350, time 0.70
Epoch:30
LR: 0.001
 * Train Acc 98.330, Loss 15.468
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 98.040, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.500, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 98.270, Loss 0.930
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.200, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 98.070, Loss 0.578
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.850, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 98.180, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 98.230, Loss 0.025
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 98.350, Loss 10.344
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.70
Epoch:37
LR: 0.001
 * Train Acc 98.410, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.67
Epoch:38
LR: 0.001
 * Train Acc 98.230, Loss 0.024
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 98.020, Loss 3.668
 * , robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 94.900, time 0.75
Epoch:40
LR: 0.001
 * Train Acc 98.510, Loss 0.030
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 98.000, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 98.200, Loss 9.010
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.650, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 98.420, Loss 0.023
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 98.440, Loss 0.021
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.71
Epoch:44
LR: 0.001
 * Train Acc 98.530, Loss 7.145
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 98.430, Loss 0.406
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 98.440, Loss 1.919
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.70
Epoch:47
LR: 0.001
 * Train Acc 98.530, Loss 0.019
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 98.500, Loss 0.137
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.350, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 98.460, Loss 0.022
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.69
after batch eps: 2.500000000000002, kappa: 0.5
sum: 14.579764366149902 - mean: 0.016874726861715317 - std: 0.00537253450602293
 * min 0.010575899854302406, max: 0.032264627516269684
sum: 298.9869079589844 - mean: 0.032442156225442886 - std: 0.013579079881310463
 * min 0.012208555825054646, max: 0.07978833466768265
sum: 203.5397186279297 - mean: 0.01104273647069931 - std: 0.003572894027456641
 * min 0.004376876167953014, max: 0.023122010752558708
sum: 480.49932861328125 - mean: 0.013034378178417683 - std: 0.003308703191578388
 * min 0.005167550407350063, max: 0.02660582959651947
sum: 2491.3896484375 - mean: 0.03379163518548012 - std: 0.008861567825078964
 * min 0.011209139600396156, max: 0.071868397295475
sum: 5341.2294921875 - mean: 0.03622253239154816 - std: 0.00769101083278656
 * min 0.013604745268821716, max: 0.08792845904827118
sum: 6084.2119140625 - mean: 0.041261203587055206 - std: 0.007853384129703045
 * min 0.015607851557433605, max: 0.09208167344331741
sum: 107.24649810791016 - mean: 0.00013091613072901964 - std: 8.069055184023455e-07
 * min 0.00011151404032716528, max: 0.00013665073493029922
sum: 5.0 - mean: 0.009765625 - std: 0.004908969160169363
 * min 0.0029446049593389034, max: 0.035081446170806885
eps: tensor([0.1519, 0.2920, 0.0994, 0.1173, 0.3041, 0.3260, 0.3714, 0.4189, 0.4190],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.800, time 0.67
 * Lower 1 Val Acc 55.200, time 0.68
 * Upper 1 Val Acc 55.200, time 0.68
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.230, Loss 0.561
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 79.590, Loss 0.440
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.69
Epoch:2
LR: 0.001
 * Train Acc 80.940, Loss 0.399
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 81.710, Loss 0.376
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 82.380, Loss 0.363
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 82.890, Loss 0.358
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 83.580, Loss 0.338
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.69
Epoch:7
LR: 0.001
 * Train Acc 83.550, Loss 0.334
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 83.960, Loss 0.314
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.66
Epoch:9
LR: 0.001
 * Train Acc 84.360, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 84.940, Loss 0.287
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 84.670, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.210, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 84.760, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 84.690, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 85.430, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.780, Loss 1.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 84.640, Loss 0.377
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 85.520, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 84.990, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 85.100, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 84.940, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 85.560, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 85.290, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 85.650, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 85.240, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.66
Epoch:26
LR: 0.001
 * Train Acc 85.410, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 85.550, Loss 0.197
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 85.700, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 85.680, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.70
Epoch:30
LR: 0.001
 * Train Acc 85.400, Loss 0.169
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 85.160, Loss 0.168
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.70
Epoch:32
LR: 0.001
 * Train Acc 85.530, Loss 0.166
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 85.410, Loss 0.168
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 84.960, Loss 0.169
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 84.940, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 85.090, Loss 0.171
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 84.910, Loss 0.172
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 84.840, Loss 0.174
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 84.900, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 85.080, Loss 0.171
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 84.630, Loss 0.173
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 84.630, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.69
Epoch:43
LR: 0.001
 * Train Acc 84.500, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.70
Epoch:44
LR: 0.001
 * Train Acc 84.900, Loss 0.175
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 84.660, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 83.530, Loss 0.946
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 84.110, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 84.120, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 83.940, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.69
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.949614524841309 - mean: 0.005728720687329769 - std: 0.0024811539333313704
 * min 0.0030290314462035894, max: 0.01335828471928835
sum: 112.88771057128906 - mean: 0.01224910095334053 - std: 0.006297678221017122
 * min 0.0036314150784164667, max: 0.03719642385840416
sum: 59.06434631347656 - mean: 0.003204445820301771 - std: 0.0012284013209864497
 * min 0.0009703371324576437, max: 0.007797618396580219
sum: 139.0560760498047 - mean: 0.003772137453779578 - std: 0.0010828496888279915
 * min 0.0012479469878599048, max: 0.009210050106048584
sum: 813.1016845703125 - mean: 0.01102839782834053 - std: 0.0032536634244024754
 * min 0.0031774260569363832, max: 0.025893859565258026
sum: 1941.666259765625 - mean: 0.013167766854166985 - std: 0.003085010452196002
 * min 0.004348750226199627, max: 0.034423861652612686
sum: 2277.032958984375 - mean: 0.015442118048667908 - std: 0.0032434898894280195
 * min 0.005335942376405001, max: 0.037361808121204376
sum: 40.783084869384766 - mean: 4.978403740096837e-05 - std: 3.2131907801158377e-07
 * min 4.190424442640506e-05, max: 5.218147634877823e-05
sum: 1.8000000715255737 - mean: 0.003515625139698386 - std: 0.00023625562607776374
 * min 0.002450919710099697, max: 0.0050887977704405785
eps: tensor([0.0516, 0.1102, 0.0288, 0.0339, 0.0993, 0.1185, 0.1390, 0.1593, 0.1594],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.900, time 0.68
 * Lower 1 Val Acc 75.150, time 0.64
 * Upper 1 Val Acc 75.150, time 0.69
validation split name: 2
 *  Val Acc 83.650, time 0.70
 * Lower 1 Val Acc 62.000, time 0.67
 * Upper 1 Val Acc 62.000, time 0.67
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 80.050, Loss 0.433
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 83.700, Loss 0.366
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 84.310, Loss 0.354
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 84.660, Loss 0.333
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 85.000, Loss 0.325
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 84.770, Loss 0.322
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 84.790, Loss 0.314
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.67
Epoch:7
LR: 0.001
 * Train Acc 84.470, Loss 0.310
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 84.790, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.69
Epoch:9
LR: 0.001
 * Train Acc 84.700, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 85.000, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 85.000, Loss 0.277
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 85.150, Loss 0.273
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 85.230, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 84.880, Loss 0.264
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 85.000, Loss 0.256
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.880, Loss 0.255
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 85.200, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 85.110, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 84.620, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 84.850, Loss 0.229
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 84.680, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 85.030, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 85.280, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 85.220, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 84.940, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 85.090, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 84.930, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 84.660, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 84.920, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 84.700, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 84.950, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 84.410, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.70
Epoch:33
LR: 0.001
 * Train Acc 84.830, Loss 0.176
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 84.200, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 84.610, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 84.450, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 84.410, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.66
Epoch:38
LR: 0.001
 * Train Acc 84.410, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 84.550, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 84.560, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.200, time 0.71
Epoch:41
LR: 0.001
 * Train Acc 83.700, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 83.570, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.500, time 0.70
Epoch:43
LR: 0.001
 * Train Acc 83.950, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.66
Epoch:44
LR: 0.001
 * Train Acc 83.670, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 83.730, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 83.370, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 83.690, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.150, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 83.770, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 83.170, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.450, time 0.69
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.6405341625213623 - mean: 0.0018987663788720965 - std: 0.0008609375217929482
 * min 0.0009674187749624252, max: 0.0045523010194301605
sum: 38.486656188964844 - mean: 0.004176069516688585 - std: 0.002225176664069295
 * min 0.0011745926458388567, max: 0.013124139048159122
sum: 18.95147705078125 - mean: 0.0010281834984198213 - std: 0.00040543609065935016
 * min 0.0002894884964916855, max: 0.002597226295620203
sum: 44.831878662109375 - mean: 0.0012161425547674298 - std: 0.00035545267746783793
 * min 0.0003850937355309725, max: 0.003059375798329711
sum: 262.95623779296875 - mean: 0.0035665722098201513 - std: 0.0010688110487535596
 * min 0.001026325160637498, max: 0.008716056123375893
sum: 645.3336181640625 - mean: 0.004376448690891266 - std: 0.0010344276670366526
 * min 0.0014191336231306195, max: 0.011610090732574463
sum: 762.7903442382812 - mean: 0.005173003301024437 - std: 0.0010969258146360517
 * min 0.0017655069241300225, max: 0.01261348556727171
sum: 13.704534530639648 - mean: 1.672916732786689e-05 - std: 1.0808353323454867e-07
 * min 1.408143089065561e-05, max: 1.7535481674713083e-05
sum: 0.5999999642372131 - mean: 0.001171874930150807 - std: 3.9470833144150674e-05
 * min 0.000997881987132132, max: 0.0013769780052825809
eps: tensor([0.0171, 0.0376, 0.0093, 0.0109, 0.0321, 0.0394, 0.0466, 0.0535, 0.0535],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.950, time 0.68
 * Lower 1 Val Acc 82.250, time 0.68
 * Upper 1 Val Acc 82.250, time 0.68
validation split name: 2
 *  Val Acc 73.450, time 0.68
 * Lower 1 Val Acc 75.800, time 0.68
 * Upper 1 Val Acc 75.800, time 0.68
validation split name: 3
 *  Val Acc 77.450, time 0.68
 * Lower 1 Val Acc 77.800, time 0.67
 * Upper 1 Val Acc 77.800, time 0.65
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 73.600, Loss 0.497
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 81.250, Loss 0.407
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 83.000, Loss 0.371
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 82.870, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 83.020, Loss 0.354
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.71
Epoch:5
LR: 0.001
 * Train Acc 83.630, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 82.950, Loss 0.339
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.69
Epoch:7
LR: 0.001
 * Train Acc 83.930, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 83.490, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 84.190, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 83.990, Loss 0.308
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.66
Epoch:11
LR: 0.001
 * Train Acc 82.990, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 83.740, Loss 0.296
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 83.040, Loss 0.293
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 83.400, Loss 0.284
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 83.540, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 83.020, Loss 0.276
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 83.060, Loss 0.298
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 82.920, Loss 0.264
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 82.760, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 82.980, Loss 0.253
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 82.940, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.70
Epoch:22
LR: 0.001
 * Train Acc 82.540, Loss 0.245
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.70
Epoch:23
LR: 0.001
 * Train Acc 82.560, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 82.620, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 82.580, Loss 0.223
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 82.100, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 81.610, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 81.650, Loss 0.221
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 81.520, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.69
Epoch:30
LR: 0.001
 * Train Acc 81.370, Loss 0.459
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 81.390, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 81.120, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 81.640, Loss 0.204
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.70
Epoch:34
LR: 0.001
 * Train Acc 80.920, Loss 0.208
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 81.460, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 81.400, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 81.700, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 80.790, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.70
Epoch:39
LR: 0.001
 * Train Acc 80.370, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 80.500, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 80.480, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 80.170, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 79.860, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 79.880, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 79.470, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 79.810, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 79.420, Loss 0.218
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 79.320, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 78.690, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.69
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.1370985507965088 - mean: 0.0013160862727090716 - std: 0.0006276379572227597
 * min 0.0006193006411194801, max: 0.003275900846347213
sum: 24.72978973388672 - mean: 0.002683353843167424 - std: 0.001518194330856204
 * min 0.0006976092699915171, max: 0.009038453921675682
sum: 11.79609489440918 - mean: 0.0006399790872819722 - std: 0.00026331646949984133
 * min 0.00016659536049701273, max: 0.0017215400002896786
sum: 27.8271484375 - mean: 0.0007548597059212625 - std: 0.00022571823501493782
 * min 0.00023042914108373225, max: 0.002033822936937213
sum: 163.9032745361328 - mean: 0.002223080489784479 - std: 0.0006782979471608996
 * min 0.0006164421210996807, max: 0.0056769330985844135
sum: 431.1383361816406 - mean: 0.002923843916505575 - std: 0.0007022101199254394
 * min 0.0009434981038793921, max: 0.008007782511413097
sum: 518.154052734375 - mean: 0.00351395714096725 - std: 0.0007592322072014213
 * min 0.0011612804373726249, max: 0.00891885720193386
sum: 9.406768798828125 - mean: 1.1482871741463896e-05 - std: 7.422389813882546e-08
 * min 9.665470315667335e-06, max: 1.203632382384967e-05
sum: 0.40000003576278687 - mean: 0.0007812500698491931 - std: 1.9081508071394637e-05
 * min 0.0007158171501941979, max: 0.0008544554002583027
eps: tensor([0.0118, 0.0242, 0.0058, 0.0068, 0.0200, 0.0263, 0.0316, 0.0367, 0.0368],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.550, time 0.66
 * Lower 1 Val Acc 85.600, time 0.68
 * Upper 1 Val Acc 85.600, time 0.68
validation split name: 2
 *  Val Acc 77.750, time 0.68
 * Lower 1 Val Acc 76.550, time 0.68
 * Upper 1 Val Acc 76.550, time 0.70
validation split name: 3
 *  Val Acc 82.700, time 0.68
 * Lower 1 Val Acc 81.850, time 0.68
 * Upper 1 Val Acc 81.850, time 0.66
validation split name: 4
 *  Val Acc 81.750, time 0.65
 * Lower 1 Val Acc 80.800, time 0.69
 * Upper 1 Val Acc 80.800, time 0.68
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 81.260, Loss 0.403
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 84.080, Loss 0.349
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 84.530, Loss 0.344
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 84.560, Loss 0.335
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 84.890, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 84.800, Loss 0.321
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 84.590, Loss 0.313
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 84.770, Loss 0.308
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 84.560, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.70
Epoch:9
LR: 0.001
 * Train Acc 84.070, Loss 0.303
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.72
Epoch:10
LR: 0.001
 * Train Acc 85.220, Loss 0.288
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.69
Epoch:11
LR: 0.001
 * Train Acc 84.700, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.730, Loss 0.281
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.71
Epoch:13
LR: 0.001
 * Train Acc 84.950, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 85.170, Loss 0.260
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 84.600, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 84.650, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 84.490, Loss 0.253
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 84.210, Loss 0.247
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 85.010, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.72
Epoch:20
LR: 0.001
 * Train Acc 84.470, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.69
Epoch:21
LR: 0.001
 * Train Acc 84.560, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.73
Epoch:22
LR: 0.001
 * Train Acc 84.470, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 84.220, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 84.060, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.71
Epoch:25
LR: 0.001
 * Train Acc 84.430, Loss 0.205
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 83.960, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 84.200, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 84.350, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.72
Epoch:29
LR: 0.001
 * Train Acc 83.710, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 83.390, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 84.280, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 84.280, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.69
Epoch:33
LR: 0.001
 * Train Acc 84.180, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 84.030, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.65
Epoch:35
LR: 0.001
 * Train Acc 83.740, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 84.060, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 83.720, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 83.800, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.71
Epoch:39
LR: 0.001
 * Train Acc 84.000, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 83.880, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.72
Epoch:41
LR: 0.001
 * Train Acc 83.510, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 83.500, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 83.200, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.66
Epoch:44
LR: 0.001
 * Train Acc 83.480, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 82.980, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 83.090, Loss 0.193
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 82.940, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 83.820, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.71
Epoch:49
LR: 0.001
 * Train Acc 82.540, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.72
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5708639621734619 - mean: 0.0006607222021557391 - std: 0.00031537635368295014
 * min 0.00031096028396859765, max: 0.001646771328523755
sum: 12.409173965454102 - mean: 0.0013464816147461534 - std: 0.0007632041233591735
 * min 0.00034938781755045056, max: 0.004539812449365854
sum: 5.917034149169922 - mean: 0.0003210196446161717 - std: 0.00013226181908976287
 * min 8.349900599569082e-05, max: 0.0008638328872621059
sum: 13.949846267700195 - mean: 0.0003784137952607125 - std: 0.00011319381155772135
 * min 0.00011538351827766746, max: 0.0010199666721746325
sum: 82.04653930664062 - mean: 0.00111282744910568 - std: 0.0003395910025574267
 * min 0.0003083753981627524, max: 0.0028427252545952797
sum: 215.06597900390625 - mean: 0.0014585094759240746 - std: 0.0003502977197058499
 * min 0.00047063652891665697, max: 0.003995039034634829
sum: 258.6473083496094 - mean: 0.0017540643457323313 - std: 0.00037899144808761775
 * min 0.0005796511541120708, max: 0.004452178720384836
sum: 4.698243618011475 - mean: 5.735160357289715e-06 - std: 3.7071391290055544e-08
 * min 4.82745281260577e-06, max: 6.0115844462416135e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 4.4897255691012106e-08
 * min 0.0003903982287738472, max: 0.00039085198659449816
eps: tensor([0.0059, 0.0121, 0.0029, 0.0034, 0.0100, 0.0131, 0.0158, 0.0184, 0.0184],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.400, time 0.71
 * Lower 1 Val Acc 85.600, time 0.67
 * Upper 1 Val Acc 85.600, time 0.68
validation split name: 2
 *  Val Acc 76.400, time 0.70
 * Lower 1 Val Acc 75.750, time 0.73
 * Upper 1 Val Acc 75.750, time 0.68
validation split name: 3
 *  Val Acc 78.450, time 0.67
 * Lower 1 Val Acc 77.850, time 0.68
 * Upper 1 Val Acc 77.850, time 0.69
validation split name: 4
 *  Val Acc 77.550, time 0.68
 * Lower 1 Val Acc 77.550, time 0.69
 * Upper 1 Val Acc 77.550, time 0.69
validation split name: 5
 *  Val Acc 80.450, time 0.73
 * Lower 1 Val Acc 81.200, time 0.69
 * Upper 1 Val Acc 81.200, time 0.70
Task 1 average acc: 97.8
Task 2 average acc: 85.275
Task 3 average acc: 77.95
Task 4 average acc: 82.1875
Task 5 average acc: 79.85
===Summary of experiment repeats: 9 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35 80.97 80.58 79.78 78.36 79.85  0.  ]
mean: 71.458 std: 23.87223064566862
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (1): LinearInterval(in_features=256, out_features=2, bias=False)
    (2): LinearInterval(in_features=256, out_features=2, bias=False)
    (3): LinearInterval(in_features=256, out_features=2, bias=False)
    (4): LinearInterval(in_features=256, out_features=2, bias=False)
    (5): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2511561
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 66.140, Loss 0.610
 * , robust loss: 0.101 robust error: 0.05000000
 *  Val Acc 80.350, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 78.340, Loss 0.446
 * , robust loss: 0.043 robust error: 0.01000000
 *  Val Acc 83.850, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 83.900, Loss 0.358
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.66
Epoch:3
LR: 0.001
 * Train Acc 86.390, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.450, time 0.72
Epoch:4
LR: 0.001
 * Train Acc 88.340, Loss 0.262
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.500, time 0.71
Epoch:5
LR: 0.001
 * Train Acc 90.410, Loss 0.222
 * , robust loss: 0.051 robust error: 0.01000000
 *  Val Acc 89.800, time 0.71
Epoch:6
LR: 0.001
 * Train Acc 90.830, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.250, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 91.910, Loss 0.182
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.150, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 92.390, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.950, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 93.320, Loss 0.144
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.200, time 0.69
Epoch:10
LR: 0.001
 * Train Acc 94.010, Loss 0.131
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.250, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 94.640, Loss 0.121
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.000, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 94.910, Loss 0.105
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.150, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 95.020, Loss 0.193
 * , robust loss: 28.355 robust error: 0.01000000
 *  Val Acc 94.100, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 95.220, Loss 0.102
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.300, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 95.870, Loss 0.078
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 92.850, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 96.230, Loss 0.161
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.050, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 96.350, Loss 0.377
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.500, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 96.090, Loss 0.444
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.150, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 96.660, Loss 0.062
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.100, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 96.840, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.000, time 0.69
Epoch:21
LR: 0.001
 * Train Acc 96.620, Loss 0.170
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.700, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 96.960, Loss 0.050
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 97.140, Loss 0.045
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 97.310, Loss 0.057
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 97.290, Loss 0.083
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 97.210, Loss 1.714
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 97.530, Loss 0.052
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.150, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 97.920, Loss 0.039
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 97.460, Loss 3.732
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 97.540, Loss 0.607
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 97.710, Loss 0.030
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 97.870, Loss 0.031
 * , robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 97.900, time 0.70
Epoch:33
LR: 0.001
 * Train Acc 97.760, Loss 0.653
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 98.020, Loss 0.028
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.72
Epoch:35
LR: 0.001
 * Train Acc 97.890, Loss 5.788
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 98.010, Loss 68.692
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.000, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 97.640, Loss 0.049
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 97.920, Loss 39.153
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 97.790, Loss 0.030
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 98.080, Loss 0.753
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 97.990, Loss 26.657
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.70
Epoch:42
LR: 0.001
 * Train Acc 98.130, Loss 0.027
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 97.740, Loss 2.651
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 98.040, Loss 0.519
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 98.210, Loss 1.466
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.70
Epoch:46
LR: 0.001
 * Train Acc 98.000, Loss 0.043
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.66
Epoch:47
LR: 0.001
 * Train Acc 98.180, Loss 0.024
 * , robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.850, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 98.190, Loss 0.199
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.64
Epoch:49
LR: 0.001
 * Train Acc 98.160, Loss 0.104
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.67
after batch eps: 2.500000000000002, kappa: 0.5
sum: 14.418416976928711 - mean: 0.01668798364698887 - std: 0.005566650070250034
 * min 0.009234342724084854, max: 0.031372055411338806
sum: 289.4743957519531 - mean: 0.03140998259186745 - std: 0.01294689066708088
 * min 0.012229597195982933, max: 0.07400482147932053
sum: 188.7301788330078 - mean: 0.010239267721772194 - std: 0.0033773118630051613
 * min 0.0038300680462270975, max: 0.02144361473619938
sum: 495.3819580078125 - mean: 0.013438095338642597 - std: 0.0036194741260260344
 * min 0.00492955232039094, max: 0.028518836945295334
sum: 2195.365234375 - mean: 0.029776547104120255 - std: 0.00883749220520258
 * min 0.010754875838756561, max: 0.07216248661279678
sum: 5157.25048828125 - mean: 0.0349748432636261 - std: 0.005648172460496426
 * min 0.012634562328457832, max: 0.08711092174053192
sum: 6402.50244140625 - mean: 0.04341974854469299 - std: 0.00607910705730319
 * min 0.015278307721018791, max: 0.10377025604248047
sum: 112.69002532958984 - mean: 0.00013756105909124017 - std: 8.794060022410122e-07
 * min 0.00011283379717497155, max: 0.00014364066009875387
sum: 5.0 - mean: 0.009765625 - std: 0.004426469095051289
 * min 0.002703495090827346, max: 0.045570578426122665
eps: tensor([0.1502, 0.2827, 0.0922, 0.1209, 0.2680, 0.3148, 0.3908, 0.4402, 0.4403],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.400, time 0.68
 * Lower 1 Val Acc 61.750, time 0.68
 * Upper 1 Val Acc 61.750, time 0.68
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 71.020, Loss 0.563
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.900, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 77.940, Loss 0.457
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 80.790, Loss 0.411
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 80.990, Loss 0.400
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 81.530, Loss 0.376
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.68
Epoch:5
LR: 0.001
 * Train Acc 82.850, Loss 0.353
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 82.380, Loss 0.356
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.69
Epoch:7
LR: 0.001
 * Train Acc 82.780, Loss 0.345
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.300, Loss 0.322
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.69
Epoch:9
LR: 0.001
 * Train Acc 83.500, Loss 0.318
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 83.220, Loss 0.312
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.68
Epoch:11
LR: 0.001
 * Train Acc 83.290, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 83.820, Loss 0.292
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 84.530, Loss 0.278
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 83.940, Loss 0.275
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 84.310, Loss 0.269
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.70
Epoch:16
LR: 0.001
 * Train Acc 84.110, Loss 0.267
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 84.710, Loss 0.250
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 84.640, Loss 0.249
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 84.450, Loss 0.242
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 84.300, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.69
Epoch:21
LR: 0.001
 * Train Acc 83.870, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.66
Epoch:22
LR: 0.001
 * Train Acc 84.000, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 84.360, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 84.280, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.67
Epoch:25
LR: 0.001
 * Train Acc 84.350, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 84.270, Loss 0.203
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 84.360, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.66
Epoch:28
LR: 0.001
 * Train Acc 84.640, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 84.480, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 84.170, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 84.620, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 84.550, Loss 0.179
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.68
Epoch:33
LR: 0.001
 * Train Acc 84.460, Loss 0.177
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 84.280, Loss 0.180
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 83.470, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.66
Epoch:36
LR: 0.001
 * Train Acc 83.750, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 83.690, Loss 0.183
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.69
Epoch:38
LR: 0.001
 * Train Acc 83.920, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 83.350, Loss 0.259
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 83.370, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 83.960, Loss 0.184
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 83.480, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 83.340, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.70
Epoch:44
LR: 0.001
 * Train Acc 82.990, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 83.660, Loss 0.181
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 83.290, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 83.420, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 83.470, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 82.820, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.67
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 5.21758508682251 - mean: 0.006038871593773365 - std: 0.0026796471793204546
 * min 0.0032980628311634064, max: 0.013847840018570423
sum: 111.19205474853516 - mean: 0.012065109796822071 - std: 0.006114503834396601
 * min 0.0037698436062783003, max: 0.03370091691613197
sum: 59.027549743652344 - mean: 0.003202449530363083 - std: 0.0011989512713626027
 * min 0.0010030219564214349, max: 0.007675569038838148
sum: 151.38092041015625 - mean: 0.004106470383703709 - std: 0.0012112115509808064
 * min 0.0012075298000127077, max: 0.010188902728259563
sum: 708.5184936523438 - mean: 0.009609897620975971 - std: 0.0031074320431798697
 * min 0.0030466916505247355, max: 0.02681984007358551
sum: 1810.447021484375 - mean: 0.012277879752218723 - std: 0.0021597675513476133
 * min 0.004094328731298447, max: 0.0326240211725235
sum: 2313.249267578125 - mean: 0.0156877264380455 - std: 0.002386269858106971
 * min 0.005101379938423634, max: 0.03918925300240517
sum: 42.63005828857422 - mean: 5.2038645662833005e-05 - std: 3.40939266152418e-07
 * min 4.2541305447230116e-05, max: 5.4370349971577525e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.00028917755116708577
 * min 0.0023306410294026136, max: 0.005365303251892328
eps: tensor([0.0543, 0.1086, 0.0288, 0.0370, 0.0865, 0.1105, 0.1412, 0.1665, 0.1666],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.650, time 0.68
 * Lower 1 Val Acc 81.950, time 0.68
 * Upper 1 Val Acc 81.950, time 0.69
validation split name: 2
 *  Val Acc 80.150, time 0.69
 * Lower 1 Val Acc 69.650, time 0.69
 * Upper 1 Val Acc 69.650, time 0.67
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.080, Loss 0.453
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.66
Epoch:1
LR: 0.001
 * Train Acc 82.960, Loss 0.385
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 83.910, Loss 0.363
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 83.600, Loss 0.356
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.70
Epoch:4
LR: 0.001
 * Train Acc 83.950, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 83.850, Loss 0.338
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 84.650, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 84.040, Loss 0.323
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 84.030, Loss 0.315
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 84.500, Loss 0.306
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 84.330, Loss 0.302
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 84.390, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 84.480, Loss 0.285
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 84.510, Loss 0.282
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 84.460, Loss 0.275
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 84.360, Loss 0.273
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 84.690, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 84.310, Loss 0.268
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 84.230, Loss 0.257
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.69
Epoch:19
LR: 0.001
 * Train Acc 84.390, Loss 0.246
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 84.200, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 84.140, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 83.880, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 84.850, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 84.230, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.69
Epoch:25
LR: 0.001
 * Train Acc 84.280, Loss 0.210
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.70
Epoch:26
LR: 0.001
 * Train Acc 84.040, Loss 0.206
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.65
Epoch:27
LR: 0.001
 * Train Acc 84.070, Loss 0.200
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 84.080, Loss 0.196
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 84.170, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 83.950, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 83.500, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 84.050, Loss 0.185
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.69
Epoch:33
LR: 0.001
 * Train Acc 83.770, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 83.980, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 83.630, Loss 0.189
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 84.140, Loss 0.188
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 83.830, Loss 0.186
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 83.560, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 83.750, Loss 0.187
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 83.120, Loss 0.198
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.70
Epoch:41
LR: 0.001
 * Train Acc 83.480, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.69
Epoch:42
LR: 0.001
 * Train Acc 83.070, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.71
Epoch:43
LR: 0.001
 * Train Acc 83.190, Loss 0.191
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 82.940, Loss 0.194
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.68
Epoch:45
LR: 0.001
 * Train Acc 83.520, Loss 0.192
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 82.940, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 82.990, Loss 0.193
 * , robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.200, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 83.130, Loss 0.195
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 83.620, Loss 0.190
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.68
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.576385736465454 - mean: 0.0018245205283164978 - std: 0.0010522888042032719
 * min 0.0008274313295260072, max: 0.004944498650729656
sum: 37.45967102050781 - mean: 0.004064634442329407 - std: 0.0024928199127316475
 * min 0.0010225244332104921, max: 0.013951720669865608
sum: 14.953373908996582 - mean: 0.0008112724754028022 - std: 0.00035413861041888595
 * min 0.00018117060244549066, max: 0.0023569746408611536
sum: 38.12154006958008 - mean: 0.0010341129964217544 - std: 0.0003403410373721272
 * min 0.00023073954798746854, max: 0.003033269662410021
sum: 183.96160888671875 - mean: 0.0024951391387730837 - std: 0.0008891534525901079
 * min 0.0006391229690052569, max: 0.008054306730628014
sum: 601.774658203125 - mean: 0.004081045743077993 - std: 0.0008031208417378366
 * min 0.00117559265345335, max: 0.012237126007676125
sum: 827.4168701171875 - mean: 0.005611279979348183 - std: 0.000967654341366142
 * min 0.0015276947524398565, max: 0.01513257808983326
sum: 15.447546005249023 - mean: 1.885686651803553e-05 - std: 1.2564248663693434e-07
 * min 1.536109994049184e-05, max: 1.9715889720828272e-05
sum: 0.6000000834465027 - mean: 0.0011718751629814506 - std: 0.00010577074863249436
 * min 0.0007802312029525638, max: 0.0017744689248502254
eps: tensor([0.0164, 0.0366, 0.0073, 0.0093, 0.0225, 0.0367, 0.0505, 0.0603, 0.0604],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.900, time 0.66
 * Lower 1 Val Acc 86.150, time 0.70
 * Upper 1 Val Acc 86.150, time 0.70
validation split name: 2
 *  Val Acc 77.450, time 0.68
 * Lower 1 Val Acc 76.750, time 0.68
 * Upper 1 Val Acc 76.750, time 0.69
validation split name: 3
 *  Val Acc 82.800, time 0.71
 * Lower 1 Val Acc 82.400, time 0.69
 * Upper 1 Val Acc 82.400, time 0.68
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 74.860, Loss 0.491
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.70
Epoch:1
LR: 0.001
 * Train Acc 80.390, Loss 0.415
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.73
Epoch:2
LR: 0.001
 * Train Acc 80.620, Loss 0.401
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.70
Epoch:3
LR: 0.001
 * Train Acc 80.890, Loss 0.395
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 80.510, Loss 0.386
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 80.780, Loss 0.383
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 80.710, Loss 0.371
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.71
Epoch:7
LR: 0.001
 * Train Acc 80.360, Loss 0.370
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 80.510, Loss 0.361
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 80.700, Loss 0.356
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 80.730, Loss 0.347
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.69
Epoch:11
LR: 0.001
 * Train Acc 80.480, Loss 0.341
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 80.800, Loss 0.327
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 80.590, Loss 0.326
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 80.960, Loss 0.317
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 80.580, Loss 0.313
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 80.920, Loss 0.299
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 80.000, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 80.310, Loss 0.293
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.72
Epoch:19
LR: 0.001
 * Train Acc 80.180, Loss 0.292
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 80.290, Loss 0.282
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 80.330, Loss 0.276
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 79.430, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 79.690, Loss 0.265
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 79.250, Loss 0.258
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.71
Epoch:25
LR: 0.001
 * Train Acc 79.400, Loss 0.251
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.67
Epoch:26
LR: 0.001
 * Train Acc 79.730, Loss 0.244
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 78.730, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 78.780, Loss 0.322
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 79.250, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 79.040, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.69
Epoch:31
LR: 0.001
 * Train Acc 78.590, Loss 0.226
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 78.620, Loss 0.225
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 79.220, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 78.250, Loss 0.227
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 78.410, Loss 0.224
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.66
Epoch:36
LR: 0.001
 * Train Acc 77.930, Loss 0.228
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 77.910, Loss 0.231
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 77.970, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 77.690, Loss 0.233
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 77.570, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 77.960, Loss 0.232
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 76.950, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.72
Epoch:43
LR: 0.001
 * Train Acc 77.250, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 77.270, Loss 0.237
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 76.750, Loss 0.241
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.700, time 0.71
Epoch:46
LR: 0.001
 * Train Acc 77.080, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.250, time 0.69
Epoch:47
LR: 0.001
 * Train Acc 77.560, Loss 0.239
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 76.340, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.68
Epoch:49
LR: 0.001
 * Train Acc 76.540, Loss 0.242
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.67
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0710349082946777 - mean: 0.0012396237580105662 - std: 0.0007699038251303136
 * min 0.0005093199433758855, max: 0.003573100082576275
sum: 22.947227478027344 - mean: 0.0024899335112422705 - std: 0.001619452377781272
 * min 0.0005612962413579226, max: 0.009139551781117916
sum: 8.932785987854004 - mean: 0.0004846346564590931 - std: 0.00022080935013946146
 * min 9.920317097567022e-05, max: 0.001464995788410306
sum: 22.98790740966797 - mean: 0.0006235868786461651 - std: 0.00021143366757314652
 * min 0.00013189585297368467, max: 0.001912097679451108
sum: 116.529296875 - mean: 0.001580529729835689 - std: 0.0005765954847447574
 * min 0.0003657778725028038, max: 0.005334664136171341
sum: 407.048828125 - mean: 0.002760476665571332 - std: 0.0005604901816695929
 * min 0.0007319883443415165, max: 0.008807756938040257
sum: 566.1856079101562 - mean: 0.0038396918680518866 - std: 0.0006872454541735351
 * min 0.0009835482342168689, max: 0.01072659995406866
sum: 10.601106643676758 - mean: 1.2940803571837023e-05 - std: 8.636308734821796e-08
 * min 1.0540801667957567e-05, max: 1.353042807750171e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 3.467612259555608e-05
 * min 0.0006701798411086202, max: 0.0009101395844481885
eps: tensor([0.0112, 0.0224, 0.0044, 0.0056, 0.0142, 0.0248, 0.0346, 0.0414, 0.0414],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.500, time 0.68
 * Lower 1 Val Acc 87.300, time 0.68
 * Upper 1 Val Acc 87.300, time 0.68
validation split name: 2
 *  Val Acc 78.000, time 0.68
 * Lower 1 Val Acc 76.700, time 0.68
 * Upper 1 Val Acc 76.700, time 0.69
validation split name: 3
 *  Val Acc 84.300, time 0.64
 * Lower 1 Val Acc 83.400, time 0.70
 * Upper 1 Val Acc 83.400, time 0.69
validation split name: 4
 *  Val Acc 78.300, time 0.69
 * Lower 1 Val Acc 76.100, time 0.67
 * Upper 1 Val Acc 76.100, time 0.68
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.240, Loss 0.453
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 82.080, Loss 0.401
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 82.030, Loss 0.388
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.72
Epoch:3
LR: 0.001
 * Train Acc 82.550, Loss 0.377
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 82.280, Loss 0.369
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.69
Epoch:5
LR: 0.001
 * Train Acc 82.270, Loss 0.364
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 81.560, Loss 0.355
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 81.820, Loss 0.352
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 82.100, Loss 0.343
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 81.690, Loss 0.332
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 82.030, Loss 0.330
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.71
Epoch:11
LR: 0.001
 * Train Acc 81.410, Loss 0.328
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 82.150, Loss 0.320
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 82.070, Loss 0.307
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.71
Epoch:14
LR: 0.001
 * Train Acc 81.590, Loss 0.311
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.65
Epoch:15
LR: 0.001
 * Train Acc 81.260, Loss 0.304
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.69
Epoch:16
LR: 0.001
 * Train Acc 81.300, Loss 0.297
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 82.510, Loss 0.280
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.71
Epoch:18
LR: 0.001
 * Train Acc 81.740, Loss 0.283
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.70
Epoch:19
LR: 0.001
 * Train Acc 81.730, Loss 0.274
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.66
Epoch:20
LR: 0.001
 * Train Acc 81.220, Loss 0.272
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 81.570, Loss 0.261
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 81.480, Loss 0.254
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.71
Epoch:23
LR: 0.001
 * Train Acc 81.760, Loss 0.248
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.70
Epoch:24
LR: 0.001
 * Train Acc 81.250, Loss 0.243
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.70
Epoch:25
LR: 0.001
 * Train Acc 81.590, Loss 0.236
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.350, time 0.70
Epoch:26
LR: 0.001
 * Train Acc 81.470, Loss 0.230
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.71
Epoch:27
LR: 0.001
 * Train Acc 81.620, Loss 0.222
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.70
Epoch:28
LR: 0.001
 * Train Acc 81.020, Loss 0.215
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.72
Epoch:29
LR: 0.001
 * Train Acc 80.380, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.73
Epoch:30
LR: 0.001
 * Train Acc 80.930, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.73
Epoch:31
LR: 0.001
 * Train Acc 80.130, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.71
Epoch:32
LR: 0.001
 * Train Acc 81.260, Loss 0.207
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 80.700, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.200, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 81.170, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.72
Epoch:35
LR: 0.001
 * Train Acc 80.320, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.71
Epoch:36
LR: 0.001
 * Train Acc 80.300, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.74
Epoch:37
LR: 0.001
 * Train Acc 80.850, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 80.460, Loss 0.211
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.750, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 80.860, Loss 0.209
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.600, time 0.69
Epoch:40
LR: 0.001
 * Train Acc 80.900, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 80.400, Loss 0.212
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 80.220, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 80.780, Loss 0.213
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.100, time 0.71
Epoch:44
LR: 0.001
 * Train Acc 79.660, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.71
Epoch:45
LR: 0.001
 * Train Acc 79.970, Loss 0.216
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.150, time 0.73
Epoch:46
LR: 0.001
 * Train Acc 80.260, Loss 0.214
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.300, time 0.71
Epoch:47
LR: 0.001
 * Train Acc 79.950, Loss 0.217
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.500, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 79.830, Loss 0.220
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.450, time 0.66
Epoch:49
LR: 0.001
 * Train Acc 79.760, Loss 0.219
 * , robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.71
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5362871885299683 - mean: 0.0006207027472555637 - std: 0.00038574874633923173
 * min 0.00025484320940449834, max: 0.0017896585632115602
sum: 11.480366706848145 - mean: 0.001245699473656714 - std: 0.0008107197354547679
 * min 0.00028065391234122217, max: 0.004574848338961601
sum: 4.468029499053955 - mean: 0.0002424061094643548 - std: 0.00011050590546801686
 * min 4.9586804379941896e-05, max: 0.0007328917272388935
sum: 11.493770599365234 - mean: 0.0003117884916719049 - std: 0.00010573256440693513
 * min 6.591910641873255e-05, max: 0.0009566431981511414
sum: 58.1875 - mean: 0.0007892184657976031 - std: 0.00028793522506020963
 * min 0.00018262908270116895, max: 0.0026640777941793203
sum: 203.4814910888672 - mean: 0.0013799471780657768 - std: 0.00028019509045407176
 * min 0.00036586818168871105, max: 0.004403227474540472
sum: 283.0554504394531 - mean: 0.0019195926142856479 - std: 0.0003435820108279586
 * min 0.0004916822072118521, max: 0.005363018251955509
sum: 5.300416469573975 - mean: 6.470234893640736e-06 - std: 4.3180428122013836e-08
 * min 5.270263045531465e-06, max: 6.765038961020764e-06
sum: 0.20000001788139343 - mean: 0.00039062503492459655 - std: 4.512581242011038e-08
 * min 0.0003903601609636098, max: 0.0003908901126123965
eps: tensor([0.0056, 0.0112, 0.0022, 0.0028, 0.0071, 0.0124, 0.0173, 0.0207, 0.0207],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.500, time 0.66
 * Lower 1 Val Acc 89.900, time 0.68
 * Upper 1 Val Acc 89.900, time 0.68
validation split name: 2
 *  Val Acc 78.950, time 0.70
 * Lower 1 Val Acc 78.650, time 0.69
 * Upper 1 Val Acc 78.650, time 0.70
validation split name: 3
 *  Val Acc 80.950, time 0.67
 * Lower 1 Val Acc 81.100, time 0.70
 * Upper 1 Val Acc 81.100, time 0.70
validation split name: 4
 *  Val Acc 76.600, time 0.71
 * Lower 1 Val Acc 75.150, time 0.68
 * Upper 1 Val Acc 75.150, time 0.69
validation split name: 5
 *  Val Acc 78.900, time 0.71
 * Lower 1 Val Acc 78.700, time 0.65
 * Upper 1 Val Acc 78.700, time 0.68
Task 1 average acc: 97.4
Task 2 average acc: 83.4
Task 3 average acc: 82.38333333333334
Task 4 average acc: 82.025
Task 5 average acc: 81.17999999999999
===Summary of experiment repeats: 10 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [75.32 81.2  79.17 79.35 80.97 80.58 79.78 78.36 79.85 81.18]
mean: 79.576 std: 1.6758949847767872
reg_coef: 0.0 mean: 79.576 std: 1.6758949847767872
* kappa decrease from 1 to 0.5 in [30.0, 30.0, 30.0, 30.0, 30.0] epoch
* eps increase by [2.5, 0.9, 0.3, 0.2, 0.1] every [50.0, 50.0, 50.0, 50.0, 50.0] epoch
* maximal eps: [0.0, 0.0, 0.0, 0.0, 0.0]
* tasks were trained [50, 50, 50, 50, 50] epoch with clipping
