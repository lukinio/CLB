split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.368, Loss 0.020
 * robust loss: 0.051 robust error: 0.01538462
 *  Val Acc 99.764, time 0.48
Epoch:1
LR: 0.001
 * Train Acc 99.850, Loss 0.004
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.905, time 0.42
Epoch:2
LR: 0.001
 * Train Acc 99.953, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.811, time 0.42
Epoch:3
LR: 0.001
 * Train Acc 99.937, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 99.984, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.45
Epoch:5
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.47
Epoch:6
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 99.953, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.41
Epoch:9
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.764, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.46
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2662.0751953125 -mean: 0.006499206647276878 - std: 0.0008001934038475156
 * min 0.0031921151094138622, max: 0.007704560179263353
sum: 1603.486083984375 -mean: 0.010021788068115711 - std: 0.0010745686013251543
 * min 0.0056017558090388775, max: 0.012388864532113075
sum: 14.672194480895996 - mean: 0.01834024302661419 - std: 0.0014144711894914508
 * min 0.015106122940778732, max: 0.022271841764450073
sum eps on layers: tensor([6.6552, 4.0087, 7.3361], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.39
 * Lower 1 Val Acc 99.905, time 0.37
 * Upper 1 Val Acc 99.905, time 0.39
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 89.428, Loss 0.387
 * robust loss: 0.147 robust error: 0.02247191
 *  Val Acc 95.054, time 0.43
Epoch:1
LR: 0.001
 * Train Acc 95.268, Loss 0.138
 * robust loss: 0.042 robust error: 0.01123596
 *  Val Acc 95.788, time 0.42
Epoch:2
LR: 0.001
 * Train Acc 95.790, Loss 0.095
 * robust loss: 0.013 robust error: 0.00000000
 *  Val Acc 95.103, time 0.41
Epoch:3
LR: 0.001
 * Train Acc 95.459, Loss 0.093
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 95.984, time 0.45
Epoch:4
LR: 0.001
 * Train Acc 95.136, Loss 0.099
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 95.201, time 0.40
Epoch:5
LR: 0.001
 * Train Acc 94.334, Loss 0.109
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 94.123, time 0.37
Epoch:6
LR: 0.001
 * Train Acc 92.828, Loss 0.123
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.605, time 0.47
Epoch:7
LR: 0.001
 * Train Acc 90.653, Loss 0.141
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 90.010, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 87.733, Loss 0.163
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.023, time 0.42
Epoch:9
LR: 0.001
 * Train Acc 84.192, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.685, time 0.41
Epoch:10
LR: 0.001
 * Train Acc 78.964, Loss 0.224
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 76.592, time 0.42
Epoch:11
LR: 0.001
 * Train Acc 73.703, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.225, time 0.37
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2103.048583984375 -mean: 0.00513439578935504 - std: 0.0015783468261361122
 * min 0.0008588386699557304, max: 0.007891912013292313
sum: 283.7164306640625 -mean: 0.0017732277046889067 - std: 0.0004936495679430664
 * min 0.00036117821582593024, max: 0.0027975973207503557
sum: 4.0661749839782715 - mean: 0.005082718562334776 - std: 0.0035915763583034277
 * min 0.0017209729412570596, max: 0.018412020057439804
sum eps on layers: tensor([5.2576, 0.7093, 2.0331], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.905, time 0.43
 * Lower 1 Val Acc 77.778, time 0.43
 * Upper 1 Val Acc 77.778, time 0.38
validation split name: 2
 *  Val Acc 70.225, time 0.43
 * Lower 1 Val Acc 86.778, time 0.38
 * Upper 1 Val Acc 86.778, time 0.38
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 94.158, Loss 0.202
 * robust loss: 0.068 robust error: 0.00000000
 *  Val Acc 97.012, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 96.422, Loss 0.110
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.279, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 96.235, Loss 0.099
 * robust loss: 0.009 robust error: 0.00000000
 *  Val Acc 96.371, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 96.129, Loss 0.099
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 96.211, time 0.40
Epoch:4
LR: 0.001
 * Train Acc 95.738, Loss 0.102
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 95.998, time 0.35
Epoch:5
LR: 0.001
 * Train Acc 95.516, Loss 0.106
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.678, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 95.037, Loss 0.111
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.464, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 94.673, Loss 0.117
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.304, time 0.44
Epoch:8
LR: 0.001
 * Train Acc 94.353, Loss 0.122
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.970, time 0.43
Epoch:9
LR: 0.001
 * Train Acc 93.492, Loss 0.129
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.504, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 92.924, Loss 0.136
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.863, time 0.43
Epoch:11
LR: 0.001
 * Train Acc 92.320, Loss 0.143
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.850, time 0.40
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1299.819091796875 -mean: 0.0031733864452689886 - std: 0.001171450363472104
 * min 0.00032328616362065077, max: 0.005383568815886974
sum: 85.79499053955078 -mean: 0.0005362186930142343 - std: 0.00016870499530341476
 * min 8.932719356380403e-05, max: 0.0009159122710116208
sum: 1.0719289779663086 - mean: 0.001339911250397563 - std: 0.0011178620625287294
 * min 0.000398181495256722, max: 0.00641245162114501
sum eps on layers: tensor([3.2495, 0.2145, 0.5360], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 74.468, time 0.42
 * Lower 1 Val Acc 31.489, time 0.42
 * Upper 1 Val Acc 31.489, time 0.44
validation split name: 2
 *  Val Acc 92.605, time 0.47
 * Lower 1 Val Acc 90.059, time 0.41
 * Upper 1 Val Acc 90.059, time 0.41
validation split name: 3
 *  Val Acc 92.850, time 0.39
 * Lower 1 Val Acc 88.901, time 0.44
 * Upper 1 Val Acc 88.901, time 0.38
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 89.674, Loss 0.319
 * robust loss: 0.261 robust error: 0.06024096
 *  Val Acc 90.785, time 0.41
Epoch:1
LR: 0.001
 * Train Acc 90.774, Loss 0.259
 * robust loss: 0.150 robust error: 0.00000000
 *  Val Acc 90.685, time 0.47
Epoch:2
LR: 0.001
 * Train Acc 90.840, Loss 0.227
 * robust loss: 0.103 robust error: 0.00000000
 *  Val Acc 90.030, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 90.692, Loss 0.204
 * robust loss: 0.072 robust error: 0.01204819
 *  Val Acc 90.735, time 0.43
Epoch:4
LR: 0.001
 * Train Acc 90.659, Loss 0.190
 * robust loss: 0.047 robust error: 0.00000000
 *  Val Acc 89.879, time 0.42
Epoch:5
LR: 0.001
 * Train Acc 90.479, Loss 0.181
 * robust loss: 0.027 robust error: 0.00000000
 *  Val Acc 90.030, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 90.462, Loss 0.176
 * robust loss: 0.018 robust error: 0.00000000
 *  Val Acc 90.030, time 0.44
Epoch:7
LR: 0.001
 * Train Acc 90.249, Loss 0.173
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 90.282, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 90.117, Loss 0.172
 * robust loss: 0.011 robust error: 0.00000000
 *  Val Acc 89.225, time 0.45
Epoch:9
LR: 0.001
 * Train Acc 89.847, Loss 0.172
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 89.879, time 0.38
Epoch:10
LR: 0.001
 * Train Acc 89.707, Loss 0.172
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 89.275, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 89.444, Loss 0.173
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 89.074, time 0.42
after batch eps: 1.000000000000011, kappa: 0.5
sum: 386.36151123046875 -mean: 0.0009432653896510601 - std: 0.00048439993406645954
 * min 2.0780826162081212e-05, max: 0.0018927713390439749
sum: 4.4075164794921875 -mean: 2.7546977435122244e-05 - std: 1.0233518878521863e-05
 * min 3.102830532952794e-06, max: 5.212093310547061e-05
sum: 0.04615486413240433 - mean: 5.769357812823728e-05 - std: 5.944185613770969e-05
 * min 1.428372797818156e-05, max: 0.00042546793702058494
sum eps on layers: tensor([0.9659, 0.0110, 0.0231], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 83.499, time 0.41
 * Lower 1 Val Acc 81.513, time 0.36
 * Upper 1 Val Acc 81.513, time 0.41
validation split name: 2
 *  Val Acc 87.071, time 0.40
 * Lower 1 Val Acc 87.953, time 0.43
 * Upper 1 Val Acc 87.953, time 0.37
validation split name: 3
 *  Val Acc 84.472, time 0.40
 * Lower 1 Val Acc 83.831, time 0.39
 * Upper 1 Val Acc 83.831, time 0.37
validation split name: 4
 *  Val Acc 89.074, time 0.36
 * Lower 1 Val Acc 89.275, time 0.42
 * Upper 1 Val Acc 89.275, time 0.42
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 43.856, Loss 0.816
 * robust loss: 0.681 robust error: 0.46000000
 *  Val Acc 45.587, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 44.186, Loss 0.749
 * robust loss: 0.646 robust error: 0.41000000
 *  Val Acc 44.629, time 0.41
Epoch:2
LR: 0.001
 * Train Acc 43.831, Loss 0.702
 * robust loss: 0.515 robust error: 0.24000000
 *  Val Acc 44.175, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 43.610, Loss 0.660
 * robust loss: 0.404 robust error: 0.12000000
 *  Val Acc 44.478, time 0.40
Epoch:4
LR: 0.001
 * Train Acc 43.492, Loss 0.623
 * robust loss: 0.368 robust error: 0.11000000
 *  Val Acc 44.226, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 43.110, Loss 0.591
 * robust loss: 0.327 robust error: 0.05000000
 *  Val Acc 44.075, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 42.992, Loss 0.564
 * robust loss: 0.281 robust error: 0.04000000
 *  Val Acc 43.722, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 42.780, Loss 0.541
 * robust loss: 0.219 robust error: 0.03000000
 *  Val Acc 44.175, time 0.48
Epoch:8
LR: 0.001
 * Train Acc 42.534, Loss 0.522
 * robust loss: 0.145 robust error: 0.01000000
 *  Val Acc 43.722, time 0.35
Epoch:9
LR: 0.001
 * Train Acc 42.347, Loss 0.507
 * robust loss: 0.128 robust error: 0.00000000
 *  Val Acc 43.066, time 0.44
Epoch:10
LR: 0.001
 * Train Acc 42.212, Loss 0.494
 * robust loss: 0.094 robust error: 0.00000000
 *  Val Acc 43.469, time 0.41
Epoch:11
LR: 0.001
 * Train Acc 41.941, Loss 0.484
 * robust loss: 0.087 robust error: 0.00000000
 *  Val Acc 43.621, time 0.40
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 198.69418334960938 -mean: 0.0004850932164117694 - std: 0.0002952893264591694
 * min 2.0269119431759464e-06, max: 0.0010727596236392856
sum: 0.4300881624221802 -mean: 2.688050926735741e-06 - std: 1.0803429404404596e-06
 * min 2.232106623978325e-07, max: 5.364948719943641e-06
sum: 0.004378666169941425 - mean: 5.47333274880657e-06 - std: 7.756969353067689e-06
 * min 1.0474311693542404e-06, max: 7.817697041900828e-05
sum eps on layers: tensor([0.4967, 0.0011, 0.0022], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 74.232, time 0.42
 * Lower 1 Val Acc 73.901, time 0.43
 * Upper 1 Val Acc 73.901, time 0.40
validation split name: 2
 *  Val Acc 90.402, time 0.39
 * Lower 1 Val Acc 90.744, time 0.38
 * Upper 1 Val Acc 90.744, time 0.36
validation split name: 3
 *  Val Acc 86.126, time 0.40
 * Lower 1 Val Acc 85.752, time 0.40
 * Upper 1 Val Acc 85.752, time 0.41
validation split name: 4
 *  Val Acc 89.074, time 0.37
 * Lower 1 Val Acc 89.124, time 0.40
 * Upper 1 Val Acc 89.124, time 0.43
validation split name: 5
 *  Val Acc 43.621, time 0.36
 * Lower 1 Val Acc 44.528, time 0.44
 * Upper 1 Val Acc 44.528, time 0.41
Task 1 average acc: 99.95271867612293
Task 2 average acc: 85.06535334801325
Task 3 average acc: 86.64096459422187
Task 4 average acc: 86.02888733742597
Task 5 average acc: 76.69069412255729
===Summary of experiment repeats: 1 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412  0.          0.          0.          0.          0.
  0.          0.          0.          0.        ]
mean: 7.669069412255729 std: 23.00720823676719
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.424, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.41
Epoch:1
LR: 0.001
 * Train Acc 99.913, Loss 0.002
 * robust loss: 0.008 robust error: 0.00000000
 *  Val Acc 99.905, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 99.953, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 100.000, time 0.40
Epoch:3
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.45
Epoch:4
LR: 0.001
 * Train Acc 99.968, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 99.976, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.43
Epoch:6
LR: 0.001
 * Train Acc 99.913, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.38
Epoch:7
LR: 0.001
 * Train Acc 99.929, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.45
Epoch:8
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.37
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2735.68994140625 -mean: 0.0066789304837584496 - std: 0.0008212170214392245
 * min 0.003573026740923524, max: 0.007814502343535423
sum: 1627.897216796875 -mean: 0.010174357332289219 - std: 0.0009856242686510086
 * min 0.006136304698884487, max: 0.012634893879294395
sum: 14.182064056396484 - mean: 0.017727579921483994 - std: 0.0011420899536460638
 * min 0.01507139578461647, max: 0.020910287275910378
sum eps on layers: tensor([6.8392, 4.0697, 7.0910], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.40
 * Lower 1 Val Acc 99.953, time 0.39
 * Upper 1 Val Acc 99.953, time 0.41
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 84.664, Loss 0.438
 * robust loss: 0.163 robust error: 0.02247191
 *  Val Acc 92.948, time 0.42
Epoch:1
LR: 0.001
 * Train Acc 94.044, Loss 0.154
 * robust loss: 0.055 robust error: 0.02247191
 *  Val Acc 94.319, time 0.41
Epoch:2
LR: 0.001
 * Train Acc 94.582, Loss 0.112
 * robust loss: 0.021 robust error: 0.01123596
 *  Val Acc 95.250, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 94.516, Loss 0.107
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 94.760, time 0.35
Epoch:4
LR: 0.001
 * Train Acc 93.837, Loss 0.111
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 93.487, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 92.522, Loss 0.123
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.989, time 0.41
Epoch:6
LR: 0.001
 * Train Acc 91.000, Loss 0.138
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.010, time 0.38
Epoch:7
LR: 0.001
 * Train Acc 87.815, Loss 0.163
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.757, time 0.40
Epoch:8
LR: 0.001
 * Train Acc 84.440, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.517, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 79.585, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 74.721, Loss 0.278
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.184, time 0.46
Epoch:11
LR: 0.001
 * Train Acc 69.998, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.483, time 0.40
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2308.0947265625 -mean: 0.005634996574372053 - std: 0.001844938611611724
 * min 0.0009053547400981188, max: 0.008596809580922127
sum: 273.8714599609375 -mean: 0.001711696619167924 - std: 0.0004623435961548239
 * min 0.00035258338903076947, max: 0.002665189327672124
sum: 3.0901689529418945 - mean: 0.0038627111352980137 - std: 0.0030575417913496494
 * min 0.001288489904254675, max: 0.014960803091526031
sum eps on layers: tensor([5.7702, 0.6847, 1.5451], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.858, time 0.41
 * Lower 1 Val Acc 99.054, time 0.39
 * Upper 1 Val Acc 99.054, time 0.42
validation split name: 2
 *  Val Acc 67.483, time 0.41
 * Lower 1 Val Acc 78.795, time 0.38
 * Upper 1 Val Acc 78.795, time 0.42
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 94.992, Loss 0.178
 * robust loss: 0.045 robust error: 0.00000000
 *  Val Acc 97.705, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 97.221, Loss 0.086
 * robust loss: 0.010 robust error: 0.00000000
 *  Val Acc 97.545, time 0.41
Epoch:2
LR: 0.001
 * Train Acc 97.407, Loss 0.078
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 97.759, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 97.114, Loss 0.079
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 96.692, time 0.40
Epoch:4
LR: 0.001
 * Train Acc 96.972, Loss 0.083
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.172, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 96.528, Loss 0.087
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.852, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 96.253, Loss 0.092
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.211, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 95.818, Loss 0.098
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.211, time 0.52
Epoch:8
LR: 0.001
 * Train Acc 95.312, Loss 0.104
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.784, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 94.655, Loss 0.110
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.664, time 0.36
Epoch:10
LR: 0.001
 * Train Acc 94.140, Loss 0.117
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.130, time 0.35
Epoch:11
LR: 0.001
 * Train Acc 93.465, Loss 0.125
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.170, time 0.40
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1325.168212890625 -mean: 0.0032352739945054054 - std: 0.0012746192514896393
 * min 0.00041642150608822703, max: 0.005153096746653318
sum: 92.21782684326172 -mean: 0.0005763614317402244 - std: 0.00017486534488853067
 * min 9.136411972576752e-05, max: 0.0009499088046140969
sum: 0.9130697250366211 - mean: 0.001141337095759809 - std: 0.0010225061560049653
 * min 0.00034599442733451724, max: 0.005370341707020998
sum eps on layers: tensor([3.3129, 0.2305, 0.4565], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 96.359, time 0.43
 * Lower 1 Val Acc 71.395, time 0.42
 * Upper 1 Val Acc 71.395, time 0.42
validation split name: 2
 *  Val Acc 89.324, time 0.39
 * Lower 1 Val Acc 91.479, time 0.43
 * Upper 1 Val Acc 91.479, time 0.42
validation split name: 3
 *  Val Acc 93.170, time 0.38
 * Lower 1 Val Acc 93.543, time 0.35
 * Upper 1 Val Acc 93.543, time 0.40
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 92.284, Loss 0.279
 * robust loss: 0.178 robust error: 0.01204819
 *  Val Acc 93.706, time 0.43
Epoch:1
LR: 0.001
 * Train Acc 93.573, Loss 0.219
 * robust loss: 0.128 robust error: 0.01204819
 *  Val Acc 93.807, time 0.44
Epoch:2
LR: 0.001
 * Train Acc 93.360, Loss 0.190
 * robust loss: 0.091 robust error: 0.01204819
 *  Val Acc 93.706, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 93.319, Loss 0.170
 * robust loss: 0.048 robust error: 0.00000000
 *  Val Acc 93.555, time 0.42
Epoch:4
LR: 0.001
 * Train Acc 93.081, Loss 0.159
 * robust loss: 0.027 robust error: 0.00000000
 *  Val Acc 93.353, time 0.37
Epoch:5
LR: 0.001
 * Train Acc 93.048, Loss 0.153
 * robust loss: 0.015 robust error: 0.00000000
 *  Val Acc 93.303, time 0.46
Epoch:6
LR: 0.001
 * Train Acc 92.966, Loss 0.150
 * robust loss: 0.010 robust error: 0.00000000
 *  Val Acc 93.303, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 92.900, Loss 0.149
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 93.353, time 0.36
Epoch:8
LR: 0.001
 * Train Acc 92.736, Loss 0.149
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 92.951, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 92.531, Loss 0.150
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 92.850, time 0.39
Epoch:10
LR: 0.001
 * Train Acc 92.522, Loss 0.150
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 92.951, time 0.42
Epoch:11
LR: 0.001
 * Train Acc 92.210, Loss 0.151
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 93.001, time 0.38
after batch eps: 1.000000000000011, kappa: 0.5
sum: 386.59124755859375 -mean: 0.0009438262786716223 - std: 0.0005219914601184428
 * min 2.8474329155869782e-05, max: 0.0017983774887397885
sum: 5.113521099090576 -mean: 3.195950557710603e-05 - std: 1.1944001926167402e-05
 * min 2.2693002392770723e-06, max: 6.088224472478032e-05
sum: 0.0414760559797287 - mean: 5.1845068810507655e-05 - std: 5.813076859340072e-05
 * min 1.2909797987958882e-05, max: 0.0004092888266313821
sum eps on layers: tensor([0.9665, 0.0128, 0.0207], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 79.905, time 0.45
 * Lower 1 Val Acc 79.149, time 0.44
 * Upper 1 Val Acc 79.149, time 0.47
validation split name: 2
 *  Val Acc 85.064, time 0.47
 * Lower 1 Val Acc 84.966, time 0.46
 * Upper 1 Val Acc 84.966, time 0.40
validation split name: 3
 *  Val Acc 85.806, time 0.37
 * Lower 1 Val Acc 85.539, time 0.43
 * Upper 1 Val Acc 85.539, time 0.40
validation split name: 4
 *  Val Acc 93.001, time 0.40
 * Lower 1 Val Acc 92.397, time 0.34
 * Upper 1 Val Acc 92.397, time 0.38
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 41.381, Loss 0.807
 * robust loss: 0.788 robust error: 0.52000000
 *  Val Acc 42.310, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 41.356, Loss 0.735
 * robust loss: 0.557 robust error: 0.31000000
 *  Val Acc 42.259, time 0.40
Epoch:2
LR: 0.001
 * Train Acc 41.220, Loss 0.683
 * robust loss: 0.510 robust error: 0.23000000
 *  Val Acc 41.452, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 41.076, Loss 0.637
 * robust loss: 0.436 robust error: 0.15000000
 *  Val Acc 41.604, time 0.44
Epoch:4
LR: 0.001
 * Train Acc 40.593, Loss 0.597
 * robust loss: 0.340 robust error: 0.04000000
 *  Val Acc 41.150, time 0.43
Epoch:5
LR: 0.001
 * Train Acc 40.415, Loss 0.565
 * robust loss: 0.281 robust error: 0.06000000
 *  Val Acc 40.898, time 0.45
Epoch:6
LR: 0.001
 * Train Acc 40.076, Loss 0.538
 * robust loss: 0.195 robust error: 0.02000000
 *  Val Acc 40.343, time 0.39
Epoch:7
LR: 0.001
 * Train Acc 39.915, Loss 0.517
 * robust loss: 0.182 robust error: 0.03000000
 *  Val Acc 40.040, time 0.43
Epoch:8
LR: 0.001
 * Train Acc 39.729, Loss 0.500
 * robust loss: 0.119 robust error: 0.00000000
 *  Val Acc 39.839, time 0.44
Epoch:9
LR: 0.001
 * Train Acc 39.398, Loss 0.486
 * robust loss: 0.123 robust error: 0.01000000
 *  Val Acc 39.990, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 39.195, Loss 0.476
 * robust loss: 0.083 robust error: 0.00000000
 *  Val Acc 39.284, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 38.915, Loss 0.468
 * robust loss: 0.079 robust error: 0.00000000
 *  Val Acc 39.183, time 0.38
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 199.04598999023438 -mean: 0.00048595209955237806 - std: 0.00032825086964294314
 * min 1.98474094759149e-06, max: 0.0010956274345517159
sum: 0.3771870732307434 -mean: 2.3574191345687723e-06 - std: 9.96935341390781e-07
 * min 1.1408352662556354e-07, max: 4.943427484249696e-06
sum: 0.0028841006569564342 - mean: 3.605125812100596e-06 - std: 5.19122431796859e-06
 * min 6.850800104984955e-07, max: 3.3440839615650475e-05
sum eps on layers: tensor([0.4976, 0.0009, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 73.333, time 0.40
 * Lower 1 Val Acc 73.381, time 0.38
 * Upper 1 Val Acc 73.381, time 0.42
validation split name: 2
 *  Val Acc 89.324, time 0.41
 * Lower 1 Val Acc 89.226, time 0.44
 * Upper 1 Val Acc 89.226, time 0.41
validation split name: 3
 *  Val Acc 88.100, time 0.38
 * Lower 1 Val Acc 87.780, time 0.42
 * Upper 1 Val Acc 87.780, time 0.43
validation split name: 4
 *  Val Acc 92.195, time 0.40
 * Lower 1 Val Acc 92.145, time 0.41
 * Upper 1 Val Acc 92.145, time 0.40
validation split name: 5
 *  Val Acc 39.183, time 0.42
 * Lower 1 Val Acc 38.880, time 0.38
 * Upper 1 Val Acc 38.880, time 0.41
Task 1 average acc: 99.95271867612293
Task 2 average acc: 83.67050798480145
Task 3 average acc: 92.95107351057493
Task 4 average acc: 85.9439676376617
Task 5 average acc: 76.42725380431092
===Summary of experiment repeats: 2 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538   0.          0.          0.          0.
  0.          0.          0.          0.        ]
mean: 15.311794792686822 std: 30.62364624164583
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.392, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.37
Epoch:1
LR: 0.001
 * Train Acc 99.913, Loss 0.003
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:2
LR: 0.001
 * Train Acc 99.929, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 99.984, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.46
Epoch:4
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
Epoch:5
LR: 0.001
 * Train Acc 99.945, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.42
Epoch:6
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.811, time 0.39
Epoch:7
LR: 0.001
 * Train Acc 99.992, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.40
Epoch:8
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 99.976, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 99.929, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.858, time 0.38
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2715.88134765625 -mean: 0.006630569696426392 - std: 0.000799167959485203
 * min 0.0035495914053171873, max: 0.007747875992208719
sum: 1610.498779296875 -mean: 0.01006561703979969 - std: 0.0010103750973939896
 * min 0.006107162218540907, max: 0.012403142638504505
sum: 14.368098258972168 - mean: 0.017960121855139732 - std: 0.000961693178396672
 * min 0.01509002223610878, max: 0.02153935469686985
sum eps on layers: tensor([6.7897, 4.0262, 7.1840], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.858, time 0.42
 * Lower 1 Val Acc 99.905, time 0.45
 * Upper 1 Val Acc 99.905, time 0.40
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 75.713, Loss 0.632
 * robust loss: 0.342 robust error: 0.11235955
 *  Val Acc 86.435, time 0.37
Epoch:1
LR: 0.001
 * Train Acc 88.833, Loss 0.284
 * robust loss: 0.087 robust error: 0.01123596
 *  Val Acc 90.940, time 0.36
Epoch:2
LR: 0.001
 * Train Acc 90.247, Loss 0.193
 * robust loss: 0.022 robust error: 0.00000000
 *  Val Acc 90.353, time 0.40
Epoch:3
LR: 0.001
 * Train Acc 89.892, Loss 0.177
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 89.471, time 0.37
Epoch:4
LR: 0.001
 * Train Acc 88.601, Loss 0.188
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 88.198, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 86.566, Loss 0.207
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 85.896, time 0.43
Epoch:6
LR: 0.001
 * Train Acc 83.837, Loss 0.232
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 83.399, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 80.685, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.040, time 0.53
Epoch:8
LR: 0.001
 * Train Acc 77.260, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.269, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 73.927, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.576, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 70.138, Loss 0.390
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.385, time 0.43
Epoch:11
LR: 0.001
 * Train Acc 66.879, Loss 0.441
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.936, time 0.40
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2445.86181640625 -mean: 0.005971342325210571 - std: 0.001927846111357212
 * min 0.0008266738732345402, max: 0.009185481816530228
sum: 232.8862762451172 -mean: 0.0014555391389876604 - std: 0.0003940315218642354
 * min 0.0002768300473690033, max: 0.0022275529336184263
sum: 2.606259822845459 - mean: 0.003257824806496501 - std: 0.002845394890755415
 * min 0.0009682686650194228, max: 0.01165342889726162
sum eps on layers: tensor([6.1147, 0.5822, 1.3031], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.858, time 0.41
 * Lower 1 Val Acc 99.527, time 0.37
 * Upper 1 Val Acc 99.527, time 0.38
validation split name: 2
 *  Val Acc 64.936, time 0.41
 * Lower 1 Val Acc 70.862, time 0.39
 * Upper 1 Val Acc 70.862, time 0.38
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 89.328, Loss 0.307
 * robust loss: 0.157 robust error: 0.01587302
 *  Val Acc 92.476, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 92.276, Loss 0.175
 * robust loss: 0.032 robust error: 0.00000000
 *  Val Acc 92.903, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 92.187, Loss 0.152
 * robust loss: 0.008 robust error: 0.00000000
 *  Val Acc 92.796, time 0.37
Epoch:3
LR: 0.001
 * Train Acc 91.983, Loss 0.150
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 92.850, time 0.37
Epoch:4
LR: 0.001
 * Train Acc 91.601, Loss 0.153
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 92.102, time 0.42
Epoch:5
LR: 0.001
 * Train Acc 91.228, Loss 0.158
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.049, time 0.38
Epoch:6
LR: 0.001
 * Train Acc 90.624, Loss 0.164
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.409, time 0.35
Epoch:7
LR: 0.001
 * Train Acc 90.189, Loss 0.170
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.822, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 89.372, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.822, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 88.857, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.968, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 88.076, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.741, time 0.36
Epoch:11
LR: 0.001
 * Train Acc 87.011, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.407, time 0.38
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1399.9595947265625 -mean: 0.0034178700298070908 - std: 0.0013503964291885495
 * min 0.00031731699709780514, max: 0.0055636633187532425
sum: 68.26630401611328 -mean: 0.00042666439549066126 - std: 0.00013360742013901472
 * min 6.778129318263382e-05, max: 0.0006882657762616873
sum: 0.658871054649353 - mean: 0.0008235888089984655 - std: 0.0008074087672866881
 * min 0.00021862125140614808, max: 0.0038890645373612642
sum eps on layers: tensor([3.4999, 0.1707, 0.3294], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.061, time 0.37
 * Lower 1 Val Acc 92.577, time 0.38
 * Upper 1 Val Acc 92.577, time 0.40
validation split name: 2
 *  Val Acc 81.538, time 0.39
 * Lower 1 Val Acc 79.628, time 0.37
 * Upper 1 Val Acc 79.628, time 0.37
validation split name: 3
 *  Val Acc 87.407, time 0.37
 * Lower 1 Val Acc 83.031, time 0.39
 * Upper 1 Val Acc 83.031, time 0.40
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 92.736, Loss 0.264
 * robust loss: 0.218 robust error: 0.04819277
 *  Val Acc 94.411, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 93.442, Loss 0.216
 * robust loss: 0.107 robust error: 0.00000000
 *  Val Acc 94.361, time 0.40
Epoch:2
LR: 0.001
 * Train Acc 93.475, Loss 0.191
 * robust loss: 0.088 robust error: 0.01204819
 *  Val Acc 94.159, time 0.37
Epoch:3
LR: 0.001
 * Train Acc 93.433, Loss 0.173
 * robust loss: 0.069 robust error: 0.00000000
 *  Val Acc 93.958, time 0.42
Epoch:4
LR: 0.001
 * Train Acc 93.499, Loss 0.160
 * robust loss: 0.047 robust error: 0.00000000
 *  Val Acc 94.008, time 0.42
Epoch:5
LR: 0.001
 * Train Acc 93.278, Loss 0.152
 * robust loss: 0.027 robust error: 0.00000000
 *  Val Acc 94.008, time 0.41
Epoch:6
LR: 0.001
 * Train Acc 93.351, Loss 0.147
 * robust loss: 0.013 robust error: 0.00000000
 *  Val Acc 93.404, time 0.39
Epoch:7
LR: 0.001
 * Train Acc 93.310, Loss 0.145
 * robust loss: 0.020 robust error: 0.00000000
 *  Val Acc 93.857, time 0.40
Epoch:8
LR: 0.001
 * Train Acc 93.212, Loss 0.144
 * robust loss: 0.009 robust error: 0.00000000
 *  Val Acc 93.656, time 0.42
Epoch:9
LR: 0.001
 * Train Acc 93.138, Loss 0.143
 * robust loss: 0.006 robust error: 0.00000000
 *  Val Acc 93.656, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 93.163, Loss 0.143
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 93.756, time 0.42
Epoch:11
LR: 0.001
 * Train Acc 92.990, Loss 0.144
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 93.756, time 0.42
after batch eps: 1.000000000000011, kappa: 0.5
sum: 387.6378479003906 -mean: 0.0009463814203627408 - std: 0.0005055101937614381
 * min 1.9571007214835845e-05, max: 0.0018376504303887486
sum: 4.763188362121582 -mean: 2.976992618641816e-05 - std: 1.0929161362582818e-05
 * min 3.026361810043454e-06, max: 5.1900246035074815e-05
sum: 0.03799484670162201 - mean: 4.7493558668065816e-05 - std: 5.2795316150877625e-05
 * min 1.0820038369274698e-05, max: 0.00032674544490873814
sum eps on layers: tensor([0.9691, 0.0119, 0.0190], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 94.563, time 0.38
 * Lower 1 Val Acc 93.664, time 0.43
 * Upper 1 Val Acc 93.664, time 0.39
validation split name: 2
 *  Val Acc 79.628, time 0.40
 * Lower 1 Val Acc 79.040, time 0.38
 * Upper 1 Val Acc 79.040, time 0.40
validation split name: 3
 *  Val Acc 84.525, time 0.39
 * Lower 1 Val Acc 82.497, time 0.40
 * Upper 1 Val Acc 82.497, time 0.38
validation split name: 4
 *  Val Acc 93.756, time 0.43
 * Lower 1 Val Acc 93.656, time 0.39
 * Upper 1 Val Acc 93.656, time 0.43
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 40.347, Loss 0.861
 * robust loss: 0.758 robust error: 0.46000000
 *  Val Acc 40.948, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 40.195, Loss 0.801
 * robust loss: 0.593 robust error: 0.39000000
 *  Val Acc 41.150, time 0.35
Epoch:2
LR: 0.001
 * Train Acc 40.017, Loss 0.756
 * robust loss: 0.619 robust error: 0.33000000
 *  Val Acc 40.645, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 39.992, Loss 0.714
 * robust loss: 0.548 robust error: 0.29000000
 *  Val Acc 40.494, time 0.40
Epoch:4
LR: 0.001
 * Train Acc 39.712, Loss 0.677
 * robust loss: 0.441 robust error: 0.17000000
 *  Val Acc 40.393, time 0.35
Epoch:5
LR: 0.001
 * Train Acc 39.568, Loss 0.644
 * robust loss: 0.367 robust error: 0.10000000
 *  Val Acc 39.990, time 0.43
Epoch:6
LR: 0.001
 * Train Acc 39.525, Loss 0.615
 * robust loss: 0.289 robust error: 0.08000000
 *  Val Acc 39.687, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 39.322, Loss 0.590
 * robust loss: 0.289 robust error: 0.07000000
 *  Val Acc 39.486, time 0.43
Epoch:8
LR: 0.001
 * Train Acc 39.203, Loss 0.569
 * robust loss: 0.246 robust error: 0.07000000
 *  Val Acc 39.082, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 39.144, Loss 0.551
 * robust loss: 0.183 robust error: 0.03000000
 *  Val Acc 39.032, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 38.831, Loss 0.536
 * robust loss: 0.176 robust error: 0.02000000
 *  Val Acc 38.679, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 38.788, Loss 0.523
 * robust loss: 0.142 robust error: 0.02000000
 *  Val Acc 38.729, time 0.40
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 199.0946502685547 -mean: 0.00048607090138830245 - std: 0.00031504180515185
 * min 1.4146596640784992e-06, max: 0.0012369141913950443
sum: 0.3646233379840851 -mean: 2.2788958631281275e-06 - std: 9.301883778789488e-07
 * min 1.5564178568183706e-07, max: 4.215428361931117e-06
sum: 0.0027036170940846205 - mean: 3.3795213312259875e-06 - std: 5.0165390348411165e-06
 * min 6.11327777733095e-07, max: 4.77751309517771e-05
sum eps on layers: tensor([0.4977, 0.0009, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 91.915, time 0.39
 * Lower 1 Val Acc 91.915, time 0.38
 * Upper 1 Val Acc 91.915, time 0.39
validation split name: 2
 *  Val Acc 82.958, time 0.32
 * Lower 1 Val Acc 82.517, time 0.39
 * Upper 1 Val Acc 82.517, time 0.37
validation split name: 3
 *  Val Acc 86.393, time 0.34
 * Lower 1 Val Acc 85.219, time 0.41
 * Upper 1 Val Acc 85.219, time 0.34
validation split name: 4
 *  Val Acc 93.605, time 0.40
 * Lower 1 Val Acc 93.857, time 0.42
 * Upper 1 Val Acc 93.857, time 0.41
validation split name: 5
 *  Val Acc 38.729, time 0.39
 * Lower 1 Val Acc 39.738, time 0.44
 * Upper 1 Val Acc 39.738, time 0.45
Task 1 average acc: 99.8581560283688
Task 2 average acc: 82.39724647647627
Task 3 average acc: 89.0019302375506
Task 4 average acc: 88.11795943050818
Task 5 average acc: 78.7199911362753
===Summary of experiment repeats: 3 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114  0.          0.          0.
  0.          0.          0.          0.        ]
mean: 23.183793906314353 std: 35.41827451282583
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.503, Loss 0.023
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.45
Epoch:1
LR: 0.001
 * Train Acc 99.913, Loss 0.004
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 99.953, time 0.43
Epoch:2
LR: 0.001
 * Train Acc 99.945, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.43
Epoch:4
LR: 0.001
 * Train Acc 99.968, Loss 0.001
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 100.000, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 99.976, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 99.937, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.716, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.36
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2656.34716796875 -mean: 0.00648522237315774 - std: 0.000794680614490062
 * min 0.003452387172728777, max: 0.007661324925720692
sum: 1639.78125 -mean: 0.010248632170259953 - std: 0.0010634070495143533
 * min 0.006168270017951727, max: 0.012670774012804031
sum: 14.519353866577148 - mean: 0.01814919151365757 - std: 0.0012793932110071182
 * min 0.015211176127195358, max: 0.021729601547122
sum eps on layers: tensor([6.6409, 4.0995, 7.2597], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.42
 * Lower 1 Val Acc 99.953, time 0.37
 * Upper 1 Val Acc 99.953, time 0.44
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 86.211, Loss 0.433
 * robust loss: 0.155 robust error: 0.05617978
 *  Val Acc 94.221, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 95.219, Loss 0.142
 * robust loss: 0.038 robust error: 0.01123596
 *  Val Acc 93.976, time 0.36
Epoch:2
LR: 0.001
 * Train Acc 94.995, Loss 0.111
 * robust loss: 0.006 robust error: 0.00000000
 *  Val Acc 94.515, time 0.31
Epoch:3
LR: 0.001
 * Train Acc 94.276, Loss 0.112
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 93.438, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 92.795, Loss 0.122
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.793, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 90.586, Loss 0.136
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 88.590, time 0.37
Epoch:6
LR: 0.001
 * Train Acc 88.229, Loss 0.153
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.533, time 0.38
Epoch:7
LR: 0.001
 * Train Acc 84.755, Loss 0.177
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 82.713, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 80.561, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.383, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 76.698, Loss 0.249
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 74.976, time 0.38
Epoch:10
LR: 0.001
 * Train Acc 73.472, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.470, time 0.39
Epoch:11
LR: 0.001
 * Train Acc 70.411, Loss 0.342
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.385, time 0.37
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2175.642822265625 -mean: 0.005311627872288227 - std: 0.0017423620447516441
 * min 0.0007511800504289567, max: 0.00825691781938076
sum: 265.09259033203125 -mean: 0.0016568286810070276 - std: 0.00043946164078079164
 * min 0.00030743787647224963, max: 0.0026811815332621336
sum: 3.796323299407959 - mean: 0.004745404236018658 - std: 0.004533329512923956
 * min 0.0012337552616372705, max: 0.02613639645278454
sum eps on layers: tensor([5.4391, 0.6627, 1.8982], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.38
 * Lower 1 Val Acc 99.811, time 0.41
 * Upper 1 Val Acc 99.811, time 0.37
validation split name: 2
 *  Val Acc 67.385, time 0.38
 * Lower 1 Val Acc 71.303, time 0.38
 * Upper 1 Val Acc 71.303, time 0.41
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 92.249, Loss 0.246
 * robust loss: 0.053 robust error: 0.00000000
 *  Val Acc 95.251, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 95.001, Loss 0.132
 * robust loss: 0.034 robust error: 0.01587302
 *  Val Acc 95.678, time 0.39
Epoch:2
LR: 0.001
 * Train Acc 94.824, Loss 0.116
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 95.251, time 0.37
Epoch:3
LR: 0.001
 * Train Acc 94.504, Loss 0.116
 * robust loss: 0.015 robust error: 0.00000000
 *  Val Acc 95.091, time 0.37
Epoch:4
LR: 0.001
 * Train Acc 94.176, Loss 0.120
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.824, time 0.35
Epoch:5
LR: 0.001
 * Train Acc 93.545, Loss 0.124
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 94.290, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 92.933, Loss 0.130
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.130, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 92.444, Loss 0.136
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.276, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 91.867, Loss 0.142
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.529, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 91.201, Loss 0.149
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.636, time 0.36
Epoch:10
LR: 0.001
 * Train Acc 90.225, Loss 0.157
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.529, time 0.36
Epoch:11
LR: 0.001
 * Train Acc 89.452, Loss 0.164
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.648, time 0.36
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1339.2879638671875 -mean: 0.0032697459682822227 - std: 0.0012914174003526568
 * min 0.0002497653185855597, max: 0.005621681455522776
sum: 73.94294738769531 -mean: 0.0004621434200089425 - std: 0.00013945323007646948
 * min 6.773997301934287e-05, max: 0.0008048139279708266
sum: 0.9338455200195312 - mean: 0.0011673069093376398 - std: 0.0013464833609759808
 * min 0.00025271528284065425, max: 0.008910728618502617
sum eps on layers: tensor([3.3482, 0.1849, 0.4669], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.676, time 0.39
 * Lower 1 Val Acc 95.225, time 0.40
 * Upper 1 Val Acc 95.225, time 0.36
validation split name: 2
 *  Val Acc 87.365, time 0.42
 * Lower 1 Val Acc 90.304, time 0.38
 * Upper 1 Val Acc 90.304, time 0.39
validation split name: 3
 *  Val Acc 89.648, time 0.37
 * Lower 1 Val Acc 85.379, time 0.39
 * Upper 1 Val Acc 85.379, time 0.40
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 89.428, Loss 0.323
 * robust loss: 0.192 robust error: 0.01204819
 *  Val Acc 91.440, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 90.766, Loss 0.265
 * robust loss: 0.167 robust error: 0.01204819
 *  Val Acc 91.138, time 0.40
Epoch:2
LR: 0.001
 * Train Acc 90.544, Loss 0.236
 * robust loss: 0.119 robust error: 0.00000000
 *  Val Acc 90.937, time 0.44
Epoch:3
LR: 0.001
 * Train Acc 90.429, Loss 0.215
 * robust loss: 0.075 robust error: 0.00000000
 *  Val Acc 90.886, time 0.41
Epoch:4
LR: 0.001
 * Train Acc 90.167, Loss 0.200
 * robust loss: 0.077 robust error: 0.01204819
 *  Val Acc 90.332, time 0.36
Epoch:5
LR: 0.001
 * Train Acc 90.019, Loss 0.191
 * robust loss: 0.075 robust error: 0.02409639
 *  Val Acc 90.634, time 0.44
Epoch:6
LR: 0.001
 * Train Acc 89.896, Loss 0.185
 * robust loss: 0.031 robust error: 0.00000000
 *  Val Acc 89.829, time 0.43
Epoch:7
LR: 0.001
 * Train Acc 89.707, Loss 0.182
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 90.433, time 0.35
Epoch:8
LR: 0.001
 * Train Acc 89.535, Loss 0.180
 * robust loss: 0.018 robust error: 0.00000000
 *  Val Acc 90.181, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 89.264, Loss 0.179
 * robust loss: 0.020 robust error: 0.00000000
 *  Val Acc 89.577, time 0.45
Epoch:10
LR: 0.001
 * Train Acc 88.911, Loss 0.179
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 89.476, time 0.44
Epoch:11
LR: 0.001
 * Train Acc 88.738, Loss 0.180
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 89.023, time 0.38
after batch eps: 1.000000000000011, kappa: 0.5
sum: 387.92633056640625 -mean: 0.0009470857330597937 - std: 0.0005136091494932771
 * min 1.3524399037123658e-05, max: 0.0019447484519332647
sum: 3.705875873565674 -mean: 2.3161723220255226e-05 - std: 8.15186467661988e-06
 * min 2.52276231549331e-06, max: 4.453350629773922e-05
sum: 0.04183882847428322 - mean: 5.229853559285402e-05 - std: 8.665245695738122e-05
 * min 8.164775863406248e-06, max: 0.0008295114384964108
sum eps on layers: tensor([0.9698, 0.0093, 0.0209], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 96.076, time 0.40
 * Lower 1 Val Acc 95.697, time 0.38
 * Upper 1 Val Acc 95.697, time 0.40
validation split name: 2
 *  Val Acc 86.386, time 0.37
 * Lower 1 Val Acc 86.876, time 0.37
 * Upper 1 Val Acc 86.876, time 0.43
validation split name: 3
 *  Val Acc 82.444, time 0.38
 * Lower 1 Val Acc 80.896, time 0.38
 * Upper 1 Val Acc 80.896, time 0.40
validation split name: 4
 *  Val Acc 89.023, time 0.40
 * Lower 1 Val Acc 88.973, time 0.38
 * Upper 1 Val Acc 88.973, time 0.40
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 49.534, Loss 0.760
 * robust loss: 0.694 robust error: 0.38000000
 *  Val Acc 52.849, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 49.881, Loss 0.699
 * robust loss: 0.546 robust error: 0.26000000
 *  Val Acc 52.698, time 0.43
Epoch:2
LR: 0.001
 * Train Acc 49.381, Loss 0.657
 * robust loss: 0.519 robust error: 0.27000000
 *  Val Acc 52.194, time 0.42
Epoch:3
LR: 0.001
 * Train Acc 49.119, Loss 0.619
 * robust loss: 0.443 robust error: 0.17000000
 *  Val Acc 51.740, time 0.36
Epoch:4
LR: 0.001
 * Train Acc 48.797, Loss 0.586
 * robust loss: 0.318 robust error: 0.10000000
 *  Val Acc 51.437, time 0.42
Epoch:5
LR: 0.001
 * Train Acc 48.458, Loss 0.557
 * robust loss: 0.306 robust error: 0.07000000
 *  Val Acc 51.387, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 48.220, Loss 0.532
 * robust loss: 0.218 robust error: 0.03000000
 *  Val Acc 51.034, time 0.42
Epoch:7
LR: 0.001
 * Train Acc 48.017, Loss 0.512
 * robust loss: 0.217 robust error: 0.04000000
 *  Val Acc 50.630, time 0.36
Epoch:8
LR: 0.001
 * Train Acc 47.720, Loss 0.495
 * robust loss: 0.193 robust error: 0.06000000
 *  Val Acc 50.328, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 47.576, Loss 0.481
 * robust loss: 0.137 robust error: 0.01000000
 *  Val Acc 50.076, time 0.39
Epoch:10
LR: 0.001
 * Train Acc 47.288, Loss 0.469
 * robust loss: 0.109 robust error: 0.00000000
 *  Val Acc 49.723, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 46.924, Loss 0.460
 * robust loss: 0.113 robust error: 0.00000000
 *  Val Acc 49.420, time 0.40
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 198.76712036132812 -mean: 0.0004852712736465037 - std: 0.0003057893190998584
 * min 1.620902935428603e-06, max: 0.0011539539555087686
sum: 0.3902893662452698 -mean: 2.4393084459006786e-06 - std: 9.227280202139809e-07
 * min 2.0518474741493264e-07, max: 4.846080173592782e-06
sum: 0.004213038831949234 - mean: 5.2662985581264365e-06 - std: 1.050041009875713e-05
 * min 6.357012694024888e-07, max: 0.00010171857138630003
sum eps on layers: tensor([0.4969, 0.0010, 0.0021], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 93.428, time 0.37
 * Lower 1 Val Acc 93.381, time 0.39
 * Upper 1 Val Acc 93.381, time 0.42
validation split name: 2
 *  Val Acc 88.149, time 0.36
 * Lower 1 Val Acc 88.883, time 0.38
 * Upper 1 Val Acc 88.883, time 0.42
validation split name: 3
 *  Val Acc 84.685, time 0.41
 * Lower 1 Val Acc 84.045, time 0.37
 * Upper 1 Val Acc 84.045, time 0.36
validation split name: 4
 *  Val Acc 90.735, time 0.40
 * Lower 1 Val Acc 90.584, time 0.34
 * Upper 1 Val Acc 90.584, time 0.34
validation split name: 5
 *  Val Acc 49.420, time 0.42
 * Lower 1 Val Acc 50.530, time 0.40
 * Upper 1 Val Acc 50.530, time 0.40
Task 1 average acc: 99.95271867612293
Task 2 average acc: 83.66881771220446
Task 3 average acc: 91.8964210692091
Task 4 average acc: 88.48216963768985
Task 5 average acc: 81.28343033563655
===Summary of experiment repeats: 4 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034  0.          0.
  0.          0.          0.          0.        ]
mean: 31.31213693987801 std: 38.36915628897125
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.116, Loss 0.022
 * robust loss: 0.009 robust error: 0.00000000
 *  Val Acc 99.953, time 0.36
Epoch:1
LR: 0.001
 * Train Acc 99.929, Loss 0.003
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:2
LR: 0.001
 * Train Acc 99.937, Loss 0.002
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.622, time 0.43
Epoch:3
LR: 0.001
 * Train Acc 99.937, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 99.953, time 0.71
Epoch:5
LR: 0.001
 * Train Acc 99.984, Loss 0.001
 * robust loss: 0.068 robust error: 0.01538462
 *  Val Acc 99.953, time 0.43
Epoch:6
LR: 0.001
 * Train Acc 99.921, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.44
Epoch:7
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2690.408203125 -mean: 0.006568379234522581 - std: 0.0008280243491753936
 * min 0.003490265691652894, max: 0.007719893008470535
sum: 1605.52587890625 -mean: 0.010034536942839622 - std: 0.0010749665088951588
 * min 0.0061224983073771, max: 0.012460943311452866
sum: 14.520330429077148 - mean: 0.018150413408875465 - std: 0.0011878005461767316
 * min 0.015455172397196293, max: 0.02136206440627575
sum eps on layers: tensor([6.7260, 4.0138, 7.2602], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.43
 * Lower 1 Val Acc 99.953, time 0.40
 * Upper 1 Val Acc 99.953, time 0.48
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 83.861, Loss 0.507
 * robust loss: 0.208 robust error: 0.02247191
 *  Val Acc 91.528, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 93.060, Loss 0.178
 * robust loss: 0.040 robust error: 0.02247191
 *  Val Acc 94.319, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 94.069, Loss 0.123
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 92.997, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 93.027, Loss 0.125
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 92.556, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 91.290, Loss 0.138
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 88.296, time 0.37
Epoch:5
LR: 0.001
 * Train Acc 88.279, Loss 0.158
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.296, time 0.42
Epoch:6
LR: 0.001
 * Train Acc 86.252, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.504, time 0.43
Epoch:7
LR: 0.001
 * Train Acc 83.026, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.342, time 0.38
Epoch:8
LR: 0.001
 * Train Acc 79.022, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.787, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 75.019, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.527, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 71.296, Loss 0.342
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.609, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 67.623, Loss 0.415
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.063, time 0.38
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2172.35400390625 -mean: 0.00530359847471118 - std: 0.0016441369662061334
 * min 0.0008130030473694205, max: 0.008439800702035427
sum: 270.6875305175781 -mean: 0.001691797049716115 - std: 0.00046076433500275016
 * min 0.00033730996074154973, max: 0.002732453402131796
sum: 3.784792900085449 - mean: 0.0047309910878539085 - std: 0.003897693706676364
 * min 0.0014001079834997654, max: 0.01723051443696022
sum eps on layers: tensor([5.4309, 0.6767, 1.8924], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.905, time 0.38
 * Lower 1 Val Acc 99.385, time 0.38
 * Upper 1 Val Acc 99.385, time 0.41
validation split name: 2
 *  Val Acc 66.063, time 0.35
 * Lower 1 Val Acc 66.454, time 0.37
 * Upper 1 Val Acc 66.454, time 0.38
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 92.311, Loss 0.224
 * robust loss: 0.063 robust error: 0.00000000
 *  Val Acc 95.784, time 0.41
Epoch:1
LR: 0.001
 * Train Acc 95.277, Loss 0.114
 * robust loss: 0.026 robust error: 0.01587302
 *  Val Acc 96.105, time 0.33
Epoch:2
LR: 0.001
 * Train Acc 95.117, Loss 0.101
 * robust loss: 0.009 robust error: 0.00000000
 *  Val Acc 95.998, time 0.44
Epoch:3
LR: 0.001
 * Train Acc 94.708, Loss 0.102
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 95.784, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 94.398, Loss 0.105
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 94.771, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 94.176, Loss 0.108
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.664, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 93.643, Loss 0.113
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 93.116, time 0.40
Epoch:7
LR: 0.001
 * Train Acc 93.146, Loss 0.118
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.717, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 92.666, Loss 0.123
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.863, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 92.027, Loss 0.129
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.743, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 91.361, Loss 0.136
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.796, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 90.491, Loss 0.142
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.982, time 0.35
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1298.01708984375 -mean: 0.003168986877426505 - std: 0.0011498368112370372
 * min 0.00030905232415534556, max: 0.005207869689911604
sum: 87.62629699707031 -mean: 0.000547664356417954 - std: 0.0001661308779148385
 * min 8.875874482328072e-05, max: 0.000938809011131525
sum: 1.0717828273773193 - mean: 0.0013397284783422947 - std: 0.001244806102477014
 * min 0.000348060333635658, max: 0.005627219565212727
sum eps on layers: tensor([3.2450, 0.2191, 0.5359], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.355, time 0.39
 * Lower 1 Val Acc 66.478, time 0.36
 * Upper 1 Val Acc 66.478, time 0.40
validation split name: 2
 *  Val Acc 84.182, time 0.40
 * Lower 1 Val Acc 86.435, time 0.40
 * Upper 1 Val Acc 86.435, time 0.40
validation split name: 3
 *  Val Acc 90.982, time 0.36
 * Lower 1 Val Acc 86.286, time 0.38
 * Upper 1 Val Acc 86.286, time 0.37
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 88.279, Loss 0.339
 * robust loss: 0.191 robust error: 0.03614458
 *  Val Acc 90.383, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 89.789, Loss 0.272
 * robust loss: 0.180 robust error: 0.02409639
 *  Val Acc 90.483, time 0.36
Epoch:2
LR: 0.001
 * Train Acc 89.650, Loss 0.238
 * robust loss: 0.123 robust error: 0.03614458
 *  Val Acc 90.332, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 89.617, Loss 0.213
 * robust loss: 0.072 robust error: 0.00000000
 *  Val Acc 89.930, time 0.35
Epoch:4
LR: 0.001
 * Train Acc 89.485, Loss 0.198
 * robust loss: 0.048 robust error: 0.00000000
 *  Val Acc 90.282, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 89.264, Loss 0.190
 * robust loss: 0.026 robust error: 0.00000000
 *  Val Acc 89.728, time 0.41
Epoch:6
LR: 0.001
 * Train Acc 89.017, Loss 0.185
 * robust loss: 0.017 robust error: 0.00000000
 *  Val Acc 89.930, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 88.903, Loss 0.182
 * robust loss: 0.008 robust error: 0.00000000
 *  Val Acc 89.829, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 88.788, Loss 0.181
 * robust loss: 0.019 robust error: 0.00000000
 *  Val Acc 89.728, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 88.615, Loss 0.181
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 89.728, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 88.484, Loss 0.181
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 89.426, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 88.312, Loss 0.182
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 89.376, time 0.43
after batch eps: 1.000000000000011, kappa: 0.5
sum: 385.68902587890625 -mean: 0.0009416235843673348 - std: 0.0004891802091151476
 * min 2.1282110537867993e-05, max: 0.0021037289407104254
sum: 4.665416240692139 -mean: 2.915885124821216e-05 - std: 1.0555216249485966e-05
 * min 2.9578588964795927e-06, max: 5.747043542214669e-05
sum: 0.048227861523628235 - mean: 6.028482675901614e-05 - std: 7.04381600371562e-05
 * min 1.1618815733527299e-05, max: 0.0005204891203902662
sum eps on layers: tensor([0.9642, 0.0117, 0.0241], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.449, time 0.45
 * Lower 1 Val Acc 87.376, time 0.39
 * Upper 1 Val Acc 87.376, time 0.42
validation split name: 2
 *  Val Acc 79.628, time 0.43
 * Lower 1 Val Acc 78.893, time 0.40
 * Upper 1 Val Acc 78.893, time 0.44
validation split name: 3
 *  Val Acc 83.565, time 0.38
 * Lower 1 Val Acc 81.857, time 0.40
 * Upper 1 Val Acc 81.857, time 0.41
validation split name: 4
 *  Val Acc 89.376, time 0.42
 * Lower 1 Val Acc 88.822, time 0.44
 * Upper 1 Val Acc 88.822, time 0.43
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 43.585, Loss 0.845
 * robust loss: 0.751 robust error: 0.47000000
 *  Val Acc 45.890, time 0.36
Epoch:1
LR: 0.001
 * Train Acc 43.737, Loss 0.777
 * robust loss: 0.633 robust error: 0.34000000
 *  Val Acc 45.184, time 0.35
Epoch:2
LR: 0.001
 * Train Acc 43.271, Loss 0.728
 * robust loss: 0.525 robust error: 0.28000000
 *  Val Acc 45.386, time 0.40
Epoch:3
LR: 0.001
 * Train Acc 43.220, Loss 0.684
 * robust loss: 0.441 robust error: 0.20000000
 *  Val Acc 44.327, time 0.40
Epoch:4
LR: 0.001
 * Train Acc 42.864, Loss 0.645
 * robust loss: 0.321 robust error: 0.12000000
 *  Val Acc 44.781, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 42.763, Loss 0.611
 * robust loss: 0.313 robust error: 0.12000000
 *  Val Acc 44.478, time 0.38
Epoch:6
LR: 0.001
 * Train Acc 42.288, Loss 0.583
 * robust loss: 0.239 robust error: 0.03000000
 *  Val Acc 44.327, time 0.42
Epoch:7
LR: 0.001
 * Train Acc 42.271, Loss 0.560
 * robust loss: 0.205 robust error: 0.02000000
 *  Val Acc 44.024, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 41.881, Loss 0.540
 * robust loss: 0.187 robust error: 0.04000000
 *  Val Acc 43.923, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 41.797, Loss 0.525
 * robust loss: 0.120 robust error: 0.00000000
 *  Val Acc 43.318, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 41.593, Loss 0.512
 * robust loss: 0.121 robust error: 0.02000000
 *  Val Acc 42.511, time 0.33
Epoch:11
LR: 0.001
 * Train Acc 41.314, Loss 0.502
 * robust loss: 0.088 robust error: 0.00000000
 *  Val Acc 43.217, time 0.37
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 198.75682067871094 -mean: 0.00048524612793698907 - std: 0.0003022209566552192
 * min 1.988199983316008e-06, max: 0.0015038655837997794
sum: 0.4072827696800232 -mean: 2.545517190810642e-06 - std: 1.0117978490598034e-06
 * min 1.7914399563778716e-07, max: 5.348985268938122e-06
sum: 0.004179593175649643 - mean: 5.2244913604226895e-06 - std: 8.626604540040717e-06
 * min 6.214089580680593e-07, max: 9.72241978161037e-05
sum eps on layers: tensor([0.4969, 0.0010, 0.0021], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.903, time 0.38
 * Lower 1 Val Acc 85.343, time 0.40
 * Upper 1 Val Acc 85.343, time 0.40
validation split name: 2
 *  Val Acc 83.692, time 0.39
 * Lower 1 Val Acc 84.133, time 0.41
 * Upper 1 Val Acc 84.133, time 0.40
validation split name: 3
 *  Val Acc 86.980, time 0.38
 * Lower 1 Val Acc 85.966, time 0.39
 * Upper 1 Val Acc 85.966, time 0.38
validation split name: 4
 *  Val Acc 88.922, time 0.41
 * Lower 1 Val Acc 89.325, time 0.39
 * Upper 1 Val Acc 89.325, time 0.41
validation split name: 5
 *  Val Acc 43.217, time 0.38
 * Lower 1 Val Acc 43.722, time 0.43
 * Upper 1 Val Acc 43.722, time 0.37
Task 1 average acc: 99.95271867612293
Task 2 average acc: 82.98406049786632
Task 3 average acc: 88.50621375278543
Task 4 average acc: 85.75429640473683
Task 5 average acc: 77.9430117665256
===Summary of experiment repeats: 5 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034 77.94301177  0.
  0.          0.          0.          0.        ]
mean: 39.10643811653057 std: 39.125948937128854
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.124, Loss 0.021
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 99.905, time 0.36
Epoch:1
LR: 0.001
 * Train Acc 99.921, Loss 0.003
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:2
LR: 0.001
 * Train Acc 99.882, Loss 0.003
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.40
Epoch:3
LR: 0.001
 * Train Acc 99.929, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.35
Epoch:4
LR: 0.001
 * Train Acc 99.984, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 99.945, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.42
Epoch:7
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.42
Epoch:9
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
Epoch:10
LR: 0.001
 * Train Acc 99.992, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.42
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.38
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2724.998291015625 -mean: 0.006652827840298414 - std: 0.0007929083076305687
 * min 0.0034850838128477335, max: 0.007819856517016888
sum: 1597.4283447265625 -mean: 0.009983927011489868 - std: 0.0010049263946712017
 * min 0.005855555646121502, max: 0.012159790843725204
sum: 14.387866973876953 - mean: 0.01798483356833458 - std: 0.0009596739546395838
 * min 0.0156981460750103, max: 0.020645594224333763
sum eps on layers: tensor([6.8125, 3.9936, 7.1939], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.39
 * Lower 1 Val Acc 99.953, time 0.37
 * Upper 1 Val Acc 99.953, time 0.36
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 86.575, Loss 0.417
 * robust loss: 0.128 robust error: 0.00000000
 *  Val Acc 94.907, time 0.36
Epoch:1
LR: 0.001
 * Train Acc 95.211, Loss 0.154
 * robust loss: 0.029 robust error: 0.00000000
 *  Val Acc 96.229, time 0.37
Epoch:2
LR: 0.001
 * Train Acc 95.831, Loss 0.107
 * robust loss: 0.011 robust error: 0.00000000
 *  Val Acc 96.327, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 95.732, Loss 0.106
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 95.690, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 95.062, Loss 0.114
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 94.662, time 0.40
Epoch:5
LR: 0.001
 * Train Acc 93.862, Loss 0.128
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 94.221, time 0.41
Epoch:6
LR: 0.001
 * Train Acc 91.910, Loss 0.145
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.548, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 88.419, Loss 0.169
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 85.651, time 0.38
Epoch:8
LR: 0.001
 * Train Acc 83.547, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.244, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 77.699, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.045, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 72.802, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.736, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 69.253, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.699, time 0.39
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2308.33642578125 -mean: 0.005635587032884359 - std: 0.0018073099199682474
 * min 0.0008572865626774728, max: 0.008684924803674221
sum: 258.3490905761719 -mean: 0.001614681794308126 - std: 0.00044057986815460026
 * min 0.0003764436114579439, max: 0.0024122062604874372
sum: 3.1665730476379395 - mean: 0.003958216402679682 - std: 0.0035341200418770313
 * min 0.0012475703842937946, max: 0.016412978991866112
sum eps on layers: tensor([5.7708, 0.6459, 1.5833], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.38
 * Lower 1 Val Acc 89.409, time 0.39
 * Upper 1 Val Acc 89.409, time 0.39
validation split name: 2
 *  Val Acc 66.699, time 0.37
 * Lower 1 Val Acc 90.353, time 0.38
 * Upper 1 Val Acc 90.353, time 0.36
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 92.489, Loss 0.232
 * robust loss: 0.095 robust error: 0.01587302
 *  Val Acc 94.504, time 0.38
Epoch:1
LR: 0.001
 * Train Acc 94.495, Loss 0.128
 * robust loss: 0.022 robust error: 0.00000000
 *  Val Acc 94.771, time 0.37
Epoch:2
LR: 0.001
 * Train Acc 94.442, Loss 0.113
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 94.610, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 94.371, Loss 0.113
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 94.557, time 0.36
Epoch:4
LR: 0.001
 * Train Acc 93.998, Loss 0.116
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 94.184, time 0.40
Epoch:5
LR: 0.001
 * Train Acc 93.581, Loss 0.121
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.810, time 0.35
Epoch:6
LR: 0.001
 * Train Acc 93.155, Loss 0.127
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.543, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 92.551, Loss 0.133
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.676, time 0.36
Epoch:8
LR: 0.001
 * Train Acc 91.894, Loss 0.140
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.156, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 91.059, Loss 0.148
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.662, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 90.313, Loss 0.156
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.502, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 88.990, Loss 0.164
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.901, time 0.38
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1369.859619140625 -mean: 0.0033443837892264128 - std: 0.0012975105782970786
 * min 0.0003272878238931298, max: 0.005507020279765129
sum: 74.4875259399414 -mean: 0.0004655470256693661 - std: 0.000145225043524988
 * min 9.427288750885054e-05, max: 0.000734466128051281
sum: 0.7782642841339111 - mean: 0.0009728303411975503 - std: 0.0010012867860496044
 * min 0.0002638773585204035, max: 0.004727116785943508
sum eps on layers: tensor([3.4246, 0.1862, 0.3891], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.842, time 0.41
 * Lower 1 Val Acc 47.234, time 0.40
 * Upper 1 Val Acc 47.234, time 0.41
validation split name: 2
 *  Val Acc 88.688, time 0.38
 * Lower 1 Val Acc 93.879, time 0.41
 * Upper 1 Val Acc 93.879, time 0.39
validation split name: 3
 *  Val Acc 88.901, time 0.38
 * Lower 1 Val Acc 92.049, time 0.37
 * Upper 1 Val Acc 92.049, time 0.38
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 91.981, Loss 0.290
 * robust loss: 0.223 robust error: 0.04819277
 *  Val Acc 93.303, time 0.41
Epoch:1
LR: 0.001
 * Train Acc 93.319, Loss 0.233
 * robust loss: 0.134 robust error: 0.01204819
 *  Val Acc 93.253, time 0.37
Epoch:2
LR: 0.001
 * Train Acc 93.195, Loss 0.206
 * robust loss: 0.089 robust error: 0.00000000
 *  Val Acc 92.800, time 0.42
Epoch:3
LR: 0.001
 * Train Acc 93.064, Loss 0.186
 * robust loss: 0.069 robust error: 0.00000000
 *  Val Acc 93.001, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 93.031, Loss 0.174
 * robust loss: 0.047 robust error: 0.00000000
 *  Val Acc 92.649, time 0.43
Epoch:5
LR: 0.001
 * Train Acc 92.933, Loss 0.166
 * robust loss: 0.024 robust error: 0.00000000
 *  Val Acc 92.649, time 0.35
Epoch:6
LR: 0.001
 * Train Acc 92.834, Loss 0.161
 * robust loss: 0.017 robust error: 0.00000000
 *  Val Acc 92.649, time 0.37
Epoch:7
LR: 0.001
 * Train Acc 92.613, Loss 0.159
 * robust loss: 0.013 robust error: 0.00000000
 *  Val Acc 92.397, time 0.36
Epoch:8
LR: 0.001
 * Train Acc 92.678, Loss 0.158
 * robust loss: 0.010 robust error: 0.00000000
 *  Val Acc 92.346, time 0.37
Epoch:9
LR: 0.001
 * Train Acc 92.563, Loss 0.158
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 92.346, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 92.309, Loss 0.158
 * robust loss: 0.006 robust error: 0.00000000
 *  Val Acc 91.390, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 92.137, Loss 0.159
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 91.994, time 0.42
after batch eps: 1.000000000000011, kappa: 0.5
sum: 388.6576232910156 -mean: 0.0009488711366429925 - std: 0.0005051879561506212
 * min 2.0975010556867346e-05, max: 0.0019033248536288738
sum: 4.07612943649292 -mean: 2.5475808797637e-05 - std: 9.4445586000802e-06
 * min 3.168944658682449e-06, max: 4.4052718294551596e-05
sum: 0.03633126989006996 - mean: 4.5414086343953386e-05 - std: 5.993008744553663e-05
 * min 8.850905032886658e-06, max: 0.00041652651270851493
sum eps on layers: tensor([0.9716, 0.0102, 0.0182], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 93.286, time 0.42
 * Lower 1 Val Acc 92.199, time 0.43
 * Upper 1 Val Acc 92.199, time 0.42
validation split name: 2
 *  Val Acc 85.309, time 0.38
 * Lower 1 Val Acc 86.876, time 0.41
 * Upper 1 Val Acc 86.876, time 0.37
validation split name: 3
 *  Val Acc 79.936, time 0.35
 * Lower 1 Val Acc 80.576, time 0.38
 * Upper 1 Val Acc 80.576, time 0.37
validation split name: 4
 *  Val Acc 91.994, time 0.39
 * Lower 1 Val Acc 92.447, time 0.37
 * Upper 1 Val Acc 92.447, time 0.37
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 46.449, Loss 0.766
 * robust loss: 0.746 robust error: 0.54000000
 *  Val Acc 47.252, time 0.38
Epoch:1
LR: 0.001
 * Train Acc 46.339, Loss 0.709
 * robust loss: 0.588 robust error: 0.28000000
 *  Val Acc 46.798, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 46.186, Loss 0.668
 * robust loss: 0.489 robust error: 0.22000000
 *  Val Acc 46.445, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 45.737, Loss 0.630
 * robust loss: 0.377 robust error: 0.09000000
 *  Val Acc 46.193, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 45.525, Loss 0.597
 * robust loss: 0.343 robust error: 0.05000000
 *  Val Acc 46.142, time 0.85
Epoch:5
LR: 0.001
 * Train Acc 45.263, Loss 0.568
 * robust loss: 0.308 robust error: 0.05000000
 *  Val Acc 45.840, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 45.008, Loss 0.543
 * robust loss: 0.275 robust error: 0.02000000
 *  Val Acc 45.335, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 44.720, Loss 0.522
 * robust loss: 0.203 robust error: 0.02000000
 *  Val Acc 45.234, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 44.492, Loss 0.505
 * robust loss: 0.166 robust error: 0.01000000
 *  Val Acc 45.033, time 0.41
Epoch:9
LR: 0.001
 * Train Acc 44.314, Loss 0.490
 * robust loss: 0.145 robust error: 0.02000000
 *  Val Acc 44.881, time 0.41
Epoch:10
LR: 0.001
 * Train Acc 43.941, Loss 0.478
 * robust loss: 0.124 robust error: 0.00000000
 *  Val Acc 44.730, time 0.41
Epoch:11
LR: 0.001
 * Train Acc 43.678, Loss 0.467
 * robust loss: 0.116 robust error: 0.01000000
 *  Val Acc 44.428, time 0.39
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 199.0789337158203 -mean: 0.00048603254253976047 - std: 0.000308278133161366
 * min 1.6036501619964838e-06, max: 0.001141438726335764
sum: 0.336875319480896 -mean: 2.1054706849099603e-06 - std: 8.577766834605427e-07
 * min 1.6559168614094233e-07, max: 3.8071104881964857e-06
sum: 0.0029209675267338753 - mean: 3.6512092265184037e-06 - std: 6.722932084812783e-06
 * min 4.854595658798644e-07, max: 5.13901213707868e-05
sum eps on layers: tensor([0.4977, 0.0008, 0.0015], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.229, time 0.41
 * Lower 1 Val Acc 79.811, time 0.38
 * Upper 1 Val Acc 79.811, time 0.38
validation split name: 2
 *  Val Acc 91.724, time 0.36
 * Lower 1 Val Acc 92.116, time 0.40
 * Upper 1 Val Acc 92.116, time 0.39
validation split name: 3
 *  Val Acc 81.964, time 0.36
 * Lower 1 Val Acc 81.964, time 0.37
 * Upper 1 Val Acc 81.964, time 0.40
validation split name: 4
 *  Val Acc 92.749, time 0.39
 * Lower 1 Val Acc 92.497, time 0.36
 * Upper 1 Val Acc 92.497, time 0.39
validation split name: 5
 *  Val Acc 44.428, time 0.40
 * Lower 1 Val Acc 44.932, time 0.39
 * Upper 1 Val Acc 44.932, time 0.39
Task 1 average acc: 99.95271867612293
Task 2 average acc: 83.32601653688614
Task 3 average acc: 88.80997194820293
Task 4 average acc: 87.63112415490569
Task 5 average acc: 78.41874164141844
===Summary of experiment repeats: 6 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034 77.94301177 78.41874164
  0.          0.          0.          0.        ]
mean: 46.94831228067241 std: 38.35308671347936
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.218, Loss 0.024
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 99.858, time 0.38
Epoch:1
LR: 0.001
 * Train Acc 99.882, Loss 0.003
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 99.937, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.42
Epoch:3
LR: 0.001
 * Train Acc 99.976, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.858, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 99.968, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 100.000, time 0.37
Epoch:6
LR: 0.001
 * Train Acc 99.992, Loss 0.000
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 100.000, time 0.42
Epoch:7
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.42
Epoch:8
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.811, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 99.968, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2652.0986328125 -mean: 0.006474850233644247 - std: 0.0008347634575329721
 * min 0.0033754713367670774, max: 0.007618642877787352
sum: 1623.970703125 -mean: 0.010149816982448101 - std: 0.001046959892846644
 * min 0.005991275422275066, max: 0.012548527680337429
sum: 14.619653701782227 - mean: 0.018274566158652306 - std: 0.0013252513017505407
 * min 0.014422539621591568, max: 0.023297324776649475
sum eps on layers: tensor([6.6302, 4.0599, 7.3098], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.36
 * Lower 1 Val Acc 99.905, time 0.35
 * Upper 1 Val Acc 99.905, time 0.40
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 86.823, Loss 0.431
 * robust loss: 0.174 robust error: 0.04494382
 *  Val Acc 95.152, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 94.648, Loss 0.157
 * robust loss: 0.051 robust error: 0.02247191
 *  Val Acc 95.690, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 95.268, Loss 0.110
 * robust loss: 0.011 robust error: 0.00000000
 *  Val Acc 96.033, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 95.045, Loss 0.105
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 95.348, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 94.268, Loss 0.114
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.711, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 93.424, Loss 0.128
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 93.438, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 91.844, Loss 0.145
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.940, time 0.40
Epoch:7
LR: 0.001
 * Train Acc 89.395, Loss 0.165
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 88.639, time 0.42
Epoch:8
LR: 0.001
 * Train Acc 86.120, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.427, time 0.35
Epoch:9
LR: 0.001
 * Train Acc 82.546, Loss 0.213
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 80.215, time 0.39
Epoch:10
LR: 0.001
 * Train Acc 77.467, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.976, time 0.35
Epoch:11
LR: 0.001
 * Train Acc 72.504, Loss 0.305
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 69.785, time 0.41
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2134.4697265625 -mean: 0.005211107432842255 - std: 0.001753810909576714
 * min 0.0007267702021636069, max: 0.00836555752903223
sum: 258.202880859375 -mean: 0.0016137679340317845 - std: 0.00044044977403245866
 * min 0.0003295730275567621, max: 0.002755432389676571
sum: 4.036638259887695 - mean: 0.005045797675848007 - std: 0.00474193599075079
 * min 0.0013750734506174922, max: 0.022883491590619087
sum eps on layers: tensor([5.3362, 0.6455, 2.0183], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.41
 * Lower 1 Val Acc 97.967, time 0.44
 * Upper 1 Val Acc 97.967, time 0.39
validation split name: 2
 *  Val Acc 69.785, time 0.36
 * Lower 1 Val Acc 80.362, time 0.37
 * Upper 1 Val Acc 80.362, time 0.37
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 93.314, Loss 0.207
 * robust loss: 0.072 robust error: 0.00000000
 *  Val Acc 96.425, time 0.38
Epoch:1
LR: 0.001
 * Train Acc 96.369, Loss 0.100
 * robust loss: 0.016 robust error: 0.00000000
 *  Val Acc 96.531, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 96.262, Loss 0.090
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 96.531, time 0.35
Epoch:3
LR: 0.001
 * Train Acc 96.200, Loss 0.091
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 96.105, time 0.35
Epoch:4
LR: 0.001
 * Train Acc 95.685, Loss 0.095
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 96.105, time 0.34
Epoch:5
LR: 0.001
 * Train Acc 95.499, Loss 0.099
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 95.464, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 94.877, Loss 0.105
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.091, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 94.495, Loss 0.111
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.397, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 93.723, Loss 0.118
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.650, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 93.075, Loss 0.126
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.850, time 0.39
Epoch:10
LR: 0.001
 * Train Acc 92.409, Loss 0.134
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.956, time 0.35
Epoch:11
LR: 0.001
 * Train Acc 91.432, Loss 0.144
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.729, time 0.42
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1287.718994140625 -mean: 0.0031438451260328293 - std: 0.0012484058970585465
 * min 0.00027570355450734496, max: 0.005195398814976215
sum: 81.64411926269531 -mean: 0.0005102757131680846 - std: 0.0001542487443657592
 * min 8.4392107964959e-05, max: 0.0009392517968080938
sum: 1.153183937072754 - mean: 0.0014414798934012651 - std: 0.0015349604655057192
 * min 0.0003371979692019522, max: 0.008142048493027687
sum eps on layers: tensor([3.2193, 0.2041, 0.5766], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.863, time 0.43
 * Lower 1 Val Acc 63.641, time 0.41
 * Upper 1 Val Acc 63.641, time 0.37
validation split name: 2
 *  Val Acc 89.667, time 0.34
 * Lower 1 Val Acc 90.744, time 0.37
 * Upper 1 Val Acc 90.744, time 0.35
validation split name: 3
 *  Val Acc 91.729, time 0.39
 * Lower 1 Val Acc 93.436, time 0.37
 * Upper 1 Val Acc 93.436, time 0.35
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 88.985, Loss 0.334
 * robust loss: 0.232 robust error: 0.04819277
 *  Val Acc 90.987, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 90.421, Loss 0.271
 * robust loss: 0.156 robust error: 0.04819277
 *  Val Acc 90.937, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 90.076, Loss 0.239
 * robust loss: 0.100 robust error: 0.02409639
 *  Val Acc 90.030, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 89.912, Loss 0.217
 * robust loss: 0.079 robust error: 0.01204819
 *  Val Acc 90.584, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 89.896, Loss 0.202
 * robust loss: 0.039 robust error: 0.00000000
 *  Val Acc 90.081, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 89.658, Loss 0.194
 * robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 90.081, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 89.592, Loss 0.188
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 89.426, time 0.38
Epoch:7
LR: 0.001
 * Train Acc 89.518, Loss 0.186
 * robust loss: 0.031 robust error: 0.01204819
 *  Val Acc 90.232, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 89.362, Loss 0.184
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 89.879, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 89.231, Loss 0.184
 * robust loss: 0.006 robust error: 0.00000000
 *  Val Acc 89.879, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 89.108, Loss 0.184
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 89.527, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 88.730, Loss 0.185
 * robust loss: 0.006 robust error: 0.00000000
 *  Val Acc 89.577, time 0.38
after batch eps: 1.000000000000011, kappa: 0.5
sum: 386.1819763183594 -mean: 0.0009428270859643817 - std: 0.0005234311684034765
 * min 1.7594149539945647e-05, max: 0.0019089862471446395
sum: 4.042840480804443 -mean: 2.5267752789659426e-05 - std: 9.052097993844654e-06
 * min 2.811535296132206e-06, max: 5.2072795369895175e-05
sum: 0.048875901848077774 - mean: 6.109487731009722e-05 - std: 8.086506568361074e-05
 * min 1.1165114301547874e-05, max: 0.0005223682383075356
sum eps on layers: tensor([0.9655, 0.0101, 0.0244], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 93.286, time 0.43
 * Lower 1 Val Acc 92.908, time 0.45
 * Upper 1 Val Acc 92.908, time 0.39
validation split name: 2
 *  Val Acc 83.056, time 0.35
 * Lower 1 Val Acc 83.790, time 0.40
 * Upper 1 Val Acc 83.790, time 0.39
validation split name: 3
 *  Val Acc 83.138, time 0.36
 * Lower 1 Val Acc 83.138, time 0.38
 * Upper 1 Val Acc 83.138, time 0.36
validation split name: 4
 *  Val Acc 89.577, time 0.38
 * Lower 1 Val Acc 89.627, time 0.37
 * Upper 1 Val Acc 89.627, time 0.40
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 41.144, Loss 0.930
 * robust loss: 0.794 robust error: 0.49000000
 *  Val Acc 42.158, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 40.924, Loss 0.854
 * robust loss: 0.689 robust error: 0.40000000
 *  Val Acc 42.158, time 0.40
Epoch:2
LR: 0.001
 * Train Acc 40.958, Loss 0.804
 * robust loss: 0.627 robust error: 0.35000000
 *  Val Acc 42.057, time 0.43
Epoch:3
LR: 0.001
 * Train Acc 40.585, Loss 0.760
 * robust loss: 0.643 robust error: 0.36000000
 *  Val Acc 41.906, time 0.40
Epoch:4
LR: 0.001
 * Train Acc 40.492, Loss 0.720
 * robust loss: 0.422 robust error: 0.19000000
 *  Val Acc 41.654, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 40.237, Loss 0.685
 * robust loss: 0.374 robust error: 0.12000000
 *  Val Acc 40.847, time 0.37
Epoch:6
LR: 0.001
 * Train Acc 40.093, Loss 0.655
 * robust loss: 0.291 robust error: 0.09000000
 *  Val Acc 41.150, time 0.37
Epoch:7
LR: 0.001
 * Train Acc 39.856, Loss 0.629
 * robust loss: 0.249 robust error: 0.05000000
 *  Val Acc 41.301, time 0.36
Epoch:8
LR: 0.001
 * Train Acc 39.669, Loss 0.608
 * robust loss: 0.213 robust error: 0.06000000
 *  Val Acc 40.494, time 0.40
Epoch:9
LR: 0.001
 * Train Acc 39.542, Loss 0.590
 * robust loss: 0.163 robust error: 0.04000000
 *  Val Acc 40.998, time 0.39
Epoch:10
LR: 0.001
 * Train Acc 39.356, Loss 0.576
 * robust loss: 0.139 robust error: 0.02000000
 *  Val Acc 39.889, time 0.34
Epoch:11
LR: 0.001
 * Train Acc 39.102, Loss 0.564
 * robust loss: 0.125 robust error: 0.02000000
 *  Val Acc 40.141, time 0.37
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 198.2574920654297 -mean: 0.00048402705579064786 - std: 0.00030846381559967995
 * min 2.3579118533234578e-06, max: 0.0011138726258650422
sum: 0.5175138711929321 -mean: 3.2344617011403898e-06 - std: 1.2445693755580578e-06
 * min 2.86272097582696e-07, max: 6.86124803905841e-06
sum: 0.006124943494796753 - mean: 7.656179150217213e-06 - std: 1.2641488865483552e-05
 * min 1.0507463912290405e-06, max: 8.438666554866359e-05
sum eps on layers: tensor([0.4956, 0.0013, 0.0031], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.369, time 0.42
 * Lower 1 Val Acc 88.652, time 0.38
 * Upper 1 Val Acc 88.652, time 0.37
validation split name: 2
 *  Val Acc 88.100, time 0.36
 * Lower 1 Val Acc 88.443, time 0.37
 * Upper 1 Val Acc 88.443, time 0.42
validation split name: 3
 *  Val Acc 84.952, time 0.41
 * Lower 1 Val Acc 84.952, time 0.36
 * Upper 1 Val Acc 84.952, time 0.34
validation split name: 4
 *  Val Acc 88.922, time 0.37
 * Lower 1 Val Acc 88.671, time 0.41
 * Upper 1 Val Acc 88.671, time 0.40
validation split name: 5
 *  Val Acc 40.141, time 0.38
 * Lower 1 Val Acc 40.645, time 0.35
 * Upper 1 Val Acc 40.645, time 0.37
Task 1 average acc: 99.95271867612293
Task 2 average acc: 84.86862182581856
Task 3 average acc: 89.0862664655051
Task 4 average acc: 87.26414808254705
Task 5 average acc: 78.096865634301
===Summary of experiment repeats: 7 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034 77.94301177 78.41874164
 78.09686563  0.          0.          0.        ]
mean: 54.75799884410251 std: 35.868884722481646
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.495, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 99.905, Loss 0.003
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.905, time 0.36
Epoch:2
LR: 0.001
 * Train Acc 99.937, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.41
Epoch:3
LR: 0.001
 * Train Acc 99.945, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.37
Epoch:4
LR: 0.001
 * Train Acc 99.929, Loss 0.001
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 99.811, time 0.37
Epoch:5
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:6
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.40
Epoch:7
LR: 0.001
 * Train Acc 99.992, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.42
Epoch:9
LR: 0.001
 * Train Acc 99.929, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.38
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.716, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 99.929, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2799.728515625 -mean: 0.006835274398326874 - std: 0.0008731996640563011
 * min 0.0032536492217332125, max: 0.008167127147316933
sum: 1555.1905517578125 -mean: 0.009719940833747387 - std: 0.001042703166604042
 * min 0.005616254638880491, max: 0.012068821117281914
sum: 14.225406646728516 - mean: 0.017781758680939674 - std: 0.001351710525341332
 * min 0.014689206145703793, max: 0.02155519835650921
sum eps on layers: tensor([6.9993, 3.8880, 7.1127], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.42
 * Lower 1 Val Acc 99.905, time 0.40
 * Upper 1 Val Acc 99.905, time 0.40
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 90.189, Loss 0.354
 * robust loss: 0.168 robust error: 0.03370787
 *  Val Acc 96.033, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 95.732, Loss 0.126
 * robust loss: 0.023 robust error: 0.00000000
 *  Val Acc 96.327, time 0.39
Epoch:2
LR: 0.001
 * Train Acc 95.947, Loss 0.096
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 96.229, time 0.41
Epoch:3
LR: 0.001
 * Train Acc 95.632, Loss 0.094
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 95.984, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 95.434, Loss 0.099
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 95.642, time 0.42
Epoch:5
LR: 0.001
 * Train Acc 94.764, Loss 0.108
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 95.201, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 94.441, Loss 0.114
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.613, time 0.41
Epoch:7
LR: 0.001
 * Train Acc 93.655, Loss 0.126
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.340, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 92.423, Loss 0.142
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 92.752, time 0.37
Epoch:9
LR: 0.001
 * Train Acc 90.835, Loss 0.161
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.744, time 0.42
Epoch:10
LR: 0.001
 * Train Acc 88.014, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.994, time 0.35
Epoch:11
LR: 0.001
 * Train Acc 83.150, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.167, time 0.37
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2350.8447265625 -mean: 0.005739367101341486 - std: 0.0018417936516925693
 * min 0.0008070724434219301, max: 0.008912462741136551
sum: 262.0315856933594 -mean: 0.0016376973362639546 - std: 0.0004383895138744265
 * min 0.0004068859852850437, max: 0.0024942236486822367
sum: 2.935619831085205 - mean: 0.003669524798169732 - std: 0.002875259844586253
 * min 0.0011165785836055875, max: 0.015635300427675247
sum eps on layers: tensor([5.8771, 0.6551, 1.4678], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 94.184, time 0.38
 * Lower 1 Val Acc 46.430, time 0.37
 * Upper 1 Val Acc 46.430, time 0.37
validation split name: 2
 *  Val Acc 80.167, time 0.41
 * Lower 1 Val Acc 91.528, time 0.37
 * Upper 1 Val Acc 91.528, time 0.38
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 94.966, Loss 0.183
 * robust loss: 0.050 robust error: 0.00000000
 *  Val Acc 97.065, time 0.38
Epoch:1
LR: 0.001
 * Train Acc 96.804, Loss 0.096
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.318, time 0.36
Epoch:2
LR: 0.001
 * Train Acc 96.910, Loss 0.087
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 97.225, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 96.742, Loss 0.087
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 96.105, time 0.39
Epoch:4
LR: 0.001
 * Train Acc 96.440, Loss 0.090
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.745, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 96.227, Loss 0.094
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.585, time 0.38
Epoch:6
LR: 0.001
 * Train Acc 96.031, Loss 0.098
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.158, time 0.38
Epoch:7
LR: 0.001
 * Train Acc 95.570, Loss 0.103
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.784, time 0.35
Epoch:8
LR: 0.001
 * Train Acc 95.143, Loss 0.108
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.824, time 0.37
Epoch:9
LR: 0.001
 * Train Acc 94.593, Loss 0.114
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.824, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 94.113, Loss 0.120
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.130, time 0.35
Epoch:11
LR: 0.001
 * Train Acc 93.368, Loss 0.126
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.276, time 0.40
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1356.9560546875 -mean: 0.0033128808718174696 - std: 0.0012403256259858608
 * min 0.00032031157752498984, max: 0.005458281375467777
sum: 81.70313262939453 -mean: 0.0005106445751152933 - std: 0.0001528106367914006
 * min 0.00010732642112998292, max: 0.0008080318802967668
sum: 0.8067036867141724 - mean: 0.001008379622362554 - std: 0.0008901748806238174
 * min 0.00027307687560096383, max: 0.004912633448839188
sum eps on layers: tensor([3.3924, 0.2043, 0.4034], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 45.437, time 0.39
 * Lower 1 Val Acc 45.532, time 0.40
 * Upper 1 Val Acc 45.532, time 0.39
validation split name: 2
 *  Val Acc 93.340, time 0.42
 * Lower 1 Val Acc 90.157, time 0.33
 * Upper 1 Val Acc 90.157, time 0.38
validation split name: 3
 *  Val Acc 93.276, time 0.37
 * Lower 1 Val Acc 91.569, time 0.38
 * Upper 1 Val Acc 91.569, time 0.36
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 88.771, Loss 0.329
 * robust loss: 0.275 robust error: 0.07228916
 *  Val Acc 88.872, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 89.994, Loss 0.264
 * robust loss: 0.169 robust error: 0.04819277
 *  Val Acc 89.527, time 0.37
Epoch:2
LR: 0.001
 * Train Acc 89.953, Loss 0.232
 * robust loss: 0.143 robust error: 0.03614458
 *  Val Acc 89.325, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 89.805, Loss 0.209
 * robust loss: 0.079 robust error: 0.01204819
 *  Val Acc 89.275, time 0.41
Epoch:4
LR: 0.001
 * Train Acc 89.567, Loss 0.195
 * robust loss: 0.042 robust error: 0.00000000
 *  Val Acc 88.973, time 0.40
Epoch:5
LR: 0.001
 * Train Acc 89.551, Loss 0.186
 * robust loss: 0.035 robust error: 0.00000000
 *  Val Acc 88.570, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 89.354, Loss 0.182
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 88.620, time 0.40
Epoch:7
LR: 0.001
 * Train Acc 89.370, Loss 0.179
 * robust loss: 0.018 robust error: 0.00000000
 *  Val Acc 88.620, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 89.214, Loss 0.178
 * robust loss: 0.008 robust error: 0.00000000
 *  Val Acc 88.469, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 88.935, Loss 0.178
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 88.369, time 0.37
Epoch:10
LR: 0.001
 * Train Acc 88.615, Loss 0.179
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 88.016, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 88.459, Loss 0.179
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 87.764, time 0.37
after batch eps: 1.000000000000011, kappa: 0.5
sum: 389.8656005859375 -mean: 0.0009518202859908342 - std: 0.0005158002604730427
 * min 1.7506488802609965e-05, max: 0.001878301496617496
sum: 3.7842459678649902 -mean: 2.3651537048863247e-05 - std: 8.56166661833413e-06
 * min 2.8903850761707872e-06, max: 4.095021722605452e-05
sum: 0.03175089508295059 - mean: 3.968861710745841e-05 - std: 4.5370717998594046e-05
 * min 8.300587069243193e-06, max: 0.00029467398417182267
sum eps on layers: tensor([0.9747, 0.0095, 0.0159], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 89.598, time 0.38
 * Lower 1 Val Acc 88.227, time 0.38
 * Upper 1 Val Acc 88.227, time 0.38
validation split name: 2
 *  Val Acc 89.128, time 0.39
 * Lower 1 Val Acc 91.185, time 0.36
 * Upper 1 Val Acc 91.185, time 0.41
validation split name: 3
 *  Val Acc 83.991, time 0.38
 * Lower 1 Val Acc 85.539, time 0.40
 * Upper 1 Val Acc 85.539, time 0.38
validation split name: 4
 *  Val Acc 87.764, time 0.38
 * Lower 1 Val Acc 87.009, time 0.42
 * Upper 1 Val Acc 87.009, time 0.41
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 42.144, Loss 0.895
 * robust loss: 0.958 robust error: 0.60000000
 *  Val Acc 42.410, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 42.017, Loss 0.822
 * robust loss: 0.663 robust error: 0.31000000
 *  Val Acc 42.713, time 0.42
Epoch:2
LR: 0.001
 * Train Acc 41.847, Loss 0.770
 * robust loss: 0.578 robust error: 0.29000000
 *  Val Acc 42.511, time 0.39
Epoch:3
LR: 0.001
 * Train Acc 41.805, Loss 0.722
 * robust loss: 0.505 robust error: 0.20000000
 *  Val Acc 42.461, time 0.41
Epoch:4
LR: 0.001
 * Train Acc 41.627, Loss 0.680
 * robust loss: 0.364 robust error: 0.11000000
 *  Val Acc 41.957, time 0.41
Epoch:5
LR: 0.001
 * Train Acc 41.237, Loss 0.645
 * robust loss: 0.314 robust error: 0.12000000
 *  Val Acc 42.057, time 0.41
Epoch:6
LR: 0.001
 * Train Acc 41.305, Loss 0.614
 * robust loss: 0.283 robust error: 0.04000000
 *  Val Acc 41.503, time 0.39
Epoch:7
LR: 0.001
 * Train Acc 40.983, Loss 0.589
 * robust loss: 0.194 robust error: 0.03000000
 *  Val Acc 41.755, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 40.703, Loss 0.568
 * robust loss: 0.214 robust error: 0.05000000
 *  Val Acc 41.704, time 0.39
Epoch:9
LR: 0.001
 * Train Acc 40.695, Loss 0.552
 * robust loss: 0.176 robust error: 0.05000000
 *  Val Acc 41.553, time 0.39
Epoch:10
LR: 0.001
 * Train Acc 40.390, Loss 0.538
 * robust loss: 0.110 robust error: 0.01000000
 *  Val Acc 41.351, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 40.288, Loss 0.527
 * robust loss: 0.100 robust error: 0.01000000
 *  Val Acc 41.251, time 0.40
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 199.09378051757812 -mean: 0.0004860687768086791 - std: 0.000314095028443262
 * min 1.6420133306382922e-06, max: 0.001089453580789268
sum: 0.34738630056381226 -mean: 2.1711643967137206e-06 - std: 8.620908715784026e-07
 * min 1.7329702473034558e-07, max: 3.976222160417819e-06
sum: 0.002794236410409212 - mean: 3.492795485726674e-06 - std: 5.361483999877237e-06
 * min 5.639468554363702e-07, max: 3.7450052332133055e-05
sum eps on layers: tensor([0.4977, 0.0009, 0.0014], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 74.421, time 0.40
 * Lower 1 Val Acc 72.955, time 0.42
 * Upper 1 Val Acc 72.955, time 0.37
validation split name: 2
 *  Val Acc 92.850, time 0.39
 * Lower 1 Val Acc 92.948, time 0.38
 * Upper 1 Val Acc 92.948, time 0.36
validation split name: 3
 *  Val Acc 87.460, time 0.36
 * Lower 1 Val Acc 88.367, time 0.34
 * Upper 1 Val Acc 88.367, time 0.37
validation split name: 4
 *  Val Acc 87.613, time 0.39
 * Lower 1 Val Acc 87.009, time 0.39
 * Upper 1 Val Acc 87.009, time 0.39
validation split name: 5
 *  Val Acc 41.251, time 0.39
 * Lower 1 Val Acc 41.402, time 0.38
 * Upper 1 Val Acc 41.402, time 0.38
Task 1 average acc: 99.95271867612293
Task 2 average acc: 87.17545029556615
Task 3 average acc: 77.35120973763537
Task 4 average acc: 87.62055672402653
Task 5 average acc: 76.71897055239621
===Summary of experiment repeats: 8 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034 77.94301177 78.41874164
 78.09686563 76.71897055  0.          0.        ]
mean: 62.429895899342135 std: 31.242654671212836
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.147, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.41
Epoch:1
LR: 0.001
 * Train Acc 99.913, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.35
Epoch:2
LR: 0.001
 * Train Acc 99.953, Loss 0.002
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:3
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.811, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 99.913, Loss 0.002
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 99.953, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.953, time 0.39
Epoch:7
LR: 0.001
 * Train Acc 99.976, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:8
LR: 0.001
 * Train Acc 99.976, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 99.984, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.38
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.41
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2653.03515625 -mean: 0.006477136630564928 - std: 0.0008019412634894252
 * min 0.003435138612985611, max: 0.007610904518514872
sum: 1641.63623046875 -mean: 0.01026022620499134 - std: 0.0010470377746969461
 * min 0.006361105013638735, max: 0.012635068967938423
sum: 14.526643753051758 - mean: 0.018158303573727608 - std: 0.001350671867839992
 * min 0.014675367623567581, max: 0.022564304992556572
sum eps on layers: tensor([6.6326, 4.1041, 7.2633], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.40
 * Lower 1 Val Acc 99.953, time 0.39
 * Upper 1 Val Acc 99.953, time 0.40
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 82.786, Loss 0.453
 * robust loss: 0.177 robust error: 0.02247191
 *  Val Acc 89.961, time 0.40
Epoch:1
LR: 0.001
 * Train Acc 90.851, Loss 0.192
 * robust loss: 0.038 robust error: 0.00000000
 *  Val Acc 89.324, time 0.39
Epoch:2
LR: 0.001
 * Train Acc 90.719, Loss 0.150
 * robust loss: 0.008 robust error: 0.00000000
 *  Val Acc 90.255, time 0.37
Epoch:3
LR: 0.001
 * Train Acc 89.395, Loss 0.150
 * robust loss: 0.006 robust error: 0.00000000
 *  Val Acc 87.414, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 87.005, Loss 0.165
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 85.749, time 0.40
Epoch:5
LR: 0.001
 * Train Acc 84.887, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.664, time 0.38
Epoch:6
LR: 0.001
 * Train Acc 81.917, Loss 0.208
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 79.138, time 0.44
Epoch:7
LR: 0.001
 * Train Acc 79.444, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.277, time 0.41
Epoch:8
LR: 0.001
 * Train Acc 76.747, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.220, time 0.42
Epoch:9
LR: 0.001
 * Train Acc 74.258, Loss 0.304
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 72.625, time 0.36
Epoch:10
LR: 0.001
 * Train Acc 71.842, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.540, time 0.43
Epoch:11
LR: 0.001
 * Train Acc 69.319, Loss 0.399
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.336, time 0.40
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2294.392822265625 -mean: 0.005601544864475727 - std: 0.0018041169969365
 * min 0.0008080715779215097, max: 0.008619007654488087
sum: 261.6363220214844 -mean: 0.001635227003134787 - std: 0.00043768895557150245
 * min 0.00033360751694999635, max: 0.002531215315684676
sum: 3.219852924346924 - mean: 0.004024816211313009 - std: 0.003609756473451853
 * min 0.0011987009784206748, max: 0.014945299364626408
sum eps on layers: tensor([5.7360, 0.6541, 1.6099], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.38
 * Lower 1 Val Acc 99.811, time 0.42
 * Upper 1 Val Acc 99.811, time 0.40
validation split name: 2
 *  Val Acc 67.336, time 0.42
 * Lower 1 Val Acc 70.960, time 0.40
 * Upper 1 Val Acc 70.960, time 0.39
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 93.288, Loss 0.212
 * robust loss: 0.088 robust error: 0.01587302
 *  Val Acc 95.945, time 0.38
Epoch:1
LR: 0.001
 * Train Acc 95.277, Loss 0.112
 * robust loss: 0.025 robust error: 0.00000000
 *  Val Acc 96.105, time 0.39
Epoch:2
LR: 0.001
 * Train Acc 95.649, Loss 0.097
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 96.318, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 95.392, Loss 0.097
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 95.784, time 0.37
Epoch:4
LR: 0.001
 * Train Acc 95.072, Loss 0.101
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 95.784, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 94.699, Loss 0.105
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.037, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 94.087, Loss 0.111
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 94.504, time 0.36
Epoch:7
LR: 0.001
 * Train Acc 93.670, Loss 0.117
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.330, time 0.39
Epoch:8
LR: 0.001
 * Train Acc 92.986, Loss 0.123
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.903, time 0.37
Epoch:9
LR: 0.001
 * Train Acc 92.320, Loss 0.130
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.942, time 0.38
Epoch:10
LR: 0.001
 * Train Acc 91.432, Loss 0.137
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.715, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 90.349, Loss 0.145
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.822, time 0.37
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1332.723388671875 -mean: 0.0032537190709263086 - std: 0.0012356804218143225
 * min 0.00032084554550237954, max: 0.005280421115458012
sum: 85.1304702758789 -mean: 0.0005320654017850757 - std: 0.00015811147750355303
 * min 9.936345304595307e-05, max: 0.0008655715500935912
sum: 0.9107317924499512 - mean: 0.0011384147219359875 - std: 0.0011284119682386518
 * min 0.0003040827577933669, max: 0.0051765297539532185
sum eps on layers: tensor([3.3318, 0.2128, 0.4554], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.298, time 0.40
 * Lower 1 Val Acc 96.927, time 0.40
 * Upper 1 Val Acc 96.927, time 0.40
validation split name: 2
 *  Val Acc 80.215, time 0.40
 * Lower 1 Val Acc 83.497, time 0.41
 * Upper 1 Val Acc 83.497, time 0.39
validation split name: 3
 *  Val Acc 90.822, time 0.37
 * Lower 1 Val Acc 91.302, time 0.35
 * Upper 1 Val Acc 91.302, time 0.38
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 88.976, Loss 0.333
 * robust loss: 0.264 robust error: 0.06024096
 *  Val Acc 90.131, time 0.39
Epoch:1
LR: 0.001
 * Train Acc 90.298, Loss 0.267
 * robust loss: 0.162 robust error: 0.01204819
 *  Val Acc 90.836, time 0.41
Epoch:2
LR: 0.001
 * Train Acc 90.134, Loss 0.233
 * robust loss: 0.131 robust error: 0.03614458
 *  Val Acc 90.383, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 89.814, Loss 0.209
 * robust loss: 0.051 robust error: 0.00000000
 *  Val Acc 90.332, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 89.732, Loss 0.195
 * robust loss: 0.039 robust error: 0.00000000
 *  Val Acc 90.332, time 0.38
Epoch:5
LR: 0.001
 * Train Acc 89.674, Loss 0.187
 * robust loss: 0.048 robust error: 0.01204819
 *  Val Acc 90.584, time 0.40
Epoch:6
LR: 0.001
 * Train Acc 89.469, Loss 0.183
 * robust loss: 0.020 robust error: 0.00000000
 *  Val Acc 90.131, time 0.44
Epoch:7
LR: 0.001
 * Train Acc 89.256, Loss 0.180
 * robust loss: 0.011 robust error: 0.00000000
 *  Val Acc 90.030, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 88.903, Loss 0.180
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 89.829, time 0.37
Epoch:9
LR: 0.001
 * Train Acc 88.812, Loss 0.180
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 89.426, time 0.36
Epoch:10
LR: 0.001
 * Train Acc 88.697, Loss 0.180
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 89.476, time 0.35
Epoch:11
LR: 0.001
 * Train Acc 88.320, Loss 0.182
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 89.426, time 0.36
after batch eps: 1.000000000000011, kappa: 0.5
sum: 386.9210510253906 -mean: 0.0009446314652450383 - std: 0.0004970635054633021
 * min 2.3580236302223057e-05, max: 0.0018252406734973192
sum: 4.703723907470703 -mean: 2.939827390946448e-05 - std: 1.0475574526935816e-05
 * min 3.858815034618601e-06, max: 5.2549738029483706e-05
sum: 0.04187629371881485 - mean: 5.2345367294037715e-05 - std: 6.104504427639768e-05
 * min 1.1733771316357888e-05, max: 0.00030893649091012776
sum eps on layers: tensor([0.9673, 0.0118, 0.0209], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 91.584, time 0.36
 * Lower 1 Val Acc 91.300, time 0.33
 * Upper 1 Val Acc 91.300, time 0.36
validation split name: 2
 *  Val Acc 79.677, time 0.34
 * Lower 1 Val Acc 80.167, time 0.36
 * Upper 1 Val Acc 80.167, time 0.38
validation split name: 3
 *  Val Acc 87.086, time 0.37
 * Lower 1 Val Acc 85.752, time 0.34
 * Upper 1 Val Acc 85.752, time 0.36
validation split name: 4
 *  Val Acc 89.426, time 0.34
 * Lower 1 Val Acc 89.174, time 0.35
 * Upper 1 Val Acc 89.174, time 0.37
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 39.559, Loss 0.880
 * robust loss: 0.743 robust error: 0.53000000
 *  Val Acc 40.847, time 0.34
Epoch:1
LR: 0.001
 * Train Acc 39.585, Loss 0.809
 * robust loss: 0.606 robust error: 0.28000000
 *  Val Acc 40.242, time 0.34
Epoch:2
LR: 0.001
 * Train Acc 39.153, Loss 0.758
 * robust loss: 0.633 robust error: 0.37000000
 *  Val Acc 39.889, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 39.119, Loss 0.712
 * robust loss: 0.456 robust error: 0.19000000
 *  Val Acc 39.637, time 0.38
Epoch:4
LR: 0.001
 * Train Acc 38.822, Loss 0.671
 * robust loss: 0.431 robust error: 0.15000000
 *  Val Acc 39.486, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 38.712, Loss 0.637
 * robust loss: 0.306 robust error: 0.09000000
 *  Val Acc 39.233, time 0.35
Epoch:6
LR: 0.001
 * Train Acc 38.322, Loss 0.608
 * robust loss: 0.263 robust error: 0.05000000
 *  Val Acc 38.931, time 0.37
Epoch:7
LR: 0.001
 * Train Acc 38.492, Loss 0.584
 * robust loss: 0.260 robust error: 0.05000000
 *  Val Acc 38.880, time 0.35
Epoch:8
LR: 0.001
 * Train Acc 38.008, Loss 0.564
 * robust loss: 0.183 robust error: 0.02000000
 *  Val Acc 38.527, time 0.35
Epoch:9
LR: 0.001
 * Train Acc 37.949, Loss 0.547
 * robust loss: 0.137 robust error: 0.01000000
 *  Val Acc 38.225, time 0.32
Epoch:10
LR: 0.001
 * Train Acc 37.746, Loss 0.534
 * robust loss: 0.115 robust error: 0.01000000
 *  Val Acc 38.174, time 0.34
Epoch:11
LR: 0.001
 * Train Acc 37.492, Loss 0.524
 * robust loss: 0.099 robust error: 0.00000000
 *  Val Acc 37.872, time 0.34
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 198.7480010986328 -mean: 0.0004852245911024511 - std: 0.00030217005405575037
 * min 2.2381809685612097e-06, max: 0.0010978111531585455
sum: 0.459481418132782 -mean: 2.871758852052153e-06 - std: 1.1206499266336323e-06
 * min 2.8438438448574743e-07, max: 5.414664428826654e-06
sum: 0.003962553106248379 - mean: 4.953191364620579e-06 - std: 7.252830982906744e-06
 * min 7.991005759322434e-07, max: 3.8101494283182546e-05
sum eps on layers: tensor([0.4969, 0.0011, 0.0020], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 89.173, time 0.35
 * Lower 1 Val Acc 89.125, time 0.33
 * Upper 1 Val Acc 89.125, time 0.38
validation split name: 2
 *  Val Acc 81.881, time 0.34
 * Lower 1 Val Acc 81.783, time 0.35
 * Upper 1 Val Acc 81.783, time 0.36
validation split name: 3
 *  Val Acc 86.980, time 0.38
 * Lower 1 Val Acc 85.752, time 0.34
 * Upper 1 Val Acc 85.752, time 0.35
validation split name: 4
 *  Val Acc 90.081, time 0.34
 * Lower 1 Val Acc 90.181, time 0.35
 * Upper 1 Val Acc 90.181, time 0.37
validation split name: 5
 *  Val Acc 37.872, time 0.34
 * Lower 1 Val Acc 38.326, time 0.34
 * Upper 1 Val Acc 38.326, time 0.35
Task 1 average acc: 99.95271867612293
Task 2 average acc: 83.64433191396745
Task 3 average acc: 89.77837299214582
Task 4 average acc: 86.9432849477135
Task 5 average acc: 77.19705676973044
===Summary of experiment repeats: 9 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034 77.94301177 78.41874164
 78.09686563 76.71897055 77.19705677  0.        ]
mean: 70.14960157631518 std: 23.42151423333877
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalMLP(
  (fc1): LinearInterval(in_features=1024, out_features=400, bias=False)
  (fc2): LinearInterval(in_features=400, out_features=400, bias=False)
  (last): ModuleDict(
    (All): LinearInterval(in_features=400, out_features=2, bias=False)
  )
)
#parameter of model: 1140803
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 99.274, Loss 0.019
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.858, time 0.37
Epoch:1
LR: 0.001
 * Train Acc 99.905, Loss 0.004
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 99.905, time 0.39
Epoch:2
LR: 0.001
 * Train Acc 99.937, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 100.000, time 0.37
Epoch:3
LR: 0.001
 * Train Acc 99.945, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 100.000, time 0.34
Epoch:4
LR: 0.001
 * Train Acc 99.913, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 100.000, time 0.34
Epoch:5
LR: 0.001
 * Train Acc 99.976, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 99.961, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.39
Epoch:7
LR: 0.001
 * Train Acc 99.921, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.35
Epoch:8
LR: 0.001
 * Train Acc 99.976, Loss 0.001
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.34
Epoch:9
LR: 0.001
 * Train Acc 99.992, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.32
Epoch:10
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.953, time 0.42
Epoch:11
LR: 0.001
 * Train Acc 100.000, Loss 0.000
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 99.905, time 0.32
after batch eps: 18.00000000000072, kappa: 0.5
sum: 2661.159912109375 -mean: 0.006496972404420376 - std: 0.000786816468462348
 * min 0.0033780091907829046, max: 0.007663642056286335
sum: 1635.556884765625 -mean: 0.010222230106592178 - std: 0.0011018132790923119
 * min 0.005988006480038166, max: 0.012523668818175793
sum: 14.516416549682617 - mean: 0.018145520240068436 - std: 0.001245694700628519
 * min 0.015273313969373703, max: 0.021608049049973488
sum eps on layers: tensor([6.6529, 4.0889, 7.2582], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.905, time 0.38
 * Lower 1 Val Acc 99.905, time 0.38
 * Upper 1 Val Acc 99.905, time 0.36
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 87.228, Loss 0.444
 * robust loss: 0.153 robust error: 0.01123596
 *  Val Acc 95.739, time 0.33
Epoch:1
LR: 0.001
 * Train Acc 94.582, Loss 0.158
 * robust loss: 0.036 robust error: 0.00000000
 *  Val Acc 95.788, time 0.36
Epoch:2
LR: 0.001
 * Train Acc 95.062, Loss 0.111
 * robust loss: 0.024 robust error: 0.01123596
 *  Val Acc 95.690, time 0.33
Epoch:3
LR: 0.001
 * Train Acc 94.756, Loss 0.107
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 95.446, time 0.35
Epoch:4
LR: 0.001
 * Train Acc 94.325, Loss 0.112
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 93.732, time 0.36
Epoch:5
LR: 0.001
 * Train Acc 93.457, Loss 0.123
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 93.389, time 0.36
Epoch:6
LR: 0.001
 * Train Acc 92.100, Loss 0.138
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.165, time 0.37
Epoch:7
LR: 0.001
 * Train Acc 89.759, Loss 0.162
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 88.639, time 0.37
Epoch:8
LR: 0.001
 * Train Acc 85.921, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.154, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 80.337, Loss 0.227
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 77.375, time 0.38
Epoch:10
LR: 0.001
 * Train Acc 73.745, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.107, time 0.33
Epoch:11
LR: 0.001
 * Train Acc 68.327, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.112, time 0.34
after batch eps: 7.999999999999834, kappa: 0.5
sum: 2144.03955078125 -mean: 0.005234471522271633 - std: 0.0016988847637549043
 * min 0.0007553449831902981, max: 0.008113373070955276
sum: 263.52008056640625 -mean: 0.0016470004338771105 - std: 0.00044591823825612664
 * min 0.0003166558453813195, max: 0.0026122594717890024
sum: 3.9622015953063965 - mean: 0.004952752031385899 - std: 0.004765554331243038
 * min 0.0013757596025243402, max: 0.024094603955745697
sum eps on layers: tensor([5.3601, 0.6588, 1.9811], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 99.953, time 0.37
 * Lower 1 Val Acc 93.759, time 0.35
 * Upper 1 Val Acc 93.759, time 0.33
validation split name: 2
 *  Val Acc 66.112, time 0.35
 * Lower 1 Val Acc 89.275, time 0.34
 * Upper 1 Val Acc 89.275, time 0.35
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 92.356, Loss 0.244
 * robust loss: 0.059 robust error: 0.00000000
 *  Val Acc 96.638, time 0.33
Epoch:1
LR: 0.001
 * Train Acc 95.889, Loss 0.120
 * robust loss: 0.036 robust error: 0.00000000
 *  Val Acc 96.905, time 0.34
Epoch:2
LR: 0.001
 * Train Acc 95.694, Loss 0.107
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 96.531, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 95.774, Loss 0.107
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 96.371, time 0.32
Epoch:4
LR: 0.001
 * Train Acc 95.312, Loss 0.110
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.945, time 0.34
Epoch:5
LR: 0.001
 * Train Acc 94.824, Loss 0.115
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 95.731, time 0.34
Epoch:6
LR: 0.001
 * Train Acc 94.300, Loss 0.121
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.717, time 0.33
Epoch:7
LR: 0.001
 * Train Acc 93.803, Loss 0.127
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.610, time 0.38
Epoch:8
LR: 0.001
 * Train Acc 93.208, Loss 0.133
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 94.237, time 0.38
Epoch:9
LR: 0.001
 * Train Acc 92.648, Loss 0.139
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.063, time 0.34
Epoch:10
LR: 0.001
 * Train Acc 91.698, Loss 0.147
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.330, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 91.157, Loss 0.155
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.982, time 0.37
after batch eps: 3.9999999999999347, kappa: 0.5
sum: 1314.63671875 -mean: 0.003209562273696065 - std: 0.001262831618078053
 * min 0.00028951416607014835, max: 0.005408610682934523
sum: 77.7565689086914 -mean: 0.00048597855493426323 - std: 0.00014924703282304108
 * min 8.240408351412043e-05, max: 0.0008274871506728232
sum: 1.0380332469940186 - mean: 0.0012975415447726846 - std: 0.0015050801448523998
 * min 0.00030055109527893364, max: 0.008965155109763145
sum eps on layers: tensor([3.2866, 0.1944, 0.5190], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 80.520, time 0.36
 * Lower 1 Val Acc 47.943, time 0.39
 * Upper 1 Val Acc 47.943, time 0.71
validation split name: 2
 *  Val Acc 92.067, time 0.60
 * Lower 1 Val Acc 90.304, time 0.60
 * Upper 1 Val Acc 90.304, time 0.35
validation split name: 3
 *  Val Acc 90.982, time 0.34
 * Lower 1 Val Acc 93.116, time 0.31
 * Upper 1 Val Acc 93.116, time 0.31
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 88.271, Loss 0.320
 * robust loss: 0.239 robust error: 0.06024096
 *  Val Acc 90.232, time 0.35
Epoch:1
LR: 0.001
 * Train Acc 89.387, Loss 0.260
 * robust loss: 0.165 robust error: 0.03614458
 *  Val Acc 90.030, time 0.37
Epoch:2
LR: 0.001
 * Train Acc 89.329, Loss 0.228
 * robust loss: 0.076 robust error: 0.00000000
 *  Val Acc 90.081, time 0.36
Epoch:3
LR: 0.001
 * Train Acc 89.149, Loss 0.207
 * robust loss: 0.076 robust error: 0.02409639
 *  Val Acc 89.829, time 0.32
Epoch:4
LR: 0.001
 * Train Acc 88.960, Loss 0.193
 * robust loss: 0.047 robust error: 0.00000000
 *  Val Acc 89.577, time 0.35
Epoch:5
LR: 0.001
 * Train Acc 88.788, Loss 0.185
 * robust loss: 0.024 robust error: 0.00000000
 *  Val Acc 89.627, time 0.32
Epoch:6
LR: 0.001
 * Train Acc 88.558, Loss 0.180
 * robust loss: 0.022 robust error: 0.00000000
 *  Val Acc 89.728, time 0.33
Epoch:7
LR: 0.001
 * Train Acc 88.468, Loss 0.177
 * robust loss: 0.027 robust error: 0.01204819
 *  Val Acc 89.527, time 0.34
Epoch:8
LR: 0.001
 * Train Acc 88.517, Loss 0.175
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 89.376, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 88.213, Loss 0.175
 * robust loss: 0.019 robust error: 0.00000000
 *  Val Acc 88.872, time 0.40
Epoch:10
LR: 0.001
 * Train Acc 88.098, Loss 0.175
 * robust loss: 0.009 robust error: 0.00000000
 *  Val Acc 89.225, time 0.37
Epoch:11
LR: 0.001
 * Train Acc 87.786, Loss 0.176
 * robust loss: 0.008 robust error: 0.00000000
 *  Val Acc 88.671, time 0.34
after batch eps: 1.000000000000011, kappa: 0.5
sum: 386.47979736328125 -mean: 0.0009435541578568518 - std: 0.0005066809826530516
 * min 2.084938569169026e-05, max: 0.0019270728807896376
sum: 4.130026340484619 -mean: 2.581266380730085e-05 - std: 9.428038538317196e-06
 * min 2.927535206254106e-06, max: 4.970268128090538e-05
sum: 0.04695071652531624 - mean: 5.868839434697293e-05 - std: 8.900287502910942e-05
 * min 1.0905368981184438e-05, max: 0.000657463853713125
sum eps on layers: tensor([0.9662, 0.0103, 0.0235], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 94.657, time 0.41
 * Lower 1 Val Acc 93.853, time 0.39
 * Upper 1 Val Acc 93.853, time 0.35
validation split name: 2
 *  Val Acc 87.953, time 0.35
 * Lower 1 Val Acc 88.296, time 0.34
 * Upper 1 Val Acc 88.296, time 0.34
validation split name: 3
 *  Val Acc 84.312, time 0.36
 * Lower 1 Val Acc 82.391, time 0.33
 * Upper 1 Val Acc 82.391, time 0.34
validation split name: 4
 *  Val Acc 88.671, time 0.32
 * Lower 1 Val Acc 89.023, time 0.39
 * Upper 1 Val Acc 89.023, time 0.36
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 42.551, Loss 0.826
 * robust loss: 0.740 robust error: 0.50000000
 *  Val Acc 44.781, time 0.37
Epoch:1
LR: 0.001
 * Train Acc 42.746, Loss 0.759
 * robust loss: 0.639 robust error: 0.40000000
 *  Val Acc 44.629, time 0.38
Epoch:2
LR: 0.001
 * Train Acc 42.492, Loss 0.712
 * robust loss: 0.499 robust error: 0.24000000
 *  Val Acc 44.831, time 0.38
Epoch:3
LR: 0.001
 * Train Acc 42.305, Loss 0.669
 * robust loss: 0.477 robust error: 0.24000000
 *  Val Acc 44.478, time 0.35
Epoch:4
LR: 0.001
 * Train Acc 42.153, Loss 0.632
 * robust loss: 0.403 robust error: 0.17000000
 *  Val Acc 44.125, time 0.39
Epoch:5
LR: 0.001
 * Train Acc 41.864, Loss 0.600
 * robust loss: 0.287 robust error: 0.09000000
 *  Val Acc 44.024, time 0.37
Epoch:6
LR: 0.001
 * Train Acc 41.644, Loss 0.574
 * robust loss: 0.268 robust error: 0.08000000
 *  Val Acc 43.722, time 0.33
Epoch:7
LR: 0.001
 * Train Acc 41.492, Loss 0.552
 * robust loss: 0.203 robust error: 0.01000000
 *  Val Acc 43.671, time 0.38
Epoch:8
LR: 0.001
 * Train Acc 41.271, Loss 0.534
 * robust loss: 0.208 robust error: 0.03000000
 *  Val Acc 43.217, time 0.36
Epoch:9
LR: 0.001
 * Train Acc 41.169, Loss 0.519
 * robust loss: 0.176 robust error: 0.04000000
 *  Val Acc 43.116, time 0.36
Epoch:10
LR: 0.001
 * Train Acc 40.847, Loss 0.508
 * robust loss: 0.143 robust error: 0.02000000
 *  Val Acc 42.814, time 0.40
Epoch:11
LR: 0.001
 * Train Acc 40.686, Loss 0.498
 * robust loss: 0.113 robust error: 0.00000000
 *  Val Acc 42.612, time 0.38
after batch eps: 0.5000000000000091, kappa: 0.5
sum: 198.88906860351562 -mean: 0.00048556900583207607 - std: 0.0003075653512496501
 * min 1.7118928781201248e-06, max: 0.0012122223852202296
sum: 0.34920018911361694 -mean: 2.1825010207976447e-06 - std: 8.739238523958193e-07
 * min 1.760202081868556e-07, max: 4.7203420763253234e-06
sum: 0.0038087645079940557 - mean: 4.7609555622329935e-06 - std: 9.228816452377941e-06
 * min 6.320790930658404e-07, max: 7.515364268328995e-05
sum eps on layers: tensor([0.4972, 0.0009, 0.0019], device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.182, time 0.45
 * Lower 1 Val Acc 81.797, time 0.37
 * Upper 1 Val Acc 81.797, time 0.42
validation split name: 2
 *  Val Acc 92.507, time 0.40
 * Lower 1 Val Acc 92.507, time 0.36
 * Upper 1 Val Acc 92.507, time 0.45
validation split name: 3
 *  Val Acc 87.567, time 0.36
 * Lower 1 Val Acc 86.446, time 0.35
 * Upper 1 Val Acc 86.446, time 0.38
validation split name: 4
 *  Val Acc 89.476, time 0.38
 * Lower 1 Val Acc 89.577, time 0.35
 * Upper 1 Val Acc 89.577, time 0.38
validation split name: 5
 *  Val Acc 42.612, time 0.33
 * Lower 1 Val Acc 43.419, time 0.36
 * Upper 1 Val Acc 43.419, time 0.37
Task 1 average acc: 99.90543735224587
Task 2 average acc: 83.03218695804188
Task 3 average acc: 87.85618430808245
Task 4 average acc: 88.89813135104724
Task 5 average acc: 78.66892382993908
===Summary of experiment repeats: 10 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.69069412 76.4272538  78.71999114 81.28343034 77.94301177 78.41874164
 78.09686563 76.71897055 77.19705677 78.66892383]
mean: 78.01649395930909 std: 1.3566717552216514
reg_coef: 0.0 mean: 78.01649395930909 std: 1.3566717552216514
* kappa decrease from 1 to 0.5 in [1.0, 1.0, 1.0, 1.0, 1.0] epoch
* eps increase by [18.0, 8.0, 4.0, 1.0, 0.5] every [12.0, 12.0, 12.0, 12.0, 12.0] epoch
* maximal eps: [0.0, 0.0, 0.0, 0.0, 0.0]
* tasks were trained [12, 12, 12, 12, 12] epoch with clipping
