Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.010, Loss 0.570
 * robust loss: 0.252 robust error: 0.03000000
 *  Val Acc 82.200, time 0.63
Epoch:1
LR: 0.001
 * Train Acc 81.860, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.250, time 0.65
Epoch:2
LR: 0.001
 * Train Acc 85.640, Loss 0.333
 * robust loss: 0.226 robust error: 0.01000000
 *  Val Acc 86.900, time 0.64
Epoch:3
LR: 0.001
 * Train Acc 87.120, Loss 0.301
 * robust loss: 0.107 robust error: 0.01000000
 *  Val Acc 89.600, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 89.490, Loss 0.244
 * robust loss: 0.191 robust error: 0.01000000
 *  Val Acc 92.150, time 0.64
Epoch:5
LR: 0.001
 * Train Acc 90.960, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.800, time 0.61
Epoch:6
LR: 0.001
 * Train Acc 92.230, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.250, time 0.65
Epoch:7
LR: 0.001
 * Train Acc 93.090, Loss 0.153
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.800, time 0.62
Epoch:8
LR: 0.001
 * Train Acc 94.110, Loss 0.141
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.150, time 0.68
Epoch:9
LR: 0.001
 * Train Acc 94.670, Loss 0.127
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.350, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 95.530, Loss 0.101
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 96.850, time 0.64
Epoch:11
LR: 0.001
 * Train Acc 95.660, Loss 0.135
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.950, time 0.64
Epoch:12
LR: 0.001
 * Train Acc 96.320, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.63
Epoch:13
LR: 0.001
 * Train Acc 96.400, Loss 0.084
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 96.510, Loss 0.079
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.400, time 0.63
Epoch:15
LR: 0.001
 * Train Acc 96.920, Loss 0.072
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.64
Epoch:16
LR: 0.001
 * Train Acc 97.030, Loss 0.346
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.900, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 96.750, Loss 0.120
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.66
Epoch:18
LR: 0.001
 * Train Acc 97.060, Loss 0.061
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 97.510, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.63
Epoch:20
LR: 0.001
 * Train Acc 97.000, Loss 16.224
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.000, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 97.250, Loss 0.052
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.250, time 0.63
Epoch:22
LR: 0.001
 * Train Acc 97.580, Loss 0.050
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 97.880, Loss 0.042
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.66
Epoch:24
LR: 0.001
 * Train Acc 97.820, Loss 0.042
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.64
Epoch:25
LR: 0.001
 * Train Acc 97.830, Loss 0.040
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 97.860, Loss 1.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.800, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 97.550, Loss 0.042
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.66
Epoch:28
LR: 0.001
 * Train Acc 98.050, Loss 0.043
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.63
Epoch:29
LR: 0.001
 * Train Acc 98.340, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 98.210, Loss 0.031
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.950, time 0.63
Epoch:31
LR: 0.001
 * Train Acc 98.230, Loss 73.602
 * robust loss: 18307.229 robust error: 0.03000000
 *  Val Acc 98.000, time 0.65
Epoch:32
LR: 0.001
 * Train Acc 97.940, Loss 67.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.64
Epoch:33
LR: 0.001
 * Train Acc 98.330, Loss 2.558
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.650, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 98.370, Loss 39.234
 * robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 98.100, time 0.64
Epoch:35
LR: 0.001
 * Train Acc 97.920, Loss 21.831
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 98.310, Loss 0.025
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 98.430, Loss 5.127
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.400, time 0.62
Epoch:38
LR: 0.001
 * Train Acc 98.550, Loss 0.081
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.62
Epoch:39
LR: 0.001
 * Train Acc 98.630, Loss 0.019
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.400, time 0.63
Epoch:40
LR: 0.001
 * Train Acc 98.620, Loss 0.019
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.400, time 0.62
Epoch:41
LR: 0.001
 * Train Acc 98.400, Loss 1.458
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 98.390, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.63
Epoch:43
LR: 0.001
 * Train Acc 98.640, Loss 6.198
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 98.520, Loss 0.027
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 98.660, Loss 0.018
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.64
Epoch:46
LR: 0.001
 * Train Acc 98.680, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.62
Epoch:47
LR: 0.001
 * Train Acc 98.790, Loss 0.017
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.63
Epoch:48
LR: 0.001
 * Train Acc 98.710, Loss 0.067
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.65
Epoch:49
LR: 0.001
 * Train Acc 98.650, Loss 0.017
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.62
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.08250617980957 - mean: 0.015141789801418781 - std: 0.005320208612829447
 * min 0.008908734656870365, max: 0.02958022803068161
sum: 306.86065673828125 - mean: 0.03329651430249214 - std: 0.014629866927862167
 * min 0.012105626054108143, max: 0.08104633539915085
sum: 188.29067993164062 - mean: 0.010215423069894314 - std: 0.0030530693475157022
 * min 0.004054726101458073, max: 0.021620120853185654
sum: 553.1347045898438 - mean: 0.015004739165306091 - std: 0.0036896478850394487
 * min 0.005740920547395945, max: 0.030147584155201912
sum: 2666.78076171875 - mean: 0.03617053106427193 - std: 0.010493342764675617
 * min 0.012939141131937504, max: 0.07859722524881363
sum: 5221.60498046875 - mean: 0.035411275923252106 - std: 0.007748336065560579
 * min 0.014241348952054977, max: 0.07948967069387436
sum: 5920.01171875 - mean: 0.040147650986909866 - std: 0.007797399070113897
 * min 0.013985831290483475, max: 0.09132985025644302
sum: 106.41842651367188 - mean: 0.00012990529648959637 - std: 6.602759299312311e-07
 * min 0.00011306370288366452, max: 0.00013364867481868714
sum: 5.0 - mean: 0.009765625 - std: 0.00544921075925231
 * min 0.0026773118879646063, max: 0.03691291809082031
eps: tensor([0.1363, 0.2997, 0.0919, 0.1350, 0.3255, 0.3187, 0.3613, 0.4157, 0.4158],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.900, time 0.66
 * Lower 1 Val Acc 50.150, time 0.63
 * Upper 1 Val Acc 50.150, time 0.63
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 52.160, Loss 0.681
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 56.000, time 0.63
Epoch:1
LR: 0.001
 * Train Acc 66.710, Loss 0.543
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 79.120, Loss 0.441
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 80.270, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.63
Epoch:4
LR: 0.001
 * Train Acc 81.500, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.63
Epoch:5
LR: 0.001
 * Train Acc 82.210, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 83.040, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 82.930, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.63
Epoch:8
LR: 0.001
 * Train Acc 83.980, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 83.820, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.64
Epoch:10
LR: 0.001
 * Train Acc 84.220, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.65
Epoch:11
LR: 0.001
 * Train Acc 84.850, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 84.900, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 84.310, Loss 1.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 83.810, Loss 0.338
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 84.520, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 85.250, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 84.810, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.64
Epoch:18
LR: 0.001
 * Train Acc 85.190, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.64
Epoch:19
LR: 0.001
 * Train Acc 85.390, Loss 0.257
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.63
Epoch:20
LR: 0.001
 * Train Acc 85.040, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.63
Epoch:21
LR: 0.001
 * Train Acc 85.190, Loss 0.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.62
Epoch:22
LR: 0.001
 * Train Acc 85.220, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.62
Epoch:23
LR: 0.001
 * Train Acc 85.290, Loss 0.264
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.65
Epoch:24
LR: 0.001
 * Train Acc 85.180, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.63
Epoch:25
LR: 0.001
 * Train Acc 85.760, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 85.500, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 85.520, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 85.840, Loss 0.214
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 85.800, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.63
Epoch:30
LR: 0.001
 * Train Acc 85.240, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.65
Epoch:31
LR: 0.001
 * Train Acc 85.590, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 85.710, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.65
Epoch:33
LR: 0.001
 * Train Acc 86.130, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.64
Epoch:34
LR: 0.001
 * Train Acc 85.580, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.63
Epoch:35
LR: 0.001
 * Train Acc 85.370, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.62
Epoch:36
LR: 0.001
 * Train Acc 85.820, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.62
Epoch:37
LR: 0.001
 * Train Acc 85.660, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 85.210, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.63
Epoch:39
LR: 0.001
 * Train Acc 85.980, Loss 0.168
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.64
Epoch:40
LR: 0.001
 * Train Acc 85.470, Loss 0.166
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.63
Epoch:41
LR: 0.001
 * Train Acc 85.460, Loss 0.168
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.62
Epoch:42
LR: 0.001
 * Train Acc 85.460, Loss 19.310
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 84.500, time 0.63
Epoch:43
LR: 0.001
 * Train Acc 84.820, Loss 1.745
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 84.910, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.64
Epoch:45
LR: 0.001
 * Train Acc 85.590, Loss 0.169
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.62
Epoch:46
LR: 0.001
 * Train Acc 85.110, Loss 0.171
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 85.650, Loss 0.167
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 84.810, Loss 9.985
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.62
Epoch:49
LR: 0.001
 * Train Acc 84.640, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.63
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.645139694213867 - mean: 0.00537631893530488 - std: 0.0028151425067335367
 * min 0.0026063856203109026, max: 0.013581041246652603
sum: 122.76264953613281 - mean: 0.013320599682629108 - std: 0.0076431711204349995
 * min 0.0034217885695397854, max: 0.04321807250380516
sum: 52.23085403442383 - mean: 0.002833705162629485 - std: 0.0010549678700044751
 * min 0.000753062660805881, max: 0.007333028130233288
sum: 148.4745330810547 - mean: 0.004027629736810923 - std: 0.0011365854879841208
 * min 0.0012818749528378248, max: 0.010275762528181076
sum: 793.2373046875 - mean: 0.010758969932794571 - std: 0.0035516528878360987
 * min 0.003277151845395565, max: 0.02738051675260067
sum: 1796.5831298828125 - mean: 0.012183859013020992 - std: 0.0030663064680993557
 * min 0.004230014514178038, max: 0.03150835633277893
sum: 2196.83154296875 - mean: 0.014898217283189297 - std: 0.0033570285886526108
 * min 0.004691889509558678, max: 0.03743709996342659
sum: 42.15334701538086 - mean: 5.145672184880823e-05 - std: 2.7806646585304406e-07
 * min 4.473597800824791e-05, max: 5.302706631482579e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.002099009696394205
 * min 0.0009132603299804032, max: 0.014051653444766998
eps: tensor([0.0484, 0.1199, 0.0255, 0.0362, 0.0968, 0.1097, 0.1341, 0.1647, 0.1647],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 84.450, time 0.64
 * Lower 1 Val Acc 77.100, time 0.68
 * Upper 1 Val Acc 77.100, time 0.62
validation split name: 2
 *  Val Acc 84.800, time 0.65
 * Lower 1 Val Acc 64.550, time 0.62
 * Upper 1 Val Acc 64.550, time 0.63
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 81.650, Loss 0.416
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.65
Epoch:1
LR: 0.001
 * Train Acc 84.930, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.60
Epoch:2
LR: 0.001
 * Train Acc 85.440, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.64
Epoch:3
LR: 0.001
 * Train Acc 85.610, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 0.64
Epoch:4
LR: 0.001
 * Train Acc 85.860, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 0.64
Epoch:5
LR: 0.001
 * Train Acc 85.770, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.64
Epoch:6
LR: 0.001
 * Train Acc 85.840, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.350, time 0.63
Epoch:7
LR: 0.001
 * Train Acc 86.540, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.450, time 0.65
Epoch:8
LR: 0.001
 * Train Acc 86.280, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.800, time 0.62
Epoch:9
LR: 0.001
 * Train Acc 86.070, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 85.470, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.64
Epoch:11
LR: 0.001
 * Train Acc 86.360, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.500, time 0.65
Epoch:12
LR: 0.001
 * Train Acc 86.340, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.500, time 0.62
Epoch:13
LR: 0.001
 * Train Acc 85.930, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.750, time 0.64
Epoch:14
LR: 0.001
 * Train Acc 86.580, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 0.65
Epoch:15
LR: 0.001
 * Train Acc 86.600, Loss 0.266
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 86.150, Loss 0.262
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.000, time 0.63
Epoch:17
LR: 0.001
 * Train Acc 86.310, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.62
Epoch:18
LR: 0.001
 * Train Acc 85.610, Loss 0.261
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 85.930, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.950, time 0.63
Epoch:20
LR: 0.001
 * Train Acc 86.120, Loss 0.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 0.65
Epoch:21
LR: 0.001
 * Train Acc 85.820, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 0.63
Epoch:22
LR: 0.001
 * Train Acc 85.980, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.200, time 0.61
Epoch:23
LR: 0.001
 * Train Acc 85.790, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.63
Epoch:24
LR: 0.001
 * Train Acc 86.400, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 0.62
Epoch:25
LR: 0.001
 * Train Acc 85.470, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.800, time 0.61
Epoch:26
LR: 0.001
 * Train Acc 85.920, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 85.950, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 0.64
Epoch:28
LR: 0.001
 * Train Acc 85.640, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.62
Epoch:29
LR: 0.001
 * Train Acc 85.610, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.62
Epoch:30
LR: 0.001
 * Train Acc 85.900, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.66
Epoch:31
LR: 0.001
 * Train Acc 85.630, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 85.060, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 0.63
Epoch:33
LR: 0.001
 * Train Acc 85.270, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.250, time 0.64
Epoch:34
LR: 0.001
 * Train Acc 85.940, Loss 0.193
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.65
Epoch:35
LR: 0.001
 * Train Acc 85.720, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.64
Epoch:36
LR: 0.001
 * Train Acc 85.750, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.61
Epoch:37
LR: 0.001
 * Train Acc 85.570, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.63
Epoch:38
LR: 0.001
 * Train Acc 84.640, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.62
Epoch:39
LR: 0.001
 * Train Acc 85.760, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.64
Epoch:40
LR: 0.001
 * Train Acc 85.520, Loss 0.170
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 0.66
Epoch:41
LR: 0.001
 * Train Acc 84.880, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.61
Epoch:42
LR: 0.001
 * Train Acc 85.290, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.63
Epoch:43
LR: 0.001
 * Train Acc 85.100, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.62
Epoch:44
LR: 0.001
 * Train Acc 84.890, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.62
Epoch:45
LR: 0.001
 * Train Acc 85.020, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.63
Epoch:46
LR: 0.001
 * Train Acc 85.420, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.63
Epoch:47
LR: 0.001
 * Train Acc 84.910, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.63
Epoch:48
LR: 0.001
 * Train Acc 84.980, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 0.62
Epoch:49
LR: 0.001
 * Train Acc 84.980, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.65
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.4887877702713013 - mean: 0.001723133958876133 - std: 0.0009157672175206244
 * min 0.0008300053887069225, max: 0.0043930429965257645
sum: 42.23301696777344 - mean: 0.004582575522363186 - std: 0.0026642950251698494
 * min 0.0011596382828429341, max: 0.015147246420383453
sum: 17.057415008544922 - mean: 0.0009254239848814905 - std: 0.000349003414157778
 * min 0.00024276335898321122, max: 0.0024029056075960398
sum: 48.79966735839844 - mean: 0.0013237757375463843 - std: 0.0003766734735108912
 * min 0.0004108773428015411, max: 0.0034057009033858776
sum: 260.9932861328125 - mean: 0.0035399480257183313 - std: 0.001173760974779725
 * min 0.0010661256965249777, max: 0.009049933403730392
sum: 596.547607421875 - mean: 0.004045597277581692 - std: 0.0010200162651017308
 * min 0.0014033993938937783, max: 0.010478870011866093
sum: 733.15380859375 - mean: 0.004972017370164394 - std: 0.0011220659362152219
 * min 0.0015651872381567955, max: 0.012499313801527023
sum: 14.074922561645508 - mean: 1.7181300790980458e-05 - std: 9.285035673656239e-08
 * min 1.4937249943614006e-05, max: 1.7705666323308833e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0006997575983405113
 * min 0.0003046635538339615, max: 0.0046838014386594296
eps: tensor([0.0155, 0.0412, 0.0083, 0.0119, 0.0319, 0.0364, 0.0447, 0.0550, 0.0550],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 84.050, time 0.63
 * Lower 1 Val Acc 82.900, time 0.62
 * Upper 1 Val Acc 82.900, time 0.67
validation split name: 2
 *  Val Acc 78.900, time 0.63
 * Lower 1 Val Acc 78.550, time 0.64
 * Upper 1 Val Acc 78.550, time 0.64
validation split name: 3
 *  Val Acc 84.300, time 0.65
 * Lower 1 Val Acc 83.600, time 0.67
 * Upper 1 Val Acc 83.600, time 0.63
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 76.770, Loss 0.493
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.64
Epoch:1
LR: 0.001
 * Train Acc 78.370, Loss 0.462
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 78.790, Loss 0.448
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 82.500, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 77.770, Loss 0.448
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.62
Epoch:4
LR: 0.001
 * Train Acc 78.500, Loss 0.441
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.64
Epoch:5
LR: 0.001
 * Train Acc 78.110, Loss 0.433
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 78.890, Loss 0.428
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.63
Epoch:7
LR: 0.001
 * Train Acc 78.110, Loss 0.424
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.62
Epoch:8
LR: 0.001
 * Train Acc 77.490, Loss 0.426
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 77.730, Loss 0.418
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.63
Epoch:10
LR: 0.001
 * Train Acc 77.590, Loss 0.416
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.63
Epoch:11
LR: 0.001
 * Train Acc 77.140, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.63
Epoch:12
LR: 0.001
 * Train Acc 77.410, Loss 0.408
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 77.400, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 76.950, Loss 0.397
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 76.460, Loss 0.397
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 76.320, Loss 0.392
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 76.970, Loss 0.379
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.63
Epoch:18
LR: 0.001
 * Train Acc 75.590, Loss 0.386
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 77.010, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.64
Epoch:20
LR: 0.001
 * Train Acc 76.160, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.63
Epoch:21
LR: 0.001
 * Train Acc 75.860, Loss 0.370
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.63
Epoch:22
LR: 0.001
 * Train Acc 75.990, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.64
Epoch:23
LR: 0.001
 * Train Acc 75.780, Loss 0.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.64
Epoch:24
LR: 0.001
 * Train Acc 75.440, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 0.64
Epoch:25
LR: 0.001
 * Train Acc 74.750, Loss 0.352
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 78.450, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 74.980, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.65
Epoch:27
LR: 0.001
 * Train Acc 74.430, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.600, time 0.64
Epoch:28
LR: 0.001
 * Train Acc 74.610, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 74.030, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.500, time 0.65
Epoch:30
LR: 0.001
 * Train Acc 73.940, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.700, time 0.64
Epoch:31
LR: 0.001
 * Train Acc 73.800, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.650, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 73.430, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.550, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 73.200, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.200, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 73.150, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.350, time 0.64
Epoch:35
LR: 0.001
 * Train Acc 72.730, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.63
Epoch:36
LR: 0.001
 * Train Acc 72.260, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.450, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 71.830, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.100, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 72.420, Loss 0.289
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.900, time 0.66
Epoch:39
LR: 0.001
 * Train Acc 71.590, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.900, time 0.65
Epoch:40
LR: 0.001
 * Train Acc 71.240, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.200, time 0.63
Epoch:41
LR: 0.001
 * Train Acc 71.100, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.500, time 0.65
Epoch:42
LR: 0.001
 * Train Acc 71.170, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.150, time 0.66
Epoch:43
LR: 0.001
 * Train Acc 70.730, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.850, time 0.63
Epoch:44
LR: 0.001
 * Train Acc 70.550, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.400, time 0.65
Epoch:45
LR: 0.001
 * Train Acc 70.010, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.550, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 70.340, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.100, time 0.73
Epoch:47
LR: 0.001
 * Train Acc 69.440, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.300, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 69.900, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.650, time 0.64
Epoch:49
LR: 0.001
 * Train Acc 69.220, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.750, time 0.63
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.1059117317199707 - mean: 0.0012799904216080904 - std: 0.0007987383869476616
 * min 0.0005380172515287995, max: 0.00363996927626431
sum: 27.905517578125 - mean: 0.0030279424972832203 - std: 0.0019490774720907211
 * min 0.0006475960835814476, max: 0.011447591707110405
sum: 10.03421401977539 - mean: 0.0005443909321911633 - std: 0.00022124344832263887
 * min 0.0001241548015968874, max: 0.0015197210013866425
sum: 29.19249725341797 - mean: 0.0007918971823528409 - std: 0.00023240521841216832
 * min 0.0002254521823488176, max: 0.002211822895333171
sum: 154.0576934814453 - mean: 0.002089541172608733 - std: 0.000706382270436734
 * min 0.000567823473829776, max: 0.0055705830454826355
sum: 352.1232604980469 - mean: 0.0023879886139184237 - std: 0.0006116259610280395
 * min 0.0007696652319282293, max: 0.006570266559720039
sum: 500.5589294433594 - mean: 0.00339463260024786 - std: 0.000786477467045188
 * min 0.000998636824078858, max: 0.008804097771644592
sum: 10.026073455810547 - mean: 1.2238859198987484e-05 - std: 6.639053395929295e-08
 * min 1.0636782462825067e-05, max: 1.261542274733074e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 0.0005026566213928163
 * min 0.00018763529078569263, max: 0.003367133205756545
eps: tensor([0.0115, 0.0273, 0.0049, 0.0071, 0.0188, 0.0215, 0.0306, 0.0392, 0.0392],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 79.550, time 0.66
 * Lower 1 Val Acc 79.100, time 0.63
 * Upper 1 Val Acc 79.100, time 0.67
validation split name: 2
 *  Val Acc 76.000, time 0.68
 * Lower 1 Val Acc 75.950, time 0.62
 * Upper 1 Val Acc 75.950, time 0.65
validation split name: 3
 *  Val Acc 80.800, time 0.65
 * Lower 1 Val Acc 80.150, time 0.66
 * Upper 1 Val Acc 80.150, time 0.64
validation split name: 4
 *  Val Acc 72.750, time 0.65
 * Lower 1 Val Acc 71.950, time 0.66
 * Upper 1 Val Acc 71.950, time 0.73
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 82.200, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.64
Epoch:1
LR: 0.001
 * Train Acc 83.480, Loss 0.370
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.63
Epoch:2
LR: 0.001
 * Train Acc 84.280, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 83.460, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.69
Epoch:4
LR: 0.001
 * Train Acc 83.920, Loss 0.355
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.64
Epoch:5
LR: 0.001
 * Train Acc 83.800, Loss 0.355
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.65
Epoch:6
LR: 0.001
 * Train Acc 84.010, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.65
Epoch:7
LR: 0.001
 * Train Acc 83.700, Loss 0.340
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.920, Loss 0.342
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.62
Epoch:9
LR: 0.001
 * Train Acc 83.660, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 83.680, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.64
Epoch:11
LR: 0.001
 * Train Acc 82.980, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.64
Epoch:12
LR: 0.001
 * Train Acc 83.590, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.63
Epoch:13
LR: 0.001
 * Train Acc 83.360, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.62
Epoch:14
LR: 0.001
 * Train Acc 83.420, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 83.290, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 83.180, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 82.950, Loss 0.304
 * robust loss: 0.042 robust error: 0.01000000
 *  Val Acc 84.050, time 0.62
Epoch:18
LR: 0.001
 * Train Acc 83.450, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.66
Epoch:19
LR: 0.001
 * Train Acc 82.890, Loss 0.294
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.66
Epoch:20
LR: 0.001
 * Train Acc 83.170, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.65
Epoch:21
LR: 0.001
 * Train Acc 82.960, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.68
Epoch:22
LR: 0.001
 * Train Acc 82.820, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.67
Epoch:23
LR: 0.001
 * Train Acc 82.780, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 82.510, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 82.530, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.65
Epoch:26
LR: 0.001
 * Train Acc 83.120, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 83.050, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.63
Epoch:28
LR: 0.001
 * Train Acc 82.980, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.62
Epoch:29
LR: 0.001
 * Train Acc 82.380, Loss 0.251
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.63
Epoch:30
LR: 0.001
 * Train Acc 82.160, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.64
Epoch:31
LR: 0.001
 * Train Acc 82.280, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 82.170, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.65
Epoch:33
LR: 0.001
 * Train Acc 82.520, Loss 0.233
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 82.280, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.67
Epoch:35
LR: 0.001
 * Train Acc 81.730, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.65
Epoch:36
LR: 0.001
 * Train Acc 82.340, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.65
Epoch:37
LR: 0.001
 * Train Acc 81.850, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 81.890, Loss 0.214
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.65
Epoch:39
LR: 0.001
 * Train Acc 81.530, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.65
Epoch:40
LR: 0.001
 * Train Acc 81.770, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 81.430, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.63
Epoch:42
LR: 0.001
 * Train Acc 81.650, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 81.700, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 81.190, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.65
Epoch:45
LR: 0.001
 * Train Acc 81.040, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.66
Epoch:46
LR: 0.001
 * Train Acc 81.240, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 81.060, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 81.290, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.67
Epoch:49
LR: 0.001
 * Train Acc 81.060, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.64
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5171312093734741 - mean: 0.0005985315074212849 - std: 0.0003774894867092371
 * min 0.00023342494387179613, max: 0.001722619985230267
sum: 13.69093132019043 - mean: 0.0014855611370876431 - std: 0.0009976039873436093
 * min 0.00029874788015149534, max: 0.006099488120526075
sum: 4.460151195526123 - mean: 0.00024197869061026722 - std: 0.00010219997056992725
 * min 5.18712040502578e-05, max: 0.0007059671333990991
sum: 13.476800918579102 - mean: 0.00036558162537403405 - std: 0.00010893226863117889
 * min 0.00010385984205640852, max: 0.0010281566064804792
sum: 75.50997161865234 - mean: 0.0010241694981232285 - std: 0.00034934922587126493
 * min 0.0002783078816719353, max: 0.0028490961994975805
sum: 177.3268280029297 - mean: 0.0012025745818391442 - std: 0.0003105505311395973
 * min 0.0003734561032615602, max: 0.0033662558998912573
sum: 255.579345703125 - mean: 0.0017332583665847778 - std: 0.000406925129937008
 * min 0.0005006718565709889, max: 0.004681805614382029
sum: 5.135712623596191 - mean: 6.269180175877409e-06 - std: 3.410246307566922e-08
 * min 5.4442871260107495e-06, max: 6.462589681177633e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 0.00027316794148646295
 * min 8.443045226158574e-05, max: 0.0018365744035691023
eps: tensor([0.0054, 0.0134, 0.0022, 0.0033, 0.0092, 0.0108, 0.0156, 0.0201, 0.0201],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.100, time 0.64
 * Lower 1 Val Acc 84.700, time 0.64
 * Upper 1 Val Acc 84.700, time 0.65
validation split name: 2
 *  Val Acc 74.950, time 0.64
 * Lower 1 Val Acc 75.250, time 0.63
 * Upper 1 Val Acc 75.250, time 0.65
validation split name: 3
 *  Val Acc 74.200, time 0.64
 * Lower 1 Val Acc 74.000, time 0.64
 * Upper 1 Val Acc 74.000, time 0.65
validation split name: 4
 *  Val Acc 68.250, time 0.65
 * Lower 1 Val Acc 69.200, time 0.68
 * Upper 1 Val Acc 69.200, time 0.65
validation split name: 5
 *  Val Acc 80.800, time 0.67
 * Lower 1 Val Acc 81.200, time 0.63
 * Upper 1 Val Acc 81.200, time 0.64
Task 1 average acc: 97.9
Task 2 average acc: 84.625
Task 3 average acc: 82.41666666666667
Task 4 average acc: 77.275
Task 5 average acc: 76.66
===Summary of experiment repeats: 1 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 7.6659999999999995 std: 22.997999999999998
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 62.550, Loss 0.620
 * robust loss: 0.099 robust error: 0.02000000
 *  Val Acc 73.250, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 77.400, Loss 0.469
 * robust loss: 0.131 robust error: 0.03000000
 *  Val Acc 84.300, time 0.66
Epoch:2
LR: 0.001
 * Train Acc 83.380, Loss 0.367
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 86.800, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 85.950, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.500, time 0.64
Epoch:4
LR: 0.001
 * Train Acc 88.510, Loss 0.267
 * robust loss: 0.202 robust error: 0.01000000
 *  Val Acc 90.800, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 89.660, Loss 0.236
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 91.600, time 0.62
Epoch:6
LR: 0.001
 * Train Acc 91.440, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.250, time 0.65
Epoch:7
LR: 0.001
 * Train Acc 93.110, Loss 0.164
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.800, time 0.64
Epoch:8
LR: 0.001
 * Train Acc 93.730, Loss 0.154
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.950, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 94.680, Loss 0.120
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.650, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 95.090, Loss 0.116
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 95.200, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 95.380, Loss 0.118
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.900, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 95.720, Loss 0.471
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.500, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 96.070, Loss 0.092
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.550, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 96.570, Loss 0.078
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.950, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 96.270, Loss 1.078
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 96.440, Loss 0.077
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.400, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 96.880, Loss 0.078
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 96.960, Loss 0.066
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 96.780, Loss 0.069
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.65
Epoch:20
LR: 0.001
 * Train Acc 97.290, Loss 0.054
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.64
Epoch:21
LR: 0.001
 * Train Acc 97.430, Loss 0.050
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 97.600, Loss 0.051
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 97.410, Loss 0.610
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.64
Epoch:24
LR: 0.001
 * Train Acc 97.490, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.65
Epoch:25
LR: 0.001
 * Train Acc 97.450, Loss 18.453
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.000, time 0.64
Epoch:26
LR: 0.001
 * Train Acc 97.420, Loss 0.964
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.67
Epoch:27
LR: 0.001
 * Train Acc 97.880, Loss 0.043
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 97.990, Loss 0.034
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.950, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 97.950, Loss 0.053
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.64
Epoch:30
LR: 0.001
 * Train Acc 98.000, Loss 0.033
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 98.070, Loss 0.032
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.71
Epoch:32
LR: 0.001
 * Train Acc 97.950, Loss 0.042
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.96
Epoch:33
LR: 0.001
 * Train Acc 98.310, Loss 0.027
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.66
Epoch:34
LR: 0.001
 * Train Acc 98.190, Loss 0.029
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.800, time 0.91
Epoch:35
LR: 0.001
 * Train Acc 98.200, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 1.00
Epoch:36
LR: 0.001
 * Train Acc 98.120, Loss 3.164
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.550, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 98.250, Loss 3.861
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.91
Epoch:38
LR: 0.001
 * Train Acc 98.560, Loss 0.328
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.150, time 0.74
Epoch:39
LR: 0.001
 * Train Acc 98.110, Loss 278.635
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.90
Epoch:40
LR: 0.001
 * Train Acc 98.380, Loss 0.543
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 1.04
Epoch:41
LR: 0.001
 * Train Acc 98.200, Loss 0.055
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.000, time 0.90
Epoch:42
LR: 0.001
 * Train Acc 98.470, Loss 0.418
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 1.01
Epoch:43
LR: 0.001
 * Train Acc 98.490, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.93
Epoch:44
LR: 0.001
 * Train Acc 98.430, Loss 0.381
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.91
Epoch:45
LR: 0.001
 * Train Acc 98.920, Loss 0.016
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.89
Epoch:46
LR: 0.001
 * Train Acc 98.510, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.88
Epoch:47
LR: 0.001
 * Train Acc 98.320, Loss 0.073
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 1.03
Epoch:48
LR: 0.001
 * Train Acc 98.730, Loss 0.016
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.88
Epoch:49
LR: 0.001
 * Train Acc 98.670, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 1.05
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.423683166503906 - mean: 0.015536670573055744 - std: 0.0060022627003490925
 * min 0.008711116388440132, max: 0.03137044608592987
sum: 316.4649963378906 - mean: 0.03433864936232567 - std: 0.017181022092700005
 * min 0.011881151236593723, max: 0.0844116359949112
sum: 179.6243896484375 - mean: 0.009745246730744839 - std: 0.003359686816111207
 * min 0.0033421916887164116, max: 0.0232333205640316
sum: 473.4049987792969 - mean: 0.012841932475566864 - std: 0.003394905710592866
 * min 0.004174646455794573, max: 0.029344214126467705
sum: 2406.99072265625 - mean: 0.03264690190553665 - std: 0.009253415279090405
 * min 0.011798740364611149, max: 0.08358198404312134
sum: 5192.89404296875 - mean: 0.0352165661752224 - std: 0.005445084068924189
 * min 0.01176784560084343, max: 0.08964657783508301
sum: 6154.37158203125 - mean: 0.04173700511455536 - std: 0.005981521215289831
 * min 0.01237662136554718, max: 0.10455488413572311
sum: 110.2484130859375 - mean: 0.00013458057946991175 - std: 8.160820925695589e-07
 * min 0.00010481724166311324, max: 0.0001425662194378674
sum: 5.0 - mean: 0.009765625 - std: 0.002559608779847622
 * min 0.0035281109157949686, max: 0.041266683489084244
eps: tensor([0.1398, 0.3090, 0.0877, 0.1156, 0.2938, 0.3169, 0.3756, 0.4307, 0.4308],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.000, time 1.02
 * Lower 1 Val Acc 66.700, time 1.06
 * Upper 1 Val Acc 66.700, time 1.10
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.050, Loss 0.616
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.750, time 0.90
Epoch:1
LR: 0.001
 * Train Acc 78.300, Loss 0.465
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.92
Epoch:2
LR: 0.001
 * Train Acc 79.770, Loss 0.422
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.93
Epoch:3
LR: 0.001
 * Train Acc 81.610, Loss 0.397
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.91
Epoch:4
LR: 0.001
 * Train Acc 81.670, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.90
Epoch:5
LR: 0.001
 * Train Acc 82.110, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.92
Epoch:6
LR: 0.001
 * Train Acc 82.780, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.96
Epoch:7
LR: 0.001
 * Train Acc 82.590, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.93
Epoch:8
LR: 0.001
 * Train Acc 83.070, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.99
Epoch:9
LR: 0.001
 * Train Acc 83.160, Loss 0.333
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.93
Epoch:10
LR: 0.001
 * Train Acc 83.350, Loss 0.328
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.90
Epoch:11
LR: 0.001
 * Train Acc 83.800, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.97
Epoch:12
LR: 0.001
 * Train Acc 83.840, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.90
Epoch:13
LR: 0.001
 * Train Acc 84.300, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.92
Epoch:14
LR: 0.001
 * Train Acc 83.330, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.96
Epoch:15
LR: 0.001
 * Train Acc 83.500, Loss 0.292
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.92
Epoch:16
LR: 0.001
 * Train Acc 83.750, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.91
Epoch:17
LR: 0.001
 * Train Acc 83.830, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.96
Epoch:18
LR: 0.001
 * Train Acc 84.150, Loss 0.277
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.93
Epoch:19
LR: 0.001
 * Train Acc 84.310, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.92
Epoch:20
LR: 0.001
 * Train Acc 84.170, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.94
Epoch:21
LR: 0.001
 * Train Acc 84.320, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.94
Epoch:22
LR: 0.001
 * Train Acc 84.580, Loss 0.257
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.94
Epoch:23
LR: 0.001
 * Train Acc 83.880, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.89
Epoch:24
LR: 0.001
 * Train Acc 84.490, Loss 0.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.90
Epoch:25
LR: 0.001
 * Train Acc 83.990, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.88
Epoch:26
LR: 0.001
 * Train Acc 83.820, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.91
Epoch:27
LR: 0.001
 * Train Acc 83.970, Loss 0.235
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.92
Epoch:28
LR: 0.001
 * Train Acc 83.850, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.92
Epoch:29
LR: 0.001
 * Train Acc 84.940, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.92
Epoch:30
LR: 0.001
 * Train Acc 84.650, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.96
Epoch:31
LR: 0.001
 * Train Acc 83.870, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.91
Epoch:32
LR: 0.001
 * Train Acc 84.260, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.83
Epoch:33
LR: 0.001
 * Train Acc 84.030, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.64
Epoch:34
LR: 0.001
 * Train Acc 83.820, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.79
Epoch:35
LR: 0.001
 * Train Acc 84.200, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 1.04
Epoch:36
LR: 0.001
 * Train Acc 83.960, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 84.140, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 1.01
Epoch:38
LR: 0.001
 * Train Acc 83.690, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.95
Epoch:39
LR: 0.001
 * Train Acc 83.870, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 1.07
Epoch:40
LR: 0.001
 * Train Acc 83.840, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.93
Epoch:41
LR: 0.001
 * Train Acc 84.330, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.90
Epoch:42
LR: 0.001
 * Train Acc 83.440, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 1.05
Epoch:43
LR: 0.001
 * Train Acc 83.680, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.91
Epoch:44
LR: 0.001
 * Train Acc 83.720, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.70
Epoch:45
LR: 0.001
 * Train Acc 83.120, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.65
Epoch:46
LR: 0.001
 * Train Acc 82.810, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.95
Epoch:47
LR: 0.001
 * Train Acc 83.840, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 1.05
Epoch:48
LR: 0.001
 * Train Acc 83.130, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.93
Epoch:49
LR: 0.001
 * Train Acc 82.990, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.91
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.3293657302856445 - mean: 0.005010840017348528 - std: 0.002123357728123665
 * min 0.0026775244623422623, max: 0.010792836546897888
sum: 128.0876922607422 - mean: 0.013898404315114021 - std: 0.0075175086967647076
 * min 0.004384009167551994, max: 0.03697692230343819
sum: 58.959877014160156 - mean: 0.003198778023943305 - std: 0.0011711466358974576
 * min 0.0009895121911540627, max: 0.007970492355525494
sum: 153.290283203125 - mean: 0.004158264957368374 - std: 0.0011356661561876535
 * min 0.001345631666481495, max: 0.010662322863936424
sum: 813.1611938476562 - mean: 0.011029204353690147 - std: 0.0032097611110657454
 * min 0.0039185951463878155, max: 0.030477363616228104
sum: 1874.79833984375 - mean: 0.01271429006010294 - std: 0.0020069004967808723
 * min 0.004044569097459316, max: 0.032878149300813675
sum: 2233.2060546875 - mean: 0.015144897624850273 - std: 0.0022179502993822098
 * min 0.004344635177403688, max: 0.03762519732117653
sum: 40.136295318603516 - mean: 4.899450141238049e-05 - std: 3.01832045579431e-07
 * min 3.8083522667875513e-05, max: 5.198430153541267e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0009448068449273705
 * min 0.0012534616980701685, max: 0.015219521708786488
eps: tensor([0.0451, 0.1251, 0.0288, 0.0374, 0.0993, 0.1144, 0.1363, 0.1568, 0.1568],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 84.450, time 0.92
 * Lower 1 Val Acc 88.500, time 0.91
 * Upper 1 Val Acc 88.500, time 0.93
validation split name: 2
 *  Val Acc 79.550, time 0.91
 * Lower 1 Val Acc 74.250, time 0.88
 * Upper 1 Val Acc 74.250, time 0.91
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.270, Loss 0.460
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.750, time 1.04
Epoch:1
LR: 0.001
 * Train Acc 82.250, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.92
Epoch:2
LR: 0.001
 * Train Acc 82.770, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.86
Epoch:3
LR: 0.001
 * Train Acc 83.310, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.90
Epoch:4
LR: 0.001
 * Train Acc 83.120, Loss 0.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.89
Epoch:5
LR: 0.001
 * Train Acc 83.260, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.91
Epoch:6
LR: 0.001
 * Train Acc 84.000, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.93
Epoch:7
LR: 0.001
 * Train Acc 83.650, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.92
Epoch:8
LR: 0.001
 * Train Acc 83.560, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.90
Epoch:9
LR: 0.001
 * Train Acc 83.860, Loss 0.328
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.95
Epoch:10
LR: 0.001
 * Train Acc 83.740, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.84
Epoch:11
LR: 0.001
 * Train Acc 83.910, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.88
Epoch:12
LR: 0.001
 * Train Acc 83.440, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.90
Epoch:13
LR: 0.001
 * Train Acc 83.790, Loss 0.316
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.91
Epoch:14
LR: 0.001
 * Train Acc 83.500, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.91
Epoch:15
LR: 0.001
 * Train Acc 83.530, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.300, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 83.640, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.76
Epoch:17
LR: 0.001
 * Train Acc 83.810, Loss 0.292
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.93
Epoch:18
LR: 0.001
 * Train Acc 83.790, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.89
Epoch:19
LR: 0.001
 * Train Acc 83.520, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.88
Epoch:20
LR: 0.001
 * Train Acc 84.350, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.89
Epoch:21
LR: 0.001
 * Train Acc 83.800, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.94
Epoch:22
LR: 0.001
 * Train Acc 83.510, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.91
Epoch:23
LR: 0.001
 * Train Acc 83.120, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.91
Epoch:24
LR: 0.001
 * Train Acc 83.000, Loss 0.268
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.87
Epoch:25
LR: 0.001
 * Train Acc 83.810, Loss 0.258
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.73
Epoch:26
LR: 0.001
 * Train Acc 83.510, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.70
Epoch:27
LR: 0.001
 * Train Acc 83.740, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.89
Epoch:28
LR: 0.001
 * Train Acc 83.440, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.90
Epoch:29
LR: 0.001
 * Train Acc 82.630, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.98
Epoch:30
LR: 0.001
 * Train Acc 83.370, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.450, time 0.89
Epoch:31
LR: 0.001
 * Train Acc 83.620, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.91
Epoch:32
LR: 0.001
 * Train Acc 83.220, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 0.83
Epoch:33
LR: 0.001
 * Train Acc 83.080, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.68
Epoch:34
LR: 0.001
 * Train Acc 83.790, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.78
Epoch:35
LR: 0.001
 * Train Acc 83.820, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 83.140, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.90
Epoch:37
LR: 0.001
 * Train Acc 83.080, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.700, time 1.05
Epoch:38
LR: 0.001
 * Train Acc 83.580, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.92
Epoch:39
LR: 0.001
 * Train Acc 82.760, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.79
Epoch:40
LR: 0.001
 * Train Acc 83.200, Loss 0.193
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.79
Epoch:41
LR: 0.001
 * Train Acc 82.990, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.88
Epoch:42
LR: 0.001
 * Train Acc 83.140, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 1.06
Epoch:43
LR: 0.001
 * Train Acc 83.190, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.93
Epoch:44
LR: 0.001
 * Train Acc 82.930, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.600, time 0.89
Epoch:45
LR: 0.001
 * Train Acc 82.500, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 1.03
Epoch:46
LR: 0.001
 * Train Acc 82.350, Loss 0.198
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.300, time 0.92
Epoch:47
LR: 0.001
 * Train Acc 81.860, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.89
Epoch:48
LR: 0.001
 * Train Acc 82.640, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.550, time 0.76
Epoch:49
LR: 0.001
 * Train Acc 82.640, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.88
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.4849262237548828 - mean: 0.0017186646582558751 - std: 0.0009451985824853182
 * min 0.0007806472131051123, max: 0.004430780187249184
sum: 42.00373840332031 - mean: 0.00455769756808877 - std: 0.002970318542793393
 * min 0.0011120546841993928, max: 0.014709553681313992
sum: 16.32448959350586 - mean: 0.000885660236235708 - std: 0.0003654237661976367
 * min 0.00021534267580136657, max: 0.0025970551650971174
sum: 42.37641906738281 - mean: 0.0011495338985696435 - std: 0.0003357220848556608
 * min 0.0003177342005074024, max: 0.0033322812523692846
sum: 227.19027709960938 - mean: 0.003081465372815728 - std: 0.0009522215696051717
 * min 0.0009924425976350904, max: 0.009783980436623096
sum: 593.0463256835938 - mean: 0.004021852742880583 - std: 0.0006716977222822607
 * min 0.001191775780171156, max: 0.011225241236388683
sum: 787.0654296875 - mean: 0.005337629001587629 - std: 0.0008292440325021744
 * min 0.0013581159291788936, max: 0.013930998742580414
sum: 14.49068832397461 - mean: 1.768882793840021e-05 - std: 1.1011357514689735e-07
 * min 1.3722456969844643e-05, max: 1.8781252947519533e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.00035472447052598
 * min 0.0003736959188245237, max: 0.005707553122192621
eps: tensor([0.0155, 0.0410, 0.0080, 0.0103, 0.0277, 0.0362, 0.0480, 0.0566, 0.0566],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.350, time 0.90
 * Lower 1 Val Acc 87.400, time 0.86
 * Upper 1 Val Acc 87.400, time 0.87
validation split name: 2
 *  Val Acc 77.000, time 0.95
 * Lower 1 Val Acc 76.300, time 0.89
 * Upper 1 Val Acc 76.300, time 0.90
validation split name: 3
 *  Val Acc 80.600, time 0.85
 * Lower 1 Val Acc 78.000, time 0.89
 * Upper 1 Val Acc 78.000, time 0.91
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 73.350, Loss 0.547
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.400, time 0.89
Epoch:1
LR: 0.001
 * Train Acc 74.930, Loss 0.507
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.050, time 0.96
Epoch:2
LR: 0.001
 * Train Acc 75.270, Loss 0.492
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.87
Epoch:3
LR: 0.001
 * Train Acc 74.920, Loss 0.492
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.86
Epoch:4
LR: 0.001
 * Train Acc 74.360, Loss 0.489
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.900, time 0.89
Epoch:5
LR: 0.001
 * Train Acc 74.510, Loss 0.481
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.89
Epoch:6
LR: 0.001
 * Train Acc 74.930, Loss 0.473
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.89
Epoch:7
LR: 0.001
 * Train Acc 74.690, Loss 0.470
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.200, time 0.90
Epoch:8
LR: 0.001
 * Train Acc 74.700, Loss 0.466
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.800, time 0.91
Epoch:9
LR: 0.001
 * Train Acc 74.530, Loss 0.461
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.450, time 0.87
Epoch:10
LR: 0.001
 * Train Acc 74.120, Loss 0.455
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.500, time 0.90
Epoch:11
LR: 0.001
 * Train Acc 73.600, Loss 0.453
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.600, time 0.87
Epoch:12
LR: 0.001
 * Train Acc 74.110, Loss 0.446
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.900, time 0.90
Epoch:13
LR: 0.001
 * Train Acc 73.090, Loss 0.443
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.85
Epoch:14
LR: 0.001
 * Train Acc 73.770, Loss 0.440
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.650, time 0.92
Epoch:15
LR: 0.001
 * Train Acc 73.680, Loss 0.429
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.350, time 0.90
Epoch:16
LR: 0.001
 * Train Acc 72.530, Loss 0.432
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.150, time 0.89
Epoch:17
LR: 0.001
 * Train Acc 72.760, Loss 0.425
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.600, time 0.84
Epoch:18
LR: 0.001
 * Train Acc 72.220, Loss 0.419
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.900, time 0.71
Epoch:19
LR: 0.001
 * Train Acc 72.470, Loss 0.416
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.350, time 0.92
Epoch:20
LR: 0.001
 * Train Acc 71.840, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.000, time 0.95
Epoch:21
LR: 0.001
 * Train Acc 72.070, Loss 0.405
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.050, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 71.540, Loss 0.401
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.650, time 0.88
Epoch:23
LR: 0.001
 * Train Acc 71.650, Loss 0.396
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.700, time 0.87
Epoch:24
LR: 0.001
 * Train Acc 70.950, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.750, time 0.95
Epoch:25
LR: 0.001
 * Train Acc 71.250, Loss 0.384
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.000, time 0.86
Epoch:26
LR: 0.001
 * Train Acc 70.610, Loss 0.382
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.950, time 0.91
Epoch:27
LR: 0.001
 * Train Acc 70.710, Loss 0.378
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.300, time 0.91
Epoch:28
LR: 0.001
 * Train Acc 70.640, Loss 0.370
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.600, time 0.94
Epoch:29
LR: 0.001
 * Train Acc 70.060, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.200, time 0.93
Epoch:30
LR: 0.001
 * Train Acc 69.980, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.550, time 0.74
Epoch:31
LR: 0.001
 * Train Acc 68.860, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.600, time 0.67
Epoch:32
LR: 0.001
 * Train Acc 69.480, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.200, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 68.720, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.200, time 0.83
Epoch:34
LR: 0.001
 * Train Acc 67.700, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.250, time 0.64
Epoch:35
LR: 0.001
 * Train Acc 68.210, Loss 0.337
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.400, time 0.90
Epoch:36
LR: 0.001
 * Train Acc 68.250, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.450, time 1.06
Epoch:37
LR: 0.001
 * Train Acc 68.050, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.150, time 0.92
Epoch:38
LR: 0.001
 * Train Acc 67.490, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.200, time 0.77
Epoch:39
LR: 0.001
 * Train Acc 67.900, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.600, time 0.64
Epoch:40
LR: 0.001
 * Train Acc 67.010, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.800, time 0.89
Epoch:41
LR: 0.001
 * Train Acc 66.190, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.850, time 1.02
Epoch:42
LR: 0.001
 * Train Acc 66.310, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.050, time 0.93
Epoch:43
LR: 0.001
 * Train Acc 65.900, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.550, time 0.92
Epoch:44
LR: 0.001
 * Train Acc 65.440, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.000, time 1.03
Epoch:45
LR: 0.001
 * Train Acc 65.980, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.200, time 0.90
Epoch:46
LR: 0.001
 * Train Acc 65.340, Loss 0.328
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.100, time 0.90
Epoch:47
LR: 0.001
 * Train Acc 64.530, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.200, time 1.01
Epoch:48
LR: 0.001
 * Train Acc 64.040, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.100, time 0.93
Epoch:49
LR: 0.001
 * Train Acc 65.260, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.300, time 1.02
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0432566404342651 - mean: 0.0012074729893356562 - std: 0.0007268470944836736
 * min 0.000510691141244024, max: 0.003318561939522624
sum: 27.835697174072266 - mean: 0.003020366420969367 - std: 0.002069152658805251
 * min 0.0006907469360157847, max: 0.010431263595819473
sum: 10.471567153930664 - mean: 0.0005681188777089119 - std: 0.00024031508655752987
 * min 0.00013381941244006157, max: 0.0017355267191305757
sum: 27.130441665649414 - mean: 0.0007359603187069297 - std: 0.00021819402172695845
 * min 0.00019035661534871906, max: 0.0022462238557636738
sum: 145.28973388671875 - mean: 0.001970618264749646 - std: 0.0006179073243401945
 * min 0.0006035895203240216, max: 0.006575527135282755
sum: 380.3221435546875 - mean: 0.0025792245287448168 - std: 0.00043805231689475477
 * min 0.0007536677876487374, max: 0.007370165549218655
sum: 515.7033081054688 - mean: 0.0034973369911313057 - std: 0.0005530751659534872
 * min 0.0008625056361779571, max: 0.00937705673277378
sum: 9.952383041381836 - mean: 1.214890471601393e-05 - std: 7.580300831477871e-08
 * min 9.427506483916659e-06, max: 1.2904545656056143e-05
sum: 0.40000003576278687 - mean: 0.0007812500698491931 - std: 0.00024713671882636845
 * min 0.00023257313296198845, max: 0.004070202354341745
eps: tensor([0.0109, 0.0272, 0.0051, 0.0066, 0.0177, 0.0232, 0.0315, 0.0389, 0.0389],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 80.150, time 1.04
 * Lower 1 Val Acc 82.150, time 1.01
 * Upper 1 Val Acc 82.150, time 1.02
validation split name: 2
 *  Val Acc 73.350, time 0.98
 * Lower 1 Val Acc 72.800, time 1.02
 * Upper 1 Val Acc 72.800, time 0.94
validation split name: 3
 *  Val Acc 75.800, time 0.67
 * Lower 1 Val Acc 72.950, time 0.90
 * Upper 1 Val Acc 72.950, time 0.88
validation split name: 4
 *  Val Acc 68.300, time 0.90
 * Lower 1 Val Acc 65.300, time 0.93
 * Upper 1 Val Acc 65.300, time 0.93
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 81.470, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.88
Epoch:1
LR: 0.001
 * Train Acc 82.440, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.90
Epoch:2
LR: 0.001
 * Train Acc 82.090, Loss 0.397
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.90
Epoch:3
LR: 0.001
 * Train Acc 82.190, Loss 0.387
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.91
Epoch:4
LR: 0.001
 * Train Acc 81.860, Loss 0.380
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.94
Epoch:5
LR: 0.001
 * Train Acc 82.030, Loss 0.378
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.88
Epoch:6
LR: 0.001
 * Train Acc 81.900, Loss 0.375
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.93
Epoch:7
LR: 0.001
 * Train Acc 81.710, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.88
Epoch:8
LR: 0.001
 * Train Acc 82.040, Loss 0.363
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.88
Epoch:9
LR: 0.001
 * Train Acc 81.840, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.83
Epoch:10
LR: 0.001
 * Train Acc 81.510, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.71
Epoch:11
LR: 0.001
 * Train Acc 82.360, Loss 0.346
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.88
Epoch:12
LR: 0.001
 * Train Acc 81.720, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.90
Epoch:13
LR: 0.001
 * Train Acc 82.110, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.93
Epoch:14
LR: 0.001
 * Train Acc 82.170, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.85
Epoch:15
LR: 0.001
 * Train Acc 81.650, Loss 0.333
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.91
Epoch:16
LR: 0.001
 * Train Acc 81.600, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.90
Epoch:17
LR: 0.001
 * Train Acc 81.760, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.90
Epoch:18
LR: 0.001
 * Train Acc 81.980, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.89
Epoch:19
LR: 0.001
 * Train Acc 81.450, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.90
Epoch:20
LR: 0.001
 * Train Acc 81.470, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.93
Epoch:21
LR: 0.001
 * Train Acc 81.540, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.91
Epoch:22
LR: 0.001
 * Train Acc 81.670, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.73
Epoch:23
LR: 0.001
 * Train Acc 81.130, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.68
Epoch:24
LR: 0.001
 * Train Acc 81.450, Loss 0.290
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.91
Epoch:25
LR: 0.001
 * Train Acc 81.420, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.94
Epoch:26
LR: 0.001
 * Train Acc 81.230, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.93
Epoch:27
LR: 0.001
 * Train Acc 81.690, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.91
Epoch:28
LR: 0.001
 * Train Acc 80.970, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.86
Epoch:29
LR: 0.001
 * Train Acc 81.460, Loss 0.266
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.94
Epoch:30
LR: 0.001
 * Train Acc 80.880, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.89
Epoch:31
LR: 0.001
 * Train Acc 80.800, Loss 0.262
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.77
Epoch:32
LR: 0.001
 * Train Acc 80.820, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.78
Epoch:33
LR: 0.001
 * Train Acc 80.520, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.70
Epoch:34
LR: 0.001
 * Train Acc 80.980, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 1.02
Epoch:35
LR: 0.001
 * Train Acc 80.390, Loss 0.239
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.92
Epoch:36
LR: 0.001
 * Train Acc 80.550, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.94
Epoch:37
LR: 0.001
 * Train Acc 81.170, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 1.03
Epoch:38
LR: 0.001
 * Train Acc 80.110, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.90
Epoch:39
LR: 0.001
 * Train Acc 80.260, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.90
Epoch:40
LR: 0.001
 * Train Acc 80.150, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 1.06
Epoch:41
LR: 0.001
 * Train Acc 80.190, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.91
Epoch:42
LR: 0.001
 * Train Acc 80.180, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 1.05
Epoch:43
LR: 0.001
 * Train Acc 80.430, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.93
Epoch:44
LR: 0.001
 * Train Acc 80.100, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.89
Epoch:45
LR: 0.001
 * Train Acc 80.290, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 1.04
Epoch:46
LR: 0.001
 * Train Acc 80.300, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.350, time 0.88
Epoch:47
LR: 0.001
 * Train Acc 80.510, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.89
Epoch:48
LR: 0.001
 * Train Acc 80.030, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.800, time 0.84
Epoch:49
LR: 0.001
 * Train Acc 79.880, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.91
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5184957981109619 - mean: 0.00060011085588485 - std: 0.0003630249120760709
 * min 0.00025574432220309973, max: 0.0016553023597225547
sum: 14.447004318237305 - mean: 0.0015676002949476242 - std: 0.0010994559852406383
 * min 0.0003422952722758055, max: 0.005594773218035698
sum: 5.114569187164307 - mean: 0.0002774831373244524 - std: 0.00011884237756021321
 * min 6.309125456027687e-05, max: 0.0008579972200095654
sum: 13.016630172729492 - mean: 0.0003530987014528364 - std: 0.00010521876538405195
 * min 9.036441042553633e-05, max: 0.0010989730944857001
sum: 70.07501220703125 - mean: 0.0009504531626589596 - std: 0.00029887439450249076
 * min 0.00028935421141795814, max: 0.00320638925768435
sum: 183.8335723876953 - mean: 0.0012467012275010347 - std: 0.0002123099402524531
 * min 0.0003554909199010581, max: 0.003563618054613471
sum: 257.5632019042969 - mean: 0.001746712252497673 - std: 0.00027702300576493144
 * min 0.00042956951074302197, max: 0.004705795086920261
sum: 5.030573844909668 - mean: 6.140837285784073e-06 - std: 3.831656414376994e-08
 * min 4.7652652028773446e-06, max: 6.5227868617512286e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 0.00012335870997048914
 * min 0.00011604261817410588, max: 0.002036649500951171
eps: tensor([0.0054, 0.0141, 0.0025, 0.0032, 0.0086, 0.0112, 0.0157, 0.0197, 0.0197],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 84.700, time 0.91
 * Lower 1 Val Acc 84.800, time 0.92
 * Upper 1 Val Acc 84.800, time 0.89
validation split name: 2
 *  Val Acc 72.050, time 0.90
 * Lower 1 Val Acc 71.700, time 0.87
 * Upper 1 Val Acc 71.700, time 0.86
validation split name: 3
 *  Val Acc 66.900, time 0.90
 * Lower 1 Val Acc 65.700, time 0.89
 * Upper 1 Val Acc 65.700, time 0.92
validation split name: 4
 *  Val Acc 62.950, time 0.94
 * Lower 1 Val Acc 62.000, time 0.92
 * Upper 1 Val Acc 62.000, time 0.91
validation split name: 5
 *  Val Acc 77.650, time 0.93
 * Lower 1 Val Acc 77.950, time 0.92
 * Upper 1 Val Acc 77.950, time 0.80
Task 1 average acc: 97.0
Task 2 average acc: 82.0
Task 3 average acc: 80.98333333333333
Task 4 average acc: 74.4
Task 5 average acc: 72.85
===Summary of experiment repeats: 2 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85  0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 14.950999999999999 std: 29.914133933644145
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.010, Loss 0.571
 * robust loss: 0.372 robust error: 0.03000000
 *  Val Acc 80.150, time 0.87
Epoch:1
LR: 0.001
 * Train Acc 81.880, Loss 0.402
 * robust loss: 0.754 robust error: 0.02000000
 *  Val Acc 84.550, time 0.94
Epoch:2
LR: 0.001
 * Train Acc 86.160, Loss 0.315
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 88.500, time 0.89
Epoch:3
LR: 0.001
 * Train Acc 87.840, Loss 0.280
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 90.750, time 0.92
Epoch:4
LR: 0.001
 * Train Acc 89.560, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.750, time 0.92
Epoch:5
LR: 0.001
 * Train Acc 91.570, Loss 0.212
 * robust loss: 0.190 robust error: 0.01000000
 *  Val Acc 92.250, time 0.92
Epoch:6
LR: 0.001
 * Train Acc 92.500, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.900, time 0.92
Epoch:7
LR: 0.001
 * Train Acc 93.350, Loss 0.149
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.400, time 0.88
Epoch:8
LR: 0.001
 * Train Acc 94.730, Loss 0.137
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.800, time 0.92
Epoch:9
LR: 0.001
 * Train Acc 95.020, Loss 0.174
 * robust loss: 1.137 robust error: 0.01000000
 *  Val Acc 96.000, time 0.87
Epoch:10
LR: 0.001
 * Train Acc 95.460, Loss 0.101
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.600, time 0.95
Epoch:11
LR: 0.001
 * Train Acc 95.990, Loss 0.091
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 95.830, Loss 0.163
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.750, time 0.93
Epoch:13
LR: 0.001
 * Train Acc 95.380, Loss 1.647
 * robust loss: 0.042 robust error: 0.00000000
 *  Val Acc 96.600, time 0.92
Epoch:14
LR: 0.001
 * Train Acc 96.030, Loss 2.418
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 0.87
Epoch:15
LR: 0.001
 * Train Acc 96.260, Loss 0.077
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.90
Epoch:16
LR: 0.001
 * Train Acc 96.750, Loss 0.070
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.250, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 96.870, Loss 0.088
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 97.400, time 0.95
Epoch:18
LR: 0.001
 * Train Acc 96.880, Loss 0.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.93
Epoch:19
LR: 0.001
 * Train Acc 97.280, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.96
Epoch:20
LR: 0.001
 * Train Acc 97.230, Loss 7.877
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 1.01
Epoch:21
LR: 0.001
 * Train Acc 97.320, Loss 0.059
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.71
Epoch:22
LR: 0.001
 * Train Acc 97.750, Loss 0.044
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.95
Epoch:23
LR: 0.001
 * Train Acc 97.610, Loss 0.045
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.95
Epoch:24
LR: 0.001
 * Train Acc 97.770, Loss 0.043
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.90
Epoch:25
LR: 0.001
 * Train Acc 97.870, Loss 5.926
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.93
Epoch:26
LR: 0.001
 * Train Acc 97.670, Loss 0.042
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.86
Epoch:27
LR: 0.001
 * Train Acc 97.920, Loss 0.125
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.98
Epoch:28
LR: 0.001
 * Train Acc 98.040, Loss 0.489
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 1.00
Epoch:29
LR: 0.001
 * Train Acc 98.060, Loss 0.034
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.92
Epoch:30
LR: 0.001
 * Train Acc 98.090, Loss 100.548
 * robust loss: 22025.947 robust error: 0.02000000
 *  Val Acc 98.050, time 0.75
Epoch:31
LR: 0.001
 * Train Acc 98.020, Loss 0.077
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.96
Epoch:32
LR: 0.001
 * Train Acc 98.060, Loss 0.031
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 98.240, Loss 0.720
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.95
Epoch:34
LR: 0.001
 * Train Acc 98.170, Loss 0.027
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.91
Epoch:35
LR: 0.001
 * Train Acc 98.240, Loss 0.150
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.84
Epoch:36
LR: 0.001
 * Train Acc 98.280, Loss 0.608
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.68
Epoch:37
LR: 0.001
 * Train Acc 98.070, Loss 87.594
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.750, time 0.81
Epoch:38
LR: 0.001
 * Train Acc 98.380, Loss 0.040
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.65
Epoch:39
LR: 0.001
 * Train Acc 98.450, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.93
Epoch:40
LR: 0.001
 * Train Acc 98.570, Loss 0.055
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.71
Epoch:41
LR: 0.001
 * Train Acc 98.640, Loss 4.143
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.98
Epoch:42
LR: 0.001
 * Train Acc 98.550, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 1.05
Epoch:43
LR: 0.001
 * Train Acc 98.470, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.250, time 0.94
Epoch:44
LR: 0.001
 * Train Acc 98.600, Loss 0.391
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.300, time 1.07
Epoch:45
LR: 0.001
 * Train Acc 98.690, Loss 0.018
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.92
Epoch:46
LR: 0.001
 * Train Acc 98.440, Loss 0.023
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 1.04
Epoch:47
LR: 0.001
 * Train Acc 98.540, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.250, time 0.95
Epoch:48
LR: 0.001
 * Train Acc 98.760, Loss 0.786
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 1.04
Epoch:49
LR: 0.001
 * Train Acc 98.700, Loss 0.018
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.93
after batch eps: 2.500000000000002, kappa: 0.5
sum: 12.873806953430176 - mean: 0.014900239184498787 - std: 0.0044652423821389675
 * min 0.009684255346655846, max: 0.027033014222979546
sum: 308.7681884765625 - mean: 0.033503491431474686 - std: 0.014214184135198593
 * min 0.01365744136273861, max: 0.07518497109413147
sum: 197.8990020751953 - mean: 0.010736708529293537 - std: 0.0035044641699641943
 * min 0.004451530985534191, max: 0.02104630507528782
sum: 507.9811706542969 - mean: 0.013779871165752411 - std: 0.0034488923847675323
 * min 0.005335155408829451, max: 0.02849222905933857
sum: 2447.85302734375 - mean: 0.03320113196969032 - std: 0.009173015132546425
 * min 0.0119491470977664, max: 0.07576674222946167
sum: 5273.29150390625 - mean: 0.035761795938014984 - std: 0.006832650862634182
 * min 0.01344947051256895, max: 0.08657702058553696
sum: 6160.9189453125 - mean: 0.041781406849622726 - std: 0.007191719487309456
 * min 0.014046123251318932, max: 0.09975190460681915
sum: 108.40817260742188 - mean: 0.00013233418576419353 - std: 8.095768748717092e-07
 * min 0.00011198870197404176, max: 0.00013712942018173635
sum: 5.0 - mean: 0.009765625 - std: 0.005333145149052143
 * min 0.0025673778727650642, max: 0.039204999804496765
eps: tensor([0.1341, 0.3015, 0.0966, 0.1240, 0.2988, 0.3219, 0.3760, 0.4235, 0.4235],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.550, time 0.90
 * Lower 1 Val Acc 50.750, time 0.92
 * Upper 1 Val Acc 50.750, time 0.98
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.270, Loss 0.601
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.100, time 1.04
Epoch:1
LR: 0.001
 * Train Acc 78.830, Loss 0.451
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.90
Epoch:2
LR: 0.001
 * Train Acc 80.400, Loss 0.419
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 80.630, Loss 0.408
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 1.06
Epoch:4
LR: 0.001
 * Train Acc 82.030, Loss 0.385
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.93
Epoch:5
LR: 0.001
 * Train Acc 82.120, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.90
Epoch:6
LR: 0.001
 * Train Acc 82.110, Loss 0.366
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 83.110, Loss 0.346
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.94
Epoch:8
LR: 0.001
 * Train Acc 82.710, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.93
Epoch:9
LR: 0.001
 * Train Acc 83.140, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.99
Epoch:10
LR: 0.001
 * Train Acc 83.730, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.94
Epoch:11
LR: 0.001
 * Train Acc 83.980, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.95
Epoch:12
LR: 0.001
 * Train Acc 83.640, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.94
Epoch:13
LR: 0.001
 * Train Acc 83.670, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.99
Epoch:14
LR: 0.001
 * Train Acc 84.010, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.98
Epoch:15
LR: 0.001
 * Train Acc 84.350, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 83.820, Loss 0.381
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.95
Epoch:17
LR: 0.001
 * Train Acc 83.950, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.96
Epoch:18
LR: 0.001
 * Train Acc 84.610, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.97
Epoch:19
LR: 0.001
 * Train Acc 84.260, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.98
Epoch:20
LR: 0.001
 * Train Acc 83.910, Loss 0.266
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.97
Epoch:21
LR: 0.001
 * Train Acc 84.560, Loss 0.258
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.94
Epoch:22
LR: 0.001
 * Train Acc 84.690, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 1.03
Epoch:23
LR: 0.001
 * Train Acc 84.500, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.92
Epoch:24
LR: 0.001
 * Train Acc 84.300, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.90
Epoch:25
LR: 0.001
 * Train Acc 85.000, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.93
Epoch:26
LR: 0.001
 * Train Acc 84.950, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.96
Epoch:27
LR: 0.001
 * Train Acc 84.440, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.96
Epoch:28
LR: 0.001
 * Train Acc 84.580, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.98
Epoch:29
LR: 0.001
 * Train Acc 84.250, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 1.00
Epoch:30
LR: 0.001
 * Train Acc 84.890, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.96
Epoch:31
LR: 0.001
 * Train Acc 84.960, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.96
Epoch:32
LR: 0.001
 * Train Acc 84.360, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 1.00
Epoch:33
LR: 0.001
 * Train Acc 85.070, Loss 0.203
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.97
Epoch:34
LR: 0.001
 * Train Acc 84.530, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.95
Epoch:35
LR: 0.001
 * Train Acc 83.330, Loss 9.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 0.97
Epoch:36
LR: 0.001
 * Train Acc 84.460, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 1.00
Epoch:37
LR: 0.001
 * Train Acc 84.050, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.94
Epoch:38
LR: 0.001
 * Train Acc 84.190, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.77
Epoch:39
LR: 0.001
 * Train Acc 84.530, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.80
Epoch:40
LR: 0.001
 * Train Acc 84.210, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.71
Epoch:41
LR: 0.001
 * Train Acc 84.310, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 1.05
Epoch:42
LR: 0.001
 * Train Acc 84.460, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.96
Epoch:43
LR: 0.001
 * Train Acc 84.620, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.98
Epoch:44
LR: 0.001
 * Train Acc 83.470, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 1.10
Epoch:45
LR: 0.001
 * Train Acc 83.590, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.93
Epoch:46
LR: 0.001
 * Train Acc 83.620, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.72
Epoch:47
LR: 0.001
 * Train Acc 83.510, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.73
Epoch:48
LR: 0.001
 * Train Acc 83.910, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.94
Epoch:49
LR: 0.001
 * Train Acc 83.380, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 1.08
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.3952765464782715 - mean: 0.005087125580757856 - std: 0.002114528324455023
 * min 0.002772657200694084, max: 0.011241072788834572
sum: 110.7221450805664 - mean: 0.012014121748507023 - std: 0.006323399022221565
 * min 0.004057347774505615, max: 0.03222474455833435
sum: 56.729270935058594 - mean: 0.003077759873121977 - std: 0.0011824031826108694
 * min 0.0010242228163406253, max: 0.007270153146237135
sum: 149.28884887695312 - mean: 0.00404971931129694 - std: 0.0011250850511714816
 * min 0.0013829570962116122, max: 0.010050742886960506
sum: 817.1142578125 - mean: 0.011082821525633335 - std: 0.0033557608257979155
 * min 0.0035254566464573145, max: 0.027330946177244186
sum: 1958.9605712890625 - mean: 0.01328505203127861 - std: 0.0027934384997934103
 * min 0.004507162608206272, max: 0.0340568982064724
sum: 2326.91650390625 - mean: 0.015780413523316383 - std: 0.002973527880385518
 * min 0.00479552336037159, max: 0.041699040681123734
sum: 41.034053802490234 - mean: 5.009039523429237e-05 - std: 3.278487099578342e-07
 * min 4.2113460949622095e-05, max: 5.208222501096316e-05
sum: 1.8000000715255737 - mean: 0.003515625139698386 - std: 0.0020624662283807993
 * min 0.0008778870105743408, max: 0.015234556049108505
eps: tensor([0.0458, 0.1081, 0.0277, 0.0364, 0.0997, 0.1196, 0.1420, 0.1603, 0.1603],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 90.500, time 1.05
 * Lower 1 Val Acc 81.100, time 1.07
 * Upper 1 Val Acc 81.100, time 1.06
validation split name: 2
 *  Val Acc 81.900, time 1.03
 * Lower 1 Val Acc 67.900, time 1.08
 * Upper 1 Val Acc 67.900, time 0.71
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.910, Loss 0.457
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.70
Epoch:1
LR: 0.001
 * Train Acc 82.910, Loss 0.375
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.92
Epoch:2
LR: 0.001
 * Train Acc 83.970, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 1.12
Epoch:3
LR: 0.001
 * Train Acc 85.130, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.92
Epoch:4
LR: 0.001
 * Train Acc 85.020, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.79
Epoch:5
LR: 0.001
 * Train Acc 85.740, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 84.960, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.92
Epoch:7
LR: 0.001
 * Train Acc 85.190, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 1.05
Epoch:8
LR: 0.001
 * Train Acc 85.180, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.150, time 0.97
Epoch:9
LR: 0.001
 * Train Acc 85.570, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 1.03
Epoch:10
LR: 0.001
 * Train Acc 84.690, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.250, time 0.75
Epoch:11
LR: 0.001
 * Train Acc 85.290, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.71
Epoch:12
LR: 0.001
 * Train Acc 85.080, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.91
Epoch:13
LR: 0.001
 * Train Acc 85.260, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 0.78
Epoch:14
LR: 0.001
 * Train Acc 85.300, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 0.92
Epoch:15
LR: 0.001
 * Train Acc 85.750, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.500, time 0.90
Epoch:16
LR: 0.001
 * Train Acc 85.280, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.85
Epoch:17
LR: 0.001
 * Train Acc 85.630, Loss 0.264
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.800, time 1.02
Epoch:18
LR: 0.001
 * Train Acc 85.120, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.90
Epoch:19
LR: 0.001
 * Train Acc 84.790, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 0.76
Epoch:20
LR: 0.001
 * Train Acc 85.480, Loss 0.256
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.850, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 85.170, Loss 0.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.000, time 0.91
Epoch:22
LR: 0.001
 * Train Acc 85.080, Loss 0.248
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 0.87
Epoch:23
LR: 0.001
 * Train Acc 84.970, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.73
Epoch:24
LR: 0.001
 * Train Acc 84.630, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.87
Epoch:25
LR: 0.001
 * Train Acc 85.280, Loss 0.240
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.90
Epoch:26
LR: 0.001
 * Train Acc 84.750, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.71
Epoch:27
LR: 0.001
 * Train Acc 85.290, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.91
Epoch:28
LR: 0.001
 * Train Acc 85.480, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.96
Epoch:29
LR: 0.001
 * Train Acc 84.880, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.70
Epoch:30
LR: 0.001
 * Train Acc 84.460, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.92
Epoch:31
LR: 0.001
 * Train Acc 84.680, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 84.770, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.97
Epoch:33
LR: 0.001
 * Train Acc 84.490, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.88
Epoch:34
LR: 0.001
 * Train Acc 84.700, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.95
Epoch:35
LR: 0.001
 * Train Acc 84.940, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 84.230, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 84.570, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.93
Epoch:38
LR: 0.001
 * Train Acc 84.750, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.81
Epoch:39
LR: 0.001
 * Train Acc 84.790, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 84.460, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.80
Epoch:41
LR: 0.001
 * Train Acc 84.840, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.63
Epoch:42
LR: 0.001
 * Train Acc 84.230, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.89
Epoch:43
LR: 0.001
 * Train Acc 83.870, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 1.04
Epoch:44
LR: 0.001
 * Train Acc 84.160, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.91
Epoch:45
LR: 0.001
 * Train Acc 84.040, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.71
Epoch:46
LR: 0.001
 * Train Acc 83.670, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.70
Epoch:47
LR: 0.001
 * Train Acc 83.960, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.90
Epoch:48
LR: 0.001
 * Train Acc 83.690, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 1.04
Epoch:49
LR: 0.001
 * Train Acc 83.760, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.90
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.467235803604126 - mean: 0.0016981896478682756 - std: 0.000706284714397043
 * min 0.0009253023308701813, max: 0.0037536746822297573
sum: 36.88701629638672 - mean: 0.004002497531473637 - std: 0.0021073492243885994
 * min 0.0013512303121387959, max: 0.010738174431025982
sum: 18.912317276000977 - mean: 0.0010260589187964797 - std: 0.00039428769377991557
 * min 0.00034138737828470767, max: 0.002423908794298768
sum: 49.76433563232422 - mean: 0.00134994403924793 - std: 0.0003750883915927261
 * min 0.0004609737661667168, max: 0.003350495593622327
sum: 272.2807312011719 - mean: 0.0036930437199771404 - std: 0.001118286745622754
 * min 0.0011746048694476485, max: 0.009107640013098717
sum: 652.8890380859375 - mean: 0.004427687264978886 - std: 0.0009310283930972219
 * min 0.001502151950262487, max: 0.011350810527801514
sum: 775.5752563476562 - mean: 0.005259706173092127 - std: 0.0009911083616316319
 * min 0.0015983553603291512, max: 0.013898726552724838
sum: 13.680180549621582 - mean: 1.669943958404474e-05 - std: 1.0930020266641804e-07
 * min 1.4040039786777925e-05, max: 1.73634853126714e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0006874886457808316
 * min 0.00029262888710945845, max: 0.005078184884041548
eps: tensor([0.0153, 0.0360, 0.0092, 0.0121, 0.0332, 0.0398, 0.0473, 0.0534, 0.0534],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.450, time 0.86
 * Lower 1 Val Acc 86.150, time 0.92
 * Upper 1 Val Acc 86.150, time 0.93
validation split name: 2
 *  Val Acc 75.350, time 0.87
 * Lower 1 Val Acc 73.300, time 0.95
 * Upper 1 Val Acc 73.300, time 0.70
validation split name: 3
 *  Val Acc 84.800, time 1.08
 * Lower 1 Val Acc 81.850, time 1.02
 * Upper 1 Val Acc 81.850, time 1.02
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 77.060, Loss 0.492
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.94
Epoch:1
LR: 0.001
 * Train Acc 79.830, Loss 0.442
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 1.02
Epoch:2
LR: 0.001
 * Train Acc 79.610, Loss 0.441
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 79.790, Loss 0.433
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.90
Epoch:4
LR: 0.001
 * Train Acc 79.550, Loss 0.431
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 1.03
Epoch:5
LR: 0.001
 * Train Acc 79.640, Loss 0.426
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.88
Epoch:6
LR: 0.001
 * Train Acc 78.990, Loss 0.421
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 1.02
Epoch:7
LR: 0.001
 * Train Acc 79.560, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 1.00
Epoch:8
LR: 0.001
 * Train Acc 78.740, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.92
Epoch:9
LR: 0.001
 * Train Acc 79.090, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.86
Epoch:10
LR: 0.001
 * Train Acc 78.740, Loss 0.409
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.88
Epoch:11
LR: 0.001
 * Train Acc 79.000, Loss 0.400
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.87
Epoch:12
LR: 0.001
 * Train Acc 79.280, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.92
Epoch:13
LR: 0.001
 * Train Acc 78.000, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.91
Epoch:14
LR: 0.001
 * Train Acc 78.770, Loss 0.384
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.90
Epoch:15
LR: 0.001
 * Train Acc 78.510, Loss 0.385
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.88
Epoch:16
LR: 0.001
 * Train Acc 78.160, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.90
Epoch:17
LR: 0.001
 * Train Acc 77.820, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.89
Epoch:18
LR: 0.001
 * Train Acc 78.190, Loss 0.372
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.90
Epoch:19
LR: 0.001
 * Train Acc 77.380, Loss 0.372
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.91
Epoch:20
LR: 0.001
 * Train Acc 77.660, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.87
Epoch:21
LR: 0.001
 * Train Acc 77.000, Loss 0.363
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.79
Epoch:22
LR: 0.001
 * Train Acc 76.850, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.93
Epoch:23
LR: 0.001
 * Train Acc 76.620, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.91
Epoch:24
LR: 0.001
 * Train Acc 75.810, Loss 0.355
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.92
Epoch:25
LR: 0.001
 * Train Acc 76.010, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.65
Epoch:26
LR: 0.001
 * Train Acc 76.040, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.87
Epoch:27
LR: 0.001
 * Train Acc 75.850, Loss 0.340
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.90
Epoch:28
LR: 0.001
 * Train Acc 75.290, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.92
Epoch:29
LR: 0.001
 * Train Acc 75.260, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.87
Epoch:30
LR: 0.001
 * Train Acc 75.230, Loss 0.328
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.89
Epoch:31
LR: 0.001
 * Train Acc 75.230, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.90
Epoch:32
LR: 0.001
 * Train Acc 73.850, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.87
Epoch:33
LR: 0.001
 * Train Acc 74.400, Loss 0.316
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.88
Epoch:34
LR: 0.001
 * Train Acc 73.610, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.91
Epoch:35
LR: 0.001
 * Train Acc 73.610, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.90
Epoch:36
LR: 0.001
 * Train Acc 73.100, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.92
Epoch:37
LR: 0.001
 * Train Acc 72.980, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.75
Epoch:38
LR: 0.001
 * Train Acc 72.170, Loss 0.292
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.600, time 0.65
Epoch:39
LR: 0.001
 * Train Acc 72.420, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.100, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 71.650, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.800, time 0.78
Epoch:41
LR: 0.001
 * Train Acc 71.610, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.200, time 0.64
Epoch:42
LR: 0.001
 * Train Acc 71.480, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.850, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 70.510, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.500, time 0.64
Epoch:44
LR: 0.001
 * Train Acc 70.240, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.850, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 70.930, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.550, time 0.87
Epoch:46
LR: 0.001
 * Train Acc 70.100, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.450, time 0.96
Epoch:47
LR: 0.001
 * Train Acc 69.410, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.400, time 0.89
Epoch:48
LR: 0.001
 * Train Acc 69.670, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.900, time 0.88
Epoch:49
LR: 0.001
 * Train Acc 68.790, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.950, time 0.93
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.005745768547058 - mean: 0.0011640576412901282 - std: 0.0005433826008811593
 * min 0.0006020309519954026, max: 0.002780632581561804
sum: 24.871639251708984 - mean: 0.0026987455785274506 - std: 0.0015525087947025895
 * min 0.0008111229981295764, max: 0.007861887104809284
sum: 11.686894416809082 - mean: 0.0006340545951388776 - std: 0.0002565704344306141
 * min 0.00019673824135679752, max: 0.001579000847414136
sum: 30.497499465942383 - mean: 0.0008272976265288889 - std: 0.00023402785882353783
 * min 0.0002811688173096627, max: 0.0020993452053517103
sum: 165.73507690429688 - mean: 0.0022479258477687836 - std: 0.0006877300911583006
 * min 0.0006940654129721224, max: 0.005624603480100632
sum: 429.51190185546875 - mean: 0.0029128140304237604 - std: 0.0006155697628855705
 * min 0.0009866990149021149, max: 0.007489972282201052
sum: 525.95458984375 - mean: 0.0035668578930199146 - std: 0.0006749139865860343
 * min 0.0010799490846693516, max: 0.009471872821450233
sum: 9.411479949951172 - mean: 1.1488622476463206e-05 - std: 7.520215916656525e-08
 * min 9.659054740041029e-06, max: 1.194553442473989e-05
sum: 0.40000003576278687 - mean: 0.0007812500698491931 - std: 0.00045967730693519115
 * min 0.00019439563038758934, max: 0.003396382788196206
eps: tensor([0.0105, 0.0243, 0.0057, 0.0074, 0.0202, 0.0262, 0.0321, 0.0368, 0.0368],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.350, time 0.68
 * Lower 1 Val Acc 80.500, time 0.91
 * Upper 1 Val Acc 80.500, time 0.91
validation split name: 2
 *  Val Acc 72.850, time 0.90
 * Lower 1 Val Acc 72.450, time 0.88
 * Upper 1 Val Acc 72.450, time 0.89
validation split name: 3
 *  Val Acc 80.000, time 0.88
 * Lower 1 Val Acc 79.000, time 0.94
 * Upper 1 Val Acc 79.000, time 0.88
validation split name: 4
 *  Val Acc 73.950, time 0.90
 * Lower 1 Val Acc 72.200, time 0.87
 * Upper 1 Val Acc 72.200, time 0.91
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 82.870, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 83.340, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 1.04
Epoch:2
LR: 0.001
 * Train Acc 84.050, Loss 0.357
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.86
Epoch:3
LR: 0.001
 * Train Acc 83.160, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.87
Epoch:4
LR: 0.001
 * Train Acc 83.560, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.99
Epoch:5
LR: 0.001
 * Train Acc 83.460, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.90
Epoch:6
LR: 0.001
 * Train Acc 83.530, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.71
Epoch:7
LR: 0.001
 * Train Acc 83.800, Loss 0.338
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 1.04
Epoch:8
LR: 0.001
 * Train Acc 84.190, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.89
Epoch:9
LR: 0.001
 * Train Acc 83.430, Loss 0.333
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.88
Epoch:10
LR: 0.001
 * Train Acc 84.100, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.84
Epoch:11
LR: 0.001
 * Train Acc 83.580, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.92
Epoch:12
LR: 0.001
 * Train Acc 83.510, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.91
Epoch:13
LR: 0.001
 * Train Acc 83.750, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.89
Epoch:14
LR: 0.001
 * Train Acc 83.110, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.91
Epoch:15
LR: 0.001
 * Train Acc 82.650, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.91
Epoch:16
LR: 0.001
 * Train Acc 83.210, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 83.310, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 83.040, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.90
Epoch:19
LR: 0.001
 * Train Acc 83.050, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.90
Epoch:20
LR: 0.001
 * Train Acc 82.770, Loss 0.289
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.88
Epoch:21
LR: 0.001
 * Train Acc 83.080, Loss 0.280
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.89
Epoch:22
LR: 0.001
 * Train Acc 82.430, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.87
Epoch:23
LR: 0.001
 * Train Acc 82.770, Loss 0.278
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.88
Epoch:24
LR: 0.001
 * Train Acc 82.850, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.91
Epoch:25
LR: 0.001
 * Train Acc 82.760, Loss 0.268
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.89
Epoch:26
LR: 0.001
 * Train Acc 82.110, Loss 0.269
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.83
Epoch:27
LR: 0.001
 * Train Acc 82.510, Loss 0.261
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.63
Epoch:28
LR: 0.001
 * Train Acc 82.350, Loss 0.256
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.91
Epoch:29
LR: 0.001
 * Train Acc 82.170, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.89
Epoch:30
LR: 0.001
 * Train Acc 81.810, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.91
Epoch:31
LR: 0.001
 * Train Acc 82.430, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.90
Epoch:32
LR: 0.001
 * Train Acc 82.230, Loss 0.239
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.90
Epoch:33
LR: 0.001
 * Train Acc 81.720, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.93
Epoch:34
LR: 0.001
 * Train Acc 81.780, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.95
Epoch:35
LR: 0.001
 * Train Acc 82.210, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 81.530, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.91
Epoch:37
LR: 0.001
 * Train Acc 81.440, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.93
Epoch:38
LR: 0.001
 * Train Acc 81.230, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.88
Epoch:39
LR: 0.001
 * Train Acc 81.650, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.91
Epoch:40
LR: 0.001
 * Train Acc 81.640, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.92
Epoch:41
LR: 0.001
 * Train Acc 80.890, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.82
Epoch:42
LR: 0.001
 * Train Acc 80.700, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.77
Epoch:43
LR: 0.001
 * Train Acc 80.930, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.67
Epoch:44
LR: 0.001
 * Train Acc 81.360, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.700, time 0.83
Epoch:45
LR: 0.001
 * Train Acc 80.870, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 80.870, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.91
Epoch:47
LR: 0.001
 * Train Acc 80.750, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 1.04
Epoch:48
LR: 0.001
 * Train Acc 80.520, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.91
Epoch:49
LR: 0.001
 * Train Acc 80.620, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.91
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5298125743865967 - mean: 0.0006132089765742421 - std: 0.0003532861592248082
 * min 0.0002782738592941314, max: 0.0017134957015514374
sum: 12.635878562927246 - mean: 0.001371080637909472 - std: 0.0009587163804098964
 * min 0.0003223509411327541, max: 0.0049934592097997665
sum: 4.626634120941162 - mean: 0.00025101096252910793 - std: 0.00011726056254701689
 * min 5.664984564646147e-05, max: 0.0007237878744490445
sum: 12.246252059936523 - mean: 0.00033220084151253104 - std: 9.997827146435156e-05
 * min 9.69827888184227e-05, max: 0.0009625394595786929
sum: 70.49456787109375 - mean: 0.0009561437764205039 - std: 0.0003022745077032596
 * min 0.00025523160002194345, max: 0.002658868208527565
sum: 202.30674743652344 - mean: 0.0013719804119318724 - std: 0.0002980060817208141
 * min 0.000442577205831185, max: 0.003808080917224288
sum: 264.57220458984375 - mean: 0.0017942450940608978 - std: 0.0003510592505335808
 * min 0.0005108907935209572, max: 0.005059037823230028
sum: 5.092163562774658 - mean: 6.2160197558114305e-06 - std: 4.0838330050974037e-08
 * min 5.22566642757738e-06, max: 6.464426405727863e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 0.00028104527154937387
 * min 7.831627590348944e-05, max: 0.002155165420845151
eps: tensor([0.0055, 0.0123, 0.0023, 0.0030, 0.0086, 0.0123, 0.0161, 0.0199, 0.0199],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.650, time 0.96
 * Lower 1 Val Acc 87.300, time 0.90
 * Upper 1 Val Acc 87.300, time 0.90
validation split name: 2
 *  Val Acc 73.600, time 0.89
 * Lower 1 Val Acc 73.450, time 0.88
 * Upper 1 Val Acc 73.450, time 0.88
validation split name: 3
 *  Val Acc 73.150, time 0.93
 * Lower 1 Val Acc 72.900, time 0.88
 * Upper 1 Val Acc 72.900, time 0.93
validation split name: 4
 *  Val Acc 69.500, time 0.89
 * Lower 1 Val Acc 69.050, time 0.92
 * Upper 1 Val Acc 69.050, time 0.93
validation split name: 5
 *  Val Acc 79.300, time 0.95
 * Lower 1 Val Acc 79.800, time 0.88
 * Upper 1 Val Acc 79.800, time 0.93
Task 1 average acc: 97.55
Task 2 average acc: 86.2
Task 3 average acc: 82.2
Task 4 average acc: 77.0375
Task 5 average acc: 76.64
===Summary of experiment repeats: 3 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64  0.    0.    0.    0.    0.    0.    0.  ]
mean: 22.615 std: 34.55891411777864
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 49.930, Loss 0.615
 * robust loss: 0.010 robust error: 0.01000000
 *  Val Acc 50.000, time 0.90
Epoch:1
LR: 0.001
 * Train Acc 50.180, Loss 0.562
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 50.000, time 1.02
Epoch:2
LR: 0.001
 * Train Acc 50.000, Loss 0.534
 * robust loss: 0.033 robust error: 0.01000000
 *  Val Acc 50.000, time 0.88
Epoch:3
LR: 0.001
 * Train Acc 50.000, Loss 0.510
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 50.000, time 1.01
Epoch:4
LR: 0.001
 * Train Acc 50.000, Loss 0.498
 * robust loss: 0.333 robust error: 0.01000000
 *  Val Acc 50.000, time 0.88
Epoch:5
LR: 0.001
 * Train Acc 50.040, Loss 0.493
 * robust loss: 0.044 robust error: 0.02000000
 *  Val Acc 50.000, time 1.10
Epoch:6
LR: 0.001
 * Train Acc 50.000, Loss 0.496
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 50.000, time 0.92
Epoch:7
LR: 0.001
 * Train Acc 50.000, Loss 0.473
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 50.000, time 1.04
Epoch:8
LR: 0.001
 * Train Acc 50.000, Loss 0.441
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 50.000, time 0.93
Epoch:9
LR: 0.001
 * Train Acc 50.000, Loss 0.448
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 50.000, time 1.04
Epoch:10
LR: 0.001
 * Train Acc 50.000, Loss 0.425
 * robust loss: 0.354 robust error: 0.00000000
 *  Val Acc 50.000, time 0.90
Epoch:11
LR: 0.001
 * Train Acc 50.000, Loss 0.444
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 50.000, time 0.92
Epoch:12
LR: 0.001
 * Train Acc 50.000, Loss 0.403
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 50.000, time 0.93
Epoch:13
LR: 0.001
 * Train Acc 50.000, Loss 0.409
 * robust loss: 0.298 robust error: 0.00000000
 *  Val Acc 50.000, time 0.94
Epoch:14
LR: 0.001
 * Train Acc 50.000, Loss 0.426
 * robust loss: 0.457 robust error: 0.00000000
 *  Val Acc 50.000, time 0.93
Epoch:15
LR: 0.001
 * Train Acc 50.000, Loss 1.134
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 50.000, time 0.88
Epoch:16
LR: 0.001
 * Train Acc 50.000, Loss 0.355
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 50.000, time 0.88
Epoch:17
LR: 0.001
 * Train Acc 50.000, Loss 0.367
 * robust loss: 6.735 robust error: 0.01000000
 *  Val Acc 50.000, time 0.91
Epoch:18
LR: 0.001
 * Train Acc 76.770, Loss 0.262
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.900, time 0.91
Epoch:19
LR: 0.001
 * Train Acc 92.750, Loss 0.779
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.750, time 0.73
Epoch:20
LR: 0.001
 * Train Acc 94.000, Loss 2.386
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.950, time 0.92
Epoch:21
LR: 0.001
 * Train Acc 94.380, Loss 0.400
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 95.200, Loss 0.129
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.650, time 0.91
Epoch:23
LR: 0.001
 * Train Acc 95.670, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.96
Epoch:24
LR: 0.001
 * Train Acc 95.090, Loss 47.625
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.350, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 96.030, Loss 0.617
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.050, time 0.93
Epoch:26
LR: 0.001
 * Train Acc 96.180, Loss 1.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.93
Epoch:27
LR: 0.001
 * Train Acc 96.460, Loss 0.079
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.92
Epoch:28
LR: 0.001
 * Train Acc 96.690, Loss 0.555
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.93
Epoch:29
LR: 0.001
 * Train Acc 96.720, Loss 0.448
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.78
Epoch:30
LR: 0.001
 * Train Acc 97.080, Loss 1.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.94
Epoch:31
LR: 0.001
 * Train Acc 97.210, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.94
Epoch:32
LR: 0.001
 * Train Acc 97.140, Loss 0.046
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 97.370, Loss 0.040
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.750, time 0.94
Epoch:34
LR: 0.001
 * Train Acc 97.300, Loss 3.125
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.93
Epoch:35
LR: 0.001
 * Train Acc 97.510, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 97.310, Loss 0.280
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.300, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 97.560, Loss 13.228
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.98
Epoch:38
LR: 0.001
 * Train Acc 97.600, Loss 7.114
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.90
Epoch:39
LR: 0.001
 * Train Acc 97.500, Loss 1.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.95
Epoch:40
LR: 0.001
 * Train Acc 97.690, Loss 1.448
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.97
Epoch:41
LR: 0.001
 * Train Acc 97.710, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.96
Epoch:42
LR: 0.001
 * Train Acc 97.990, Loss 11.501
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.96
Epoch:43
LR: 0.001
 * Train Acc 97.830, Loss 0.029
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.150, time 0.91
Epoch:44
LR: 0.001
 * Train Acc 97.860, Loss 7.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.94
Epoch:45
LR: 0.001
 * Train Acc 98.060, Loss 1.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.92
Epoch:46
LR: 0.001
 * Train Acc 98.060, Loss 0.026
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.750, time 0.82
Epoch:47
LR: 0.001
 * Train Acc 98.180, Loss 0.548
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.73
Epoch:48
LR: 0.001
 * Train Acc 97.840, Loss 8.162
 * robust loss: 0.076 robust error: 0.00000000
 *  Val Acc 97.900, time 0.74
Epoch:49
LR: 0.001
 * Train Acc 97.970, Loss 142.168
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 1.08
after batch eps: 2.500000000000002, kappa: 0.5
sum: 16.732498168945312 - mean: 0.019366318359971046 - std: 0.004303283989429474
 * min 0.013415148481726646, max: 0.03073832392692566
sum: 302.0553894042969 - mean: 0.03277510777115822 - std: 0.010183281265199184
 * min 0.016331598162651062, max: 0.06434612721204758
sum: 258.400634765625 - mean: 0.014019131660461426 - std: 0.0038037204649299383
 * min 0.0060010794550180435, max: 0.030325401574373245
sum: 696.603759765625 - mean: 0.018896587193012238 - std: 0.004443863406777382
 * min 0.007150373421609402, max: 0.036542750895023346
sum: 2502.843017578125 - mean: 0.03394697979092598 - std: 0.009250219911336899
 * min 0.013165128417313099, max: 0.07461315393447876
sum: 4998.9375 - mean: 0.033901214599609375 - std: 0.007319444324821234
 * min 0.015625236555933952, max: 0.07004830241203308
sum: 5603.921875 - mean: 0.03800402954220772 - std: 0.007123943883925676
 * min 0.015797751024365425, max: 0.0740470290184021
sum: 100.06167602539062 - mean: 0.00012214560410939157 - std: 6.226351842997246e-07
 * min 9.41056277952157e-05, max: 0.00013226304145064205
sum: 5.0 - mean: 0.009765625 - std: 0.000808954646345228
 * min 0.005706613417714834, max: 0.018809039145708084
eps: tensor([0.1743, 0.2950, 0.1262, 0.1701, 0.3055, 0.3051, 0.3420, 0.3909, 0.3909],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.750, time 1.06
 * Lower 1 Val Acc 56.000, time 1.11
 * Upper 1 Val Acc 56.000, time 1.02
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 71.220, Loss 0.585
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 77.900, time 0.94
Epoch:1
LR: 0.001
 * Train Acc 79.380, Loss 0.450
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.93
Epoch:2
LR: 0.001
 * Train Acc 81.200, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 1.05
Epoch:3
LR: 0.001
 * Train Acc 81.340, Loss 0.403
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.92
Epoch:4
LR: 0.001
 * Train Acc 82.010, Loss 0.388
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.93
Epoch:5
LR: 0.001
 * Train Acc 82.790, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 1.06
Epoch:6
LR: 0.001
 * Train Acc 82.900, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 1.00
Epoch:7
LR: 0.001
 * Train Acc 82.950, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.94
Epoch:8
LR: 0.001
 * Train Acc 83.490, Loss 0.382
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 1.07
Epoch:9
LR: 0.001
 * Train Acc 83.720, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.97
Epoch:10
LR: 0.001
 * Train Acc 83.530, Loss 0.357
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.94
Epoch:11
LR: 0.001
 * Train Acc 84.100, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 1.05
Epoch:12
LR: 0.001
 * Train Acc 83.860, Loss 0.459
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.96
Epoch:13
LR: 0.001
 * Train Acc 84.560, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.94
Epoch:14
LR: 0.001
 * Train Acc 84.350, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.95
Epoch:15
LR: 0.001
 * Train Acc 84.170, Loss 0.290
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.94
Epoch:16
LR: 0.001
 * Train Acc 85.260, Loss 0.906
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.95
Epoch:17
LR: 0.001
 * Train Acc 84.570, Loss 0.278
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.97
Epoch:18
LR: 0.001
 * Train Acc 84.360, Loss 1.112
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.94
Epoch:19
LR: 0.001
 * Train Acc 84.540, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.97
Epoch:20
LR: 0.001
 * Train Acc 84.960, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.95
Epoch:21
LR: 0.001
 * Train Acc 85.070, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.95
Epoch:22
LR: 0.001
 * Train Acc 85.340, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.92
Epoch:23
LR: 0.001
 * Train Acc 85.010, Loss 0.244
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.95
Epoch:24
LR: 0.001
 * Train Acc 84.720, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.95
Epoch:25
LR: 0.001
 * Train Acc 84.850, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.95
Epoch:26
LR: 0.001
 * Train Acc 84.390, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.95
Epoch:27
LR: 0.001
 * Train Acc 85.470, Loss 0.226
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.95
Epoch:28
LR: 0.001
 * Train Acc 85.000, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.95
Epoch:29
LR: 0.001
 * Train Acc 84.930, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.93
Epoch:30
LR: 0.001
 * Train Acc 85.380, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.93
Epoch:31
LR: 0.001
 * Train Acc 84.790, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.68
Epoch:32
LR: 0.001
 * Train Acc 84.900, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.95
Epoch:33
LR: 0.001
 * Train Acc 84.990, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.92
Epoch:34
LR: 0.001
 * Train Acc 84.510, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 1.00
Epoch:35
LR: 0.001
 * Train Acc 84.950, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 84.800, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.90
Epoch:37
LR: 0.001
 * Train Acc 84.350, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.94
Epoch:38
LR: 0.001
 * Train Acc 84.610, Loss 0.589
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.99
Epoch:39
LR: 0.001
 * Train Acc 84.180, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.95
Epoch:40
LR: 0.001
 * Train Acc 84.350, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.98
Epoch:41
LR: 0.001
 * Train Acc 84.700, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.97
Epoch:42
LR: 0.001
 * Train Acc 83.820, Loss 3.008
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.88
Epoch:43
LR: 0.001
 * Train Acc 84.850, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.89
Epoch:44
LR: 0.001
 * Train Acc 84.270, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.96
Epoch:45
LR: 0.001
 * Train Acc 84.360, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.81
Epoch:46
LR: 0.001
 * Train Acc 84.680, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.82
Epoch:47
LR: 0.001
 * Train Acc 84.200, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.66
Epoch:48
LR: 0.001
 * Train Acc 84.280, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 1.04
Epoch:49
LR: 0.001
 * Train Acc 83.490, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.93
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 5.727569103240967 - mean: 0.006629130803048611 - std: 0.002460277872160077
 * min 0.0039592767134308815, max: 0.013760486617684364
sum: 122.08732604980469 - mean: 0.013247323222458363 - std: 0.005953003652393818
 * min 0.004889820236712694, max: 0.03272020071744919
sum: 72.39684295654297 - mean: 0.003927780315279961 - std: 0.001330927712842822
 * min 0.0013373667607083917, max: 0.010424385778605938
sum: 193.11618041992188 - mean: 0.005238611716777086 - std: 0.0013860873878002167
 * min 0.001741785672493279, max: 0.011715204454958439
sum: 783.56494140625 - mean: 0.010627780109643936 - std: 0.0032434009481221437
 * min 0.0034215410705655813, max: 0.02824454940855503
sum: 1803.4736328125 - mean: 0.012230588123202324 - std: 0.0029766091611236334
 * min 0.0050073289312422276, max: 0.029745256528258324
sum: 2104.055908203125 - mean: 0.014269042760133743 - std: 0.0029764266218990088
 * min 0.005101373884826899, max: 0.031869396567344666
sum: 38.96599578857422 - mean: 4.7565910790581256e-05 - std: 2.6108278916581185e-07
 * min 3.514955096761696e-05, max: 5.179690197110176e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.00038392533315345645
 * min 0.0018203313229605556, max: 0.007902100682258606
eps: tensor([0.0597, 0.1192, 0.0354, 0.0471, 0.0957, 0.1101, 0.1284, 0.1522, 0.1523],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.300, time 0.75
 * Lower 1 Val Acc 64.900, time 0.69
 * Upper 1 Val Acc 64.900, time 1.05
validation split name: 2
 *  Val Acc 84.000, time 1.03
 * Lower 1 Val Acc 65.050, time 1.09
 * Upper 1 Val Acc 65.050, time 1.03
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.920, Loss 0.459
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.97
Epoch:1
LR: 0.001
 * Train Acc 82.720, Loss 0.384
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 84.600, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 84.330, Loss 0.357
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 1.05
Epoch:3
LR: 0.001
 * Train Acc 84.430, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.93
Epoch:4
LR: 0.001
 * Train Acc 84.580, Loss 0.342
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 1.04
Epoch:5
LR: 0.001
 * Train Acc 84.410, Loss 0.343
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.97
Epoch:6
LR: 0.001
 * Train Acc 85.110, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.450, time 0.94
Epoch:7
LR: 0.001
 * Train Acc 85.180, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 1.08
Epoch:8
LR: 0.001
 * Train Acc 84.370, Loss 0.320
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.94
Epoch:9
LR: 0.001
 * Train Acc 85.200, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.97
Epoch:10
LR: 0.001
 * Train Acc 85.050, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 1.05
Epoch:11
LR: 0.001
 * Train Acc 84.750, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.94
Epoch:12
LR: 0.001
 * Train Acc 85.160, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.94
Epoch:13
LR: 0.001
 * Train Acc 85.120, Loss 0.361
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 84.350, time 1.01
Epoch:14
LR: 0.001
 * Train Acc 84.720, Loss 0.638
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.85
Epoch:15
LR: 0.001
 * Train Acc 85.090, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.93
Epoch:16
LR: 0.001
 * Train Acc 85.320, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.96
Epoch:17
LR: 0.001
 * Train Acc 84.690, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.78
Epoch:18
LR: 0.001
 * Train Acc 85.010, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.94
Epoch:19
LR: 0.001
 * Train Acc 84.710, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.96
Epoch:20
LR: 0.001
 * Train Acc 84.290, Loss 0.887
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.95
Epoch:21
LR: 0.001
 * Train Acc 85.100, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.850, time 0.95
Epoch:22
LR: 0.001
 * Train Acc 84.800, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.94
Epoch:23
LR: 0.001
 * Train Acc 84.570, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.91
Epoch:24
LR: 0.001
 * Train Acc 84.940, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.100, time 0.94
Epoch:25
LR: 0.001
 * Train Acc 84.850, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.93
Epoch:26
LR: 0.001
 * Train Acc 85.070, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.71
Epoch:27
LR: 0.001
 * Train Acc 84.780, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.95
Epoch:28
LR: 0.001
 * Train Acc 84.580, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.95
Epoch:29
LR: 0.001
 * Train Acc 84.550, Loss 0.532
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.95
Epoch:30
LR: 0.001
 * Train Acc 84.470, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.95
Epoch:31
LR: 0.001
 * Train Acc 84.400, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.97
Epoch:32
LR: 0.001
 * Train Acc 84.330, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 84.460, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.91
Epoch:34
LR: 0.001
 * Train Acc 84.420, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.91
Epoch:35
LR: 0.001
 * Train Acc 84.490, Loss 2.486
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.92
Epoch:36
LR: 0.001
 * Train Acc 84.530, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.91
Epoch:37
LR: 0.001
 * Train Acc 84.110, Loss 0.198
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.94
Epoch:38
LR: 0.001
 * Train Acc 84.330, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.95
Epoch:39
LR: 0.001
 * Train Acc 84.160, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.74
Epoch:40
LR: 0.001
 * Train Acc 84.080, Loss 0.184
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.68
Epoch:41
LR: 0.001
 * Train Acc 84.090, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.93
Epoch:42
LR: 0.001
 * Train Acc 83.850, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.95
Epoch:43
LR: 0.001
 * Train Acc 84.010, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.80
Epoch:44
LR: 0.001
 * Train Acc 83.920, Loss 0.533
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 83.810, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.85
Epoch:46
LR: 0.001
 * Train Acc 83.390, Loss 0.193
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.67
Epoch:47
LR: 0.001
 * Train Acc 83.520, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:48
LR: 0.001
 * Train Acc 83.530, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.63
Epoch:49
LR: 0.001
 * Train Acc 83.640, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.64
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 2.0012993812561035 - mean: 0.0023163186851888895 - std: 0.0013562944950535893
 * min 0.001106787589378655, max: 0.006364315282553434
sum: 41.72047424316406 - mean: 0.004526961129158735 - std: 0.0027867730241268873
 * min 0.0011752338614314795, max: 0.016008200123906136
sum: 18.956790924072266 - mean: 0.0010284717427566648 - std: 0.0004353974654804915
 * min 0.000269727548584342, max: 0.00321603799238801
sum: 53.06120681762695 - mean: 0.0014393773162737489 - std: 0.0004269216733518988
 * min 0.00043647916754707694, max: 0.0036027885507792234
sum: 225.29258728027344 - mean: 0.00305572641082108 - std: 0.0009976212168112397
 * min 0.0008943357388488948, max: 0.00882658176124096
sum: 590.92236328125 - mean: 0.00400744890794158 - std: 0.0010261527495458722
 * min 0.0014995125820860267, max: 0.010408422909677029
sum: 718.6502075195312 - mean: 0.0048736585304141045 - std: 0.0010734579991549253
 * min 0.001620735740289092, max: 0.011726499535143375
sum: 13.918805122375488 - mean: 1.6990728909149766e-05 - std: 9.426258174016766e-08
 * min 1.2444209460227285e-05, max: 1.8513666873332113e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.00016036516171880066
 * min 0.0006026345072314143, max: 0.0027175210416316986
eps: tensor([0.0208, 0.0407, 0.0093, 0.0130, 0.0275, 0.0361, 0.0439, 0.0544, 0.0544],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 83.050, time 0.67
 * Lower 1 Val Acc 83.800, time 0.68
 * Upper 1 Val Acc 83.800, time 0.64
validation split name: 2
 *  Val Acc 76.050, time 0.67
 * Lower 1 Val Acc 73.750, time 0.65
 * Upper 1 Val Acc 73.750, time 0.68
validation split name: 3
 *  Val Acc 82.900, time 0.69
 * Lower 1 Val Acc 81.300, time 0.64
 * Upper 1 Val Acc 81.300, time 0.66
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 74.430, Loss 0.535
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 77.030, Loss 0.480
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 76.990, Loss 0.478
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.61
Epoch:3
LR: 0.001
 * Train Acc 76.830, Loss 0.474
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.64
Epoch:4
LR: 0.001
 * Train Acc 76.630, Loss 0.470
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.65
Epoch:5
LR: 0.001
 * Train Acc 76.730, Loss 0.467
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.65
Epoch:6
LR: 0.001
 * Train Acc 77.000, Loss 0.459
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 76.790, Loss 0.451
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.64
Epoch:8
LR: 0.001
 * Train Acc 75.900, Loss 0.452
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.69
Epoch:9
LR: 0.001
 * Train Acc 76.440, Loss 0.446
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 76.680, Loss 0.442
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.71
Epoch:11
LR: 0.001
 * Train Acc 75.860, Loss 0.437
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.74
Epoch:12
LR: 0.001
 * Train Acc 76.320, Loss 0.429
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.67
Epoch:13
LR: 0.001
 * Train Acc 75.330, Loss 0.428
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.64
Epoch:14
LR: 0.001
 * Train Acc 75.760, Loss 0.425
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.68
Epoch:15
LR: 0.001
 * Train Acc 75.940, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.64
Epoch:16
LR: 0.001
 * Train Acc 75.340, Loss 0.413
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.850, time 0.66
Epoch:17
LR: 0.001
 * Train Acc 75.210, Loss 0.408
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 75.000, Loss 0.405
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.800, time 0.63
Epoch:19
LR: 0.001
 * Train Acc 74.770, Loss 0.395
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.500, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 74.990, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.64
Epoch:21
LR: 0.001
 * Train Acc 74.530, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.450, time 0.66
Epoch:22
LR: 0.001
 * Train Acc 74.130, Loss 0.386
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.500, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 74.350, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.850, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 74.090, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.050, time 0.66
Epoch:25
LR: 0.001
 * Train Acc 74.020, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.650, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 73.420, Loss 0.368
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.750, time 0.63
Epoch:27
LR: 0.001
 * Train Acc 73.320, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.400, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 72.600, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 73.020, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 73.050, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.500, time 0.65
Epoch:31
LR: 0.001
 * Train Acc 72.120, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.550, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 71.980, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.450, time 0.66
Epoch:33
LR: 0.001
 * Train Acc 71.920, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.750, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 71.660, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.700, time 0.62
Epoch:35
LR: 0.001
 * Train Acc 71.370, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.850, time 0.66
Epoch:36
LR: 0.001
 * Train Acc 70.830, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.450, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 70.510, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.800, time 0.65
Epoch:38
LR: 0.001
 * Train Acc 70.630, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.100, time 0.66
Epoch:39
LR: 0.001
 * Train Acc 70.610, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.450, time 0.73
Epoch:40
LR: 0.001
 * Train Acc 70.360, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.500, time 0.65
Epoch:41
LR: 0.001
 * Train Acc 69.920, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.750, time 0.63
Epoch:42
LR: 0.001
 * Train Acc 70.370, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.650, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 69.780, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.550, time 0.68
Epoch:44
LR: 0.001
 * Train Acc 69.150, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.050, time 0.67
Epoch:45
LR: 0.001
 * Train Acc 68.880, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.700, time 0.67
Epoch:46
LR: 0.001
 * Train Acc 68.390, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.550, time 0.66
Epoch:47
LR: 0.001
 * Train Acc 68.180, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.450, time 0.73
Epoch:48
LR: 0.001
 * Train Acc 68.240, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.650, time 0.69
Epoch:49
LR: 0.001
 * Train Acc 68.100, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.500, time 0.69
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.348026990890503 - mean: 0.0015602164203301072 - std: 0.0009859015699476004
 * min 0.0006986843072809279, max: 0.004519606940448284
sum: 27.333589553833008 - mean: 0.002965884283185005 - std: 0.0019283788278698921
 * min 0.0007047587423585355, max: 0.01115312147885561
sum: 11.802385330200195 - mean: 0.0006403204170055687 - std: 0.00027956903795711696
 * min 0.00015670739230699837, max: 0.0020302576012909412
sum: 33.0606689453125 - mean: 0.0008968280744738877 - std: 0.00026954972418025136
 * min 0.0002543131122365594, max: 0.002364890882745385
sum: 141.77845764160156 - mean: 0.0019229934550821781 - std: 0.0006342223496176302
 * min 0.0005413455073721707, max: 0.005699719302356243
sum: 378.1634521484375 - mean: 0.002564585069194436 - std: 0.0006628251285292208
 * min 0.0009260696242563426, max: 0.006794402375817299
sum: 481.894287109375 - mean: 0.0032680549193173647 - std: 0.0007280742865987122
 * min 0.0010583630064502358, max: 0.008018472231924534
sum: 9.67750072479248 - mean: 1.1813354831247125e-05 - std: 6.584886591554096e-08
 * min 8.605766197433695e-06, max: 1.2876151231466793e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 0.00010915875463979319
 * min 0.00038809143006801605, max: 0.0019088194239884615
eps: tensor([0.0140, 0.0267, 0.0058, 0.0081, 0.0173, 0.0231, 0.0294, 0.0378, 0.0378],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 80.200, time 0.68
 * Lower 1 Val Acc 80.100, time 0.66
 * Upper 1 Val Acc 80.100, time 0.63
validation split name: 2
 *  Val Acc 72.950, time 0.69
 * Lower 1 Val Acc 70.900, time 0.70
 * Upper 1 Val Acc 70.900, time 0.69
validation split name: 3
 *  Val Acc 80.400, time 0.71
 * Lower 1 Val Acc 77.350, time 0.67
 * Upper 1 Val Acc 77.350, time 0.66
validation split name: 4
 *  Val Acc 71.500, time 0.68
 * Lower 1 Val Acc 70.500, time 0.66
 * Upper 1 Val Acc 70.500, time 0.64
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 80.200, Loss 0.432
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.65
Epoch:1
LR: 0.001
 * Train Acc 80.680, Loss 0.416
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.71
Epoch:2
LR: 0.001
 * Train Acc 80.630, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 80.960, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 81.220, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.62
Epoch:5
LR: 0.001
 * Train Acc 80.820, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 81.030, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.66
Epoch:7
LR: 0.001
 * Train Acc 81.140, Loss 0.380
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.64
Epoch:8
LR: 0.001
 * Train Acc 81.070, Loss 0.375
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.64
Epoch:9
LR: 0.001
 * Train Acc 80.870, Loss 0.372
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 80.930, Loss 0.372
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.65
Epoch:11
LR: 0.001
 * Train Acc 80.050, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.70
Epoch:12
LR: 0.001
 * Train Acc 80.000, Loss 0.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.64
Epoch:13
LR: 0.001
 * Train Acc 80.690, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.67
Epoch:14
LR: 0.001
 * Train Acc 80.190, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.66
Epoch:15
LR: 0.001
 * Train Acc 80.510, Loss 0.346
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.64
Epoch:16
LR: 0.001
 * Train Acc 80.310, Loss 0.346
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.71
Epoch:17
LR: 0.001
 * Train Acc 79.750, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 80.230, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.71
Epoch:19
LR: 0.001
 * Train Acc 79.830, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.67
Epoch:20
LR: 0.001
 * Train Acc 79.640, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 80.280, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 79.870, Loss 0.316
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.63
Epoch:23
LR: 0.001
 * Train Acc 80.620, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.66
Epoch:24
LR: 0.001
 * Train Acc 79.760, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.68
Epoch:25
LR: 0.001
 * Train Acc 79.890, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.64
Epoch:26
LR: 0.001
 * Train Acc 79.570, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 79.340, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 80.290, Loss 0.280
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 79.550, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.63
Epoch:30
LR: 0.001
 * Train Acc 79.470, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.63
Epoch:31
LR: 0.001
 * Train Acc 79.020, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.64
Epoch:32
LR: 0.001
 * Train Acc 79.450, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.70
Epoch:33
LR: 0.001
 * Train Acc 78.880, Loss 0.262
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.66
Epoch:34
LR: 0.001
 * Train Acc 78.920, Loss 0.257
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.65
Epoch:35
LR: 0.001
 * Train Acc 79.410, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 79.260, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.71
Epoch:37
LR: 0.001
 * Train Acc 79.100, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 79.160, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.70
Epoch:39
LR: 0.001
 * Train Acc 78.570, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.100, time 0.70
Epoch:40
LR: 0.001
 * Train Acc 78.400, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.70
Epoch:41
LR: 0.001
 * Train Acc 78.640, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 78.340, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.72
Epoch:43
LR: 0.001
 * Train Acc 78.490, Loss 0.233
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 78.190, Loss 0.233
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.71
Epoch:45
LR: 0.001
 * Train Acc 78.180, Loss 0.235
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.950, time 0.68
Epoch:46
LR: 0.001
 * Train Acc 78.480, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.71
Epoch:47
LR: 0.001
 * Train Acc 78.200, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.67
Epoch:48
LR: 0.001
 * Train Acc 77.910, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.70
Epoch:49
LR: 0.001
 * Train Acc 78.010, Loss 0.235
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.050, time 0.71
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.6338503360748291 - mean: 0.0007336231064982712 - std: 0.0004858962493017316
 * min 0.00030162170878611505, max: 0.002200608141720295
sum: 13.441049575805664 - mean: 0.0014584471937268972 - std: 0.0009975314605981112
 * min 0.0002972026413772255, max: 0.005849684588611126
sum: 5.244643688201904 - mean: 0.0002845401468221098 - std: 0.00012790744949597865
 * min 6.621581269428134e-05, max: 0.0009325304417870939
sum: 14.680580139160156 - mean: 0.0003982362395618111 - std: 0.00012114229321014136
 * min 0.00011184271716047078, max: 0.0010611789766699076
sum: 63.463966369628906 - mean: 0.0008607851341366768 - std: 0.00028626571292988956
 * min 0.00023956924269441515, max: 0.002553326543420553
sum: 191.05345153808594 - mean: 0.0012956641148775816 - std: 0.00033698766492307186
 * min 0.0004657264507841319, max: 0.0035830107517540455
sum: 249.446044921875 - mean: 0.0016916642198339105 - std: 0.0003791679919231683
 * min 0.0005445871502161026, max: 0.004230935592204332
sum: 5.053473472595215 - mean: 6.168790605443064e-06 - std: 3.443335572228534e-08
 * min 4.489425464271335e-06, max: 6.724394097545883e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 5.682820483343676e-05
 * min 0.00018010874919127673, max: 0.00105514214374125
eps: tensor([0.0066, 0.0131, 0.0026, 0.0036, 0.0077, 0.0117, 0.0152, 0.0197, 0.0198],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.850, time 0.70
 * Lower 1 Val Acc 87.600, time 0.68
 * Upper 1 Val Acc 87.600, time 0.70
validation split name: 2
 *  Val Acc 75.150, time 0.69
 * Lower 1 Val Acc 74.250, time 0.68
 * Upper 1 Val Acc 74.250, time 0.68
validation split name: 3
 *  Val Acc 74.350, time 0.65
 * Lower 1 Val Acc 72.950, time 0.72
 * Upper 1 Val Acc 72.950, time 0.73
validation split name: 4
 *  Val Acc 66.800, time 0.68
 * Lower 1 Val Acc 66.300, time 0.67
 * Upper 1 Val Acc 66.300, time 0.69
validation split name: 5
 *  Val Acc 79.050, time 0.70
 * Lower 1 Val Acc 78.950, time 0.69
 * Upper 1 Val Acc 78.950, time 0.71
Task 1 average acc: 97.75
Task 2 average acc: 85.65
Task 3 average acc: 80.66666666666667
Task 4 average acc: 76.2625
Task 5 average acc: 76.64
===Summary of experiment repeats: 4 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64  0.    0.    0.    0.    0.    0.  ]
mean: 30.278999999999996 std: 37.09862381544631
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 50.440, Loss 0.637
 * robust loss: 0.089 robust error: 0.01000000
 *  Val Acc 50.000, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 50.000, Loss 0.561
 * robust loss: 0.026 robust error: 0.02000000
 *  Val Acc 50.900, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 79.470, Loss 0.442
 * robust loss: 0.105 robust error: 0.02000000
 *  Val Acc 83.300, time 0.68
Epoch:3
LR: 0.001
 * Train Acc 83.880, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.70
Epoch:4
LR: 0.001
 * Train Acc 85.090, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.69
Epoch:5
LR: 0.001
 * Train Acc 87.170, Loss 0.285
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 86.750, time 0.67
Epoch:6
LR: 0.001
 * Train Acc 88.270, Loss 0.264
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.500, time 0.72
Epoch:7
LR: 0.001
 * Train Acc 90.140, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 0.69
Epoch:8
LR: 0.001
 * Train Acc 90.390, Loss 0.216
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.100, time 0.71
Epoch:9
LR: 0.001
 * Train Acc 91.690, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.600, time 0.75
Epoch:10
LR: 0.001
 * Train Acc 92.940, Loss 0.161
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.350, time 0.72
Epoch:11
LR: 0.001
 * Train Acc 93.760, Loss 0.156
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.950, time 0.73
Epoch:12
LR: 0.001
 * Train Acc 93.820, Loss 0.129
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.900, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 95.290, Loss 0.102
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.050, time 0.70
Epoch:14
LR: 0.001
 * Train Acc 95.340, Loss 0.393
 * robust loss: 73.040 robust error: 0.02000000
 *  Val Acc 96.200, time 0.73
Epoch:15
LR: 0.001
 * Train Acc 95.340, Loss 0.135
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.350, time 0.68
Epoch:16
LR: 0.001
 * Train Acc 96.300, Loss 0.184
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.800, time 0.71
Epoch:17
LR: 0.001
 * Train Acc 96.280, Loss 0.117
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.450, time 0.72
Epoch:18
LR: 0.001
 * Train Acc 96.430, Loss 0.080
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.500, time 0.71
Epoch:19
LR: 0.001
 * Train Acc 96.510, Loss 0.067
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 96.840, Loss 0.577
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.850, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 96.470, Loss 0.091
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 95.600, time 0.67
Epoch:22
LR: 0.001
 * Train Acc 96.670, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.66
Epoch:23
LR: 0.001
 * Train Acc 97.250, Loss 0.053
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.050, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 97.340, Loss 1.212
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.550, time 0.70
Epoch:25
LR: 0.001
 * Train Acc 96.910, Loss 0.054
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 97.550, Loss 0.118
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.500, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 97.420, Loss 0.055
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.69
Epoch:28
LR: 0.001
 * Train Acc 97.570, Loss 0.299
 * robust loss: 0.076 robust error: 0.00000000
 *  Val Acc 97.200, time 0.79
Epoch:29
LR: 0.001
 * Train Acc 97.280, Loss 0.847
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.69
Epoch:30
LR: 0.001
 * Train Acc 97.650, Loss 0.039
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.700, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 97.700, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 97.970, Loss 0.036
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.750, time 0.74
Epoch:33
LR: 0.001
 * Train Acc 97.800, Loss 1.709
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 97.890, Loss 0.032
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 98.160, Loss 0.163
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.70
Epoch:36
LR: 0.001
 * Train Acc 98.220, Loss 0.028
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.250, time 0.70
Epoch:37
LR: 0.001
 * Train Acc 98.200, Loss 0.026
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.72
Epoch:38
LR: 0.001
 * Train Acc 98.250, Loss 0.044
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.600, time 0.76
Epoch:39
LR: 0.001
 * Train Acc 98.170, Loss 0.026
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.750, time 0.71
Epoch:40
LR: 0.001
 * Train Acc 98.050, Loss 68.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.71
Epoch:41
LR: 0.001
 * Train Acc 98.410, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 98.240, Loss 0.026
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.400, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 98.290, Loss 1.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.800, time 0.69
Epoch:44
LR: 0.001
 * Train Acc 98.390, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.200, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 98.420, Loss 0.485
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.69
Epoch:46
LR: 0.001
 * Train Acc 98.510, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 98.450, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.72
Epoch:48
LR: 0.001
 * Train Acc 98.390, Loss 0.483
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.350, time 0.74
Epoch:49
LR: 0.001
 * Train Acc 98.360, Loss 1.810
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.67
after batch eps: 2.500000000000002, kappa: 0.5
sum: 12.958002090454102 - mean: 0.014997688122093678 - std: 0.005403900519013405
 * min 0.009049071930348873, max: 0.03084014356136322
sum: 277.27392578125 - mean: 0.030086146667599678 - std: 0.014744464308023453
 * min 0.01059567742049694, max: 0.08163245022296906
sum: 172.904052734375 - mean: 0.009380645118653774 - std: 0.003305991180241108
 * min 0.0031192367896437645, max: 0.021340711042284966
sum: 427.42559814453125 - mean: 0.011594661511480808 - std: 0.003073181724175811
 * min 0.004116829484701157, max: 0.025700382888317108
sum: 2197.077392578125 - mean: 0.029799768701195717 - std: 0.008826538920402527
 * min 0.009485411457717419, max: 0.0812971293926239
sum: 4894.91357421875 - mean: 0.03319575637578964 - std: 0.005861666053533554
 * min 0.010823148302733898, max: 0.09448622912168503
sum: 6559.0517578125 - mean: 0.044481415301561356 - std: 0.006066307425498962
 * min 0.012659718282520771, max: 0.12007710337638855
sum: 120.07994079589844 - mean: 0.0001465819514123723 - std: 9.937680260918569e-07
 * min 0.00010673858923837543, max: 0.00015946502389851958
sum: 5.0 - mean: 0.009765625 - std: 0.0019300173735246062
 * min 0.0030888135079294443, max: 0.04664686322212219
eps: tensor([0.1350, 0.2708, 0.0844, 0.1044, 0.2682, 0.2988, 0.4003, 0.4691, 0.4691],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.000, time 0.70
 * Lower 1 Val Acc 69.250, time 0.69
 * Upper 1 Val Acc 69.250, time 0.67
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.450, Loss 0.578
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.100, time 0.72
Epoch:1
LR: 0.001
 * Train Acc 79.250, Loss 0.440
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.72
Epoch:2
LR: 0.001
 * Train Acc 80.770, Loss 0.424
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.72
Epoch:3
LR: 0.001
 * Train Acc 81.580, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.76
Epoch:4
LR: 0.001
 * Train Acc 82.580, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.71
Epoch:5
LR: 0.001
 * Train Acc 82.940, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 82.710, Loss 0.359
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.68
Epoch:7
LR: 0.001
 * Train Acc 82.450, Loss 0.359
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.72
Epoch:8
LR: 0.001
 * Train Acc 83.260, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.74
Epoch:9
LR: 0.001
 * Train Acc 83.160, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 82.720, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.73
Epoch:11
LR: 0.001
 * Train Acc 83.580, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.70
Epoch:12
LR: 0.001
 * Train Acc 83.490, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.76
Epoch:13
LR: 0.001
 * Train Acc 83.400, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.69
Epoch:14
LR: 0.001
 * Train Acc 83.640, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 83.470, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.70
Epoch:16
LR: 0.001
 * Train Acc 84.000, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.68
Epoch:17
LR: 0.001
 * Train Acc 83.320, Loss 0.302
 * robust loss: 4.514 robust error: 0.01000000
 *  Val Acc 81.450, time 0.71
Epoch:18
LR: 0.001
 * Train Acc 82.960, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.67
Epoch:19
LR: 0.001
 * Train Acc 83.550, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.71
Epoch:20
LR: 0.001
 * Train Acc 83.140, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 83.580, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.69
Epoch:22
LR: 0.001
 * Train Acc 83.980, Loss 0.264
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 83.350, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.69
Epoch:24
LR: 0.001
 * Train Acc 83.760, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.71
Epoch:25
LR: 0.001
 * Train Acc 83.590, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:26
LR: 0.001
 * Train Acc 83.090, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 84.010, Loss 0.240
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.66
Epoch:28
LR: 0.001
 * Train Acc 83.660, Loss 0.240
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 83.650, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 83.750, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.74
Epoch:31
LR: 0.001
 * Train Acc 84.000, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 83.720, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.73
Epoch:33
LR: 0.001
 * Train Acc 83.070, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.76
Epoch:34
LR: 0.001
 * Train Acc 82.900, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.70
Epoch:35
LR: 0.001
 * Train Acc 83.400, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 83.560, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.67
Epoch:37
LR: 0.001
 * Train Acc 83.210, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.75
Epoch:38
LR: 0.001
 * Train Acc 82.990, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.71
Epoch:39
LR: 0.001
 * Train Acc 83.030, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.73
Epoch:40
LR: 0.001
 * Train Acc 82.980, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.74
Epoch:41
LR: 0.001
 * Train Acc 83.030, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.75
Epoch:42
LR: 0.001
 * Train Acc 82.790, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.72
Epoch:43
LR: 0.001
 * Train Acc 83.070, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.72
Epoch:44
LR: 0.001
 * Train Acc 83.160, Loss 0.193
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.69
Epoch:45
LR: 0.001
 * Train Acc 82.790, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.74
Epoch:46
LR: 0.001
 * Train Acc 83.200, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.73
Epoch:47
LR: 0.001
 * Train Acc 82.790, Loss 0.195
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 81.950, time 0.74
Epoch:48
LR: 0.001
 * Train Acc 83.390, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.70
Epoch:49
LR: 0.001
 * Train Acc 82.000, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.72
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.667542457580566 - mean: 0.005402248352766037 - std: 0.0023952824994921684
 * min 0.0028803469613194466, max: 0.012403956614434719
sum: 105.99525451660156 - mean: 0.011501220986247063 - std: 0.006331210024654865
 * min 0.003539864206686616, max: 0.03391436114907265
sum: 56.33251190185547 - mean: 0.003056234447285533 - std: 0.0011767051182687283
 * min 0.0008307409007102251, max: 0.007730755489319563
sum: 139.2598876953125 - mean: 0.003777666250243783 - std: 0.00105388427618891
 * min 0.0012360240798443556, max: 0.00897468812763691
sum: 735.1094970703125 - mean: 0.009970560669898987 - std: 0.0030652929563075304
 * min 0.002965518506243825, max: 0.02744966186583042
sum: 1730.7462158203125 - mean: 0.011737373657524586 - std: 0.0021283344831317663
 * min 0.0036782368551939726, max: 0.03381417319178581
sum: 2374.3447265625 - mean: 0.016102055087685585 - std: 0.002246903721243143
 * min 0.004407168366014957, max: 0.04424368962645531
sum: 44.29383087158203 - mean: 5.406961645348929e-05 - std: 3.7838037769688526e-07
 * min 3.901927630067803e-05, max: 5.9004600188927725e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0007250033668242395
 * min 0.0010701542487367988, max: 0.017567602917551994
eps: tensor([0.0486, 0.1035, 0.0275, 0.0340, 0.0897, 0.1056, 0.1449, 0.1730, 0.1731],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 89.250, time 0.72
 * Lower 1 Val Acc 82.400, time 0.72
 * Upper 1 Val Acc 82.400, time 0.70
validation split name: 2
 *  Val Acc 81.400, time 0.65
 * Lower 1 Val Acc 69.950, time 0.74
 * Upper 1 Val Acc 69.950, time 0.67
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.560, Loss 0.449
 * robust loss: 0.002 robust error: 0.00000000
 *  Val Acc 81.350, time 0.67
Epoch:1
LR: 0.001
 * Train Acc 81.480, Loss 0.404
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.300, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 82.720, Loss 0.380
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.70
Epoch:3
LR: 0.001
 * Train Acc 82.640, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 82.650, Loss 0.375
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.70
Epoch:5
LR: 0.001
 * Train Acc 82.570, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.68
Epoch:6
LR: 0.001
 * Train Acc 82.790, Loss 0.359
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.71
Epoch:7
LR: 0.001
 * Train Acc 83.140, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 83.500, Loss 0.340
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.75
Epoch:9
LR: 0.001
 * Train Acc 83.230, Loss 0.340
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.70
Epoch:10
LR: 0.001
 * Train Acc 82.990, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.73
Epoch:11
LR: 0.001
 * Train Acc 83.290, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.71
Epoch:12
LR: 0.001
 * Train Acc 82.660, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.68
Epoch:13
LR: 0.001
 * Train Acc 83.290, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.76
Epoch:14
LR: 0.001
 * Train Acc 83.170, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.74
Epoch:15
LR: 0.001
 * Train Acc 82.770, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.74
Epoch:16
LR: 0.001
 * Train Acc 83.350, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.76
Epoch:17
LR: 0.001
 * Train Acc 83.330, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.70
Epoch:18
LR: 0.001
 * Train Acc 83.410, Loss 0.294
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.72
Epoch:19
LR: 0.001
 * Train Acc 83.270, Loss 0.290
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 83.130, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 0.71
Epoch:21
LR: 0.001
 * Train Acc 82.820, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.74
Epoch:22
LR: 0.001
 * Train Acc 83.120, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.74
Epoch:23
LR: 0.001
 * Train Acc 82.610, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.70
Epoch:24
LR: 0.001
 * Train Acc 82.770, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.71
Epoch:25
LR: 0.001
 * Train Acc 82.980, Loss 0.264
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.350, time 0.76
Epoch:26
LR: 0.001
 * Train Acc 82.770, Loss 0.264
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.69
Epoch:27
LR: 0.001
 * Train Acc 82.680, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.68
Epoch:28
LR: 0.001
 * Train Acc 82.450, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.73
Epoch:29
LR: 0.001
 * Train Acc 82.860, Loss 0.248
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.72
Epoch:30
LR: 0.001
 * Train Acc 82.490, Loss 0.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.72
Epoch:31
LR: 0.001
 * Train Acc 82.630, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 82.750, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.74
Epoch:33
LR: 0.001
 * Train Acc 82.540, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.69
Epoch:34
LR: 0.001
 * Train Acc 83.020, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.74
Epoch:35
LR: 0.001
 * Train Acc 82.130, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.69
Epoch:36
LR: 0.001
 * Train Acc 82.550, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.74
Epoch:37
LR: 0.001
 * Train Acc 82.540, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 82.230, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.69
Epoch:39
LR: 0.001
 * Train Acc 82.290, Loss 0.202
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.65
Epoch:40
LR: 0.001
 * Train Acc 82.430, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.73
Epoch:41
LR: 0.001
 * Train Acc 81.880, Loss 0.203
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.68
Epoch:42
LR: 0.001
 * Train Acc 82.240, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.84
Epoch:43
LR: 0.001
 * Train Acc 82.570, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.72
Epoch:44
LR: 0.001
 * Train Acc 82.450, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.73
Epoch:45
LR: 0.001
 * Train Acc 82.370, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.72
Epoch:46
LR: 0.001
 * Train Acc 82.430, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 81.800, Loss 0.202
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.76
Epoch:48
LR: 0.001
 * Train Acc 81.870, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.65
Epoch:49
LR: 0.001
 * Train Acc 81.940, Loss 0.202
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.73
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.5960144996643066 - mean: 0.0018472389783710241 - std: 0.0010914087761193514
 * min 0.0007895388989709318, max: 0.005055123940110207
sum: 32.44147872924805 - mean: 0.0035201257560402155 - std: 0.0023128821048885584
 * min 0.0008208656217902899, max: 0.012505315244197845
sum: 14.67487621307373 - mean: 0.0007961629889905453 - std: 0.0003414310922380537
 * min 0.00016622772091068327, max: 0.002375131705775857
sum: 36.36959457397461 - mean: 0.0009865884203463793 - std: 0.00029506112332455814
 * min 0.00025853049010038376, max: 0.002760097151622176
sum: 204.7599639892578 - mean: 0.002777234883978963 - std: 0.0009071832173503935
 * min 0.0006389507325366139, max: 0.008784042671322823
sum: 581.8750610351562 - mean: 0.003946092911064625 - std: 0.0007493937155231833
 * min 0.0010752767557278275, max: 0.011923862621188164
sum: 840.973876953125 - mean: 0.005703219212591648 - std: 0.0008461833349429071
 * min 0.0013577960198745131, max: 0.01676681824028492
sum: 15.847221374511719 - mean: 1.9344752217875794e-05 - std: 1.386535757319507e-07
 * min 1.3879644939152058e-05, max: 2.116549148922786e-05
sum: 0.6000000834465027 - mean: 0.0011718751629814506 - std: 0.00028801109874621034
 * min 0.0003193453885614872, max: 0.006982947699725628
eps: tensor([0.0166, 0.0317, 0.0072, 0.0089, 0.0250, 0.0355, 0.0513, 0.0619, 0.0619],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.000, time 0.75
 * Lower 1 Val Acc 83.100, time 0.75
 * Upper 1 Val Acc 83.100, time 0.70
validation split name: 2
 *  Val Acc 77.850, time 0.73
 * Lower 1 Val Acc 75.300, time 0.68
 * Upper 1 Val Acc 75.300, time 0.64
validation split name: 3
 *  Val Acc 81.850, time 0.69
 * Lower 1 Val Acc 79.650, time 0.69
 * Upper 1 Val Acc 79.650, time 0.68
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.120, Loss 0.616
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.850, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 69.400, Loss 0.581
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.400, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 70.010, Loss 0.571
 * robust loss: 0.005 robust error: 0.00000000
 *  Val Acc 75.200, time 0.70
Epoch:3
LR: 0.001
 * Train Acc 69.880, Loss 0.564
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.500, time 0.72
Epoch:4
LR: 0.001
 * Train Acc 70.840, Loss 0.554
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.700, time 0.78
Epoch:5
LR: 0.001
 * Train Acc 69.700, Loss 0.553
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.600, time 0.70
Epoch:6
LR: 0.001
 * Train Acc 69.500, Loss 0.544
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.450, time 0.73
Epoch:7
LR: 0.001
 * Train Acc 69.380, Loss 0.541
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.950, time 0.73
Epoch:8
LR: 0.001
 * Train Acc 69.770, Loss 0.531
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.650, time 0.70
Epoch:9
LR: 0.001
 * Train Acc 69.200, Loss 0.531
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.450, time 0.74
Epoch:10
LR: 0.001
 * Train Acc 68.720, Loss 0.522
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.750, time 0.74
Epoch:11
LR: 0.001
 * Train Acc 68.690, Loss 0.517
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.100, time 0.76
Epoch:12
LR: 0.001
 * Train Acc 69.080, Loss 0.512
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.400, time 0.65
Epoch:13
LR: 0.001
 * Train Acc 68.560, Loss 0.507
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.600, time 0.80
Epoch:14
LR: 0.001
 * Train Acc 68.070, Loss 0.498
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.600, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 67.750, Loss 0.496
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.300, time 0.74
Epoch:16
LR: 0.001
 * Train Acc 67.020, Loss 0.492
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.300, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 67.000, Loss 0.485
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.550, time 0.69
Epoch:18
LR: 0.001
 * Train Acc 67.620, Loss 0.477
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.650, time 0.70
Epoch:19
LR: 0.001
 * Train Acc 66.960, Loss 0.469
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.000, time 0.76
Epoch:20
LR: 0.001
 * Train Acc 67.070, Loss 0.461
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.550, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 65.940, Loss 0.463
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.500, time 0.74
Epoch:22
LR: 0.001
 * Train Acc 66.990, Loss 0.450
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.250, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 66.010, Loss 0.448
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.700, time 0.73
Epoch:24
LR: 0.001
 * Train Acc 66.320, Loss 0.442
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.750, time 0.71
Epoch:25
LR: 0.001
 * Train Acc 65.550, Loss 0.436
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.600, time 0.69
Epoch:26
LR: 0.001
 * Train Acc 66.060, Loss 0.425
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.500, time 0.72
Epoch:27
LR: 0.001
 * Train Acc 65.810, Loss 0.422
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.900, time 0.72
Epoch:28
LR: 0.001
 * Train Acc 65.370, Loss 0.413
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.950, time 0.73
Epoch:29
LR: 0.001
 * Train Acc 64.690, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.400, time 0.70
Epoch:30
LR: 0.001
 * Train Acc 64.190, Loss 0.409
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.700, time 0.74
Epoch:31
LR: 0.001
 * Train Acc 64.090, Loss 0.403
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.800, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 63.960, Loss 0.394
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.900, time 0.71
Epoch:33
LR: 0.001
 * Train Acc 63.760, Loss 0.387
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.500, time 0.76
Epoch:34
LR: 0.001
 * Train Acc 63.770, Loss 0.380
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.150, time 0.67
Epoch:35
LR: 0.001
 * Train Acc 63.480, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.950, time 0.67
Epoch:36
LR: 0.001
 * Train Acc 62.510, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.600, time 0.70
Epoch:37
LR: 0.001
 * Train Acc 62.790, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.900, time 0.76
Epoch:38
LR: 0.001
 * Train Acc 61.960, Loss 0.357
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.450, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 62.690, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.000, time 0.73
Epoch:40
LR: 0.001
 * Train Acc 61.650, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.350, time 0.67
Epoch:41
LR: 0.001
 * Train Acc 61.950, Loss 0.346
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.250, time 0.70
Epoch:42
LR: 0.001
 * Train Acc 61.560, Loss 0.349
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.300, time 0.69
Epoch:43
LR: 0.001
 * Train Acc 61.860, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.700, time 0.75
Epoch:44
LR: 0.001
 * Train Acc 60.990, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.150, time 0.70
Epoch:45
LR: 0.001
 * Train Acc 60.850, Loss 0.355
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.550, time 0.77
Epoch:46
LR: 0.001
 * Train Acc 60.430, Loss 0.357
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.100, time 0.68
Epoch:47
LR: 0.001
 * Train Acc 60.260, Loss 0.359
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.700, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 60.510, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.350, time 0.70
Epoch:49
LR: 0.001
 * Train Acc 59.750, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.400, time 0.72
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.0282378196716309 - mean: 0.001190090086311102 - std: 0.0007769500953145325
 * min 0.0004789423546753824, max: 0.0034565527457743883
sum: 21.27719497680664 - mean: 0.0023087235167622566 - std: 0.001625329488888383
 * min 0.0004737330600619316, max: 0.008683682419359684
sum: 8.6549072265625 - mean: 0.00046955878497101367 - std: 0.0002112385700456798
 * min 9.072916873265058e-05, max: 0.0015121566830202937
sum: 21.706045150756836 - mean: 0.0005888141458854079 - std: 0.00018081930465996265
 * min 0.00014135190576780587, max: 0.0017761668423190713
sum: 124.63497924804688 - mean: 0.0016904701478779316 - std: 0.0005643822369165719
 * min 0.00037867994979023933, max: 0.0056061167269945145
sum: 384.52935791015625 - mean: 0.0026077565271407366 - std: 0.0005056004738435149
 * min 0.0006638085469603539, max: 0.008502032607793808
sum: 569.4237060546875 - mean: 0.0038616517558693886 - std: 0.0005927993333898485
 * min 0.0008534429362043738, max: 0.01171091292053461
sum: 10.949302673339844 - mean: 1.3365847735258285e-05 - std: 9.658015898139638e-08
 * min 9.57277552515734e-06, max: 1.4640195331594441e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 0.00022116508625913411
 * min 0.00019315825193189085, max: 0.005369097925722599
eps: tensor([0.0107, 0.0208, 0.0042, 0.0053, 0.0152, 0.0235, 0.0348, 0.0428, 0.0428],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.300, time 0.71
 * Lower 1 Val Acc 77.950, time 0.69
 * Upper 1 Val Acc 77.950, time 0.67
validation split name: 2
 *  Val Acc 76.100, time 0.69
 * Lower 1 Val Acc 75.000, time 0.68
 * Upper 1 Val Acc 75.000, time 0.63
validation split name: 3
 *  Val Acc 78.850, time 0.72
 * Lower 1 Val Acc 79.100, time 0.73
 * Upper 1 Val Acc 79.100, time 0.72
validation split name: 4
 *  Val Acc 63.400, time 0.68
 * Lower 1 Val Acc 63.650, time 0.66
 * Upper 1 Val Acc 63.650, time 0.68
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.410, Loss 0.451
 * robust loss: 0.003 robust error: 0.00000000
 *  Val Acc 77.150, time 0.65
Epoch:1
LR: 0.001
 * Train Acc 80.130, Loss 0.427
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.700, time 0.67
Epoch:2
LR: 0.001
 * Train Acc 79.850, Loss 0.422
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.150, time 0.67
Epoch:3
LR: 0.001
 * Train Acc 79.830, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.550, time 0.68
Epoch:4
LR: 0.001
 * Train Acc 79.830, Loss 0.413
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.900, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 79.710, Loss 0.407
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.69
Epoch:6
LR: 0.001
 * Train Acc 79.370, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.200, time 0.64
Epoch:7
LR: 0.001
 * Train Acc 79.560, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.350, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 79.140, Loss 0.394
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 79.410, Loss 0.390
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.100, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 79.580, Loss 0.380
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.800, time 0.66
Epoch:11
LR: 0.001
 * Train Acc 78.880, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.850, time 0.69
Epoch:12
LR: 0.001
 * Train Acc 78.850, Loss 0.378
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.300, time 0.70
Epoch:13
LR: 0.001
 * Train Acc 79.440, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.600, time 0.68
Epoch:14
LR: 0.001
 * Train Acc 79.400, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.550, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 79.130, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.64
Epoch:16
LR: 0.001
 * Train Acc 79.160, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 78.880, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.400, time 0.67
Epoch:18
LR: 0.001
 * Train Acc 78.660, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.650, time 0.66
Epoch:19
LR: 0.001
 * Train Acc 78.820, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.250, time 0.66
Epoch:20
LR: 0.001
 * Train Acc 78.690, Loss 0.335
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.700, time 0.64
Epoch:21
LR: 0.001
 * Train Acc 78.620, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.050, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 79.380, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.300, time 0.65
Epoch:23
LR: 0.001
 * Train Acc 78.250, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.550, time 0.67
Epoch:24
LR: 0.001
 * Train Acc 78.640, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.150, time 0.64
Epoch:25
LR: 0.001
 * Train Acc 78.520, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.100, time 0.62
Epoch:26
LR: 0.001
 * Train Acc 78.370, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.450, time 0.64
Epoch:27
LR: 0.001
 * Train Acc 77.780, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.63
Epoch:28
LR: 0.001
 * Train Acc 78.480, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.800, time 0.65
Epoch:29
LR: 0.001
 * Train Acc 77.630, Loss 0.294
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.63
Epoch:30
LR: 0.001
 * Train Acc 78.430, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.900, time 0.67
Epoch:31
LR: 0.001
 * Train Acc 78.400, Loss 0.282
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.800, time 0.64
Epoch:32
LR: 0.001
 * Train Acc 77.740, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.900, time 0.64
Epoch:33
LR: 0.001
 * Train Acc 78.050, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.150, time 0.63
Epoch:34
LR: 0.001
 * Train Acc 77.430, Loss 0.268
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.62
Epoch:35
LR: 0.001
 * Train Acc 77.780, Loss 0.261
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.750, time 0.63
Epoch:36
LR: 0.001
 * Train Acc 78.220, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.250, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 77.170, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.750, time 0.66
Epoch:38
LR: 0.001
 * Train Acc 77.240, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.050, time 0.65
Epoch:39
LR: 0.001
 * Train Acc 77.470, Loss 0.239
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.750, time 0.65
Epoch:40
LR: 0.001
 * Train Acc 76.840, Loss 0.239
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.550, time 0.61
Epoch:41
LR: 0.001
 * Train Acc 77.140, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.150, time 0.64
Epoch:42
LR: 0.001
 * Train Acc 77.010, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.350, time 0.67
Epoch:43
LR: 0.001
 * Train Acc 76.540, Loss 0.241
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.450, time 0.62
Epoch:44
LR: 0.001
 * Train Acc 76.650, Loss 0.241
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.200, time 0.61
Epoch:45
LR: 0.001
 * Train Acc 76.740, Loss 0.241
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.000, time 0.64
Epoch:46
LR: 0.001
 * Train Acc 76.710, Loss 0.241
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.850, time 0.65
Epoch:47
LR: 0.001
 * Train Acc 76.530, Loss 0.244
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.150, time 0.63
Epoch:48
LR: 0.001
 * Train Acc 75.710, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.050, time 0.60
Epoch:49
LR: 0.001
 * Train Acc 76.710, Loss 0.244
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.450, time 0.62
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.521165132522583 - mean: 0.000603200402110815 - std: 0.0003945301577914506
 * min 0.0002423305413685739, max: 0.0017540737753733993
sum: 10.75599479675293 - mean: 0.0011671001557260752 - std: 0.000823280424810946
 * min 0.00023924987181089818, max: 0.004393934737890959
sum: 4.372927665710449 - mean: 0.00023724650964140892 - std: 0.00010695320088416338
 * min 4.57907299278304e-05, max: 0.0007642163545824587
sum: 10.958407402038574 - mean: 0.0002972658257931471 - std: 9.135132131632417e-05
 * min 7.133498002076522e-05, max: 0.00089660519734025
sum: 62.77714538574219 - mean: 0.0008514695218764246 - std: 0.00028435117565095425
 * min 0.00019058895122725517, max: 0.002826683223247528
sum: 192.07908630371094 - mean: 0.0013026196975260973 - std: 0.00025258545065298676
 * min 0.00033155991695821285, max: 0.004246815573424101
sum: 283.2183532714844 - mean: 0.0019206973956897855 - std: 0.00029487014398910105
 * min 0.00042446900624781847, max: 0.005825410597026348
sum: 5.450364589691162 - mean: 6.65327706883545e-06 - std: 4.8076135072960824e-08
 * min 4.765154699271079e-06, max: 7.287628250196576e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 0.00011055216600652784
 * min 9.659308852860704e-05, max: 0.00268386397510767
eps: tensor([0.0054, 0.0105, 0.0021, 0.0027, 0.0077, 0.0117, 0.0173, 0.0213, 0.0213],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.300, time 0.61
 * Lower 1 Val Acc 86.750, time 0.61
 * Upper 1 Val Acc 86.750, time 0.61
validation split name: 2
 *  Val Acc 76.050, time 0.62
 * Lower 1 Val Acc 76.100, time 0.65
 * Upper 1 Val Acc 76.100, time 0.63
validation split name: 3
 *  Val Acc 70.650, time 0.62
 * Lower 1 Val Acc 70.850, time 0.64
 * Upper 1 Val Acc 70.850, time 0.68
validation split name: 4
 *  Val Acc 59.500, time 0.77
 * Lower 1 Val Acc 60.500, time 0.82
 * Upper 1 Val Acc 60.500, time 0.68
validation split name: 5
 *  Val Acc 74.450, time 0.64
 * Lower 1 Val Acc 74.700, time 0.61
 * Upper 1 Val Acc 74.700, time 0.64
Task 1 average acc: 98.0
Task 2 average acc: 85.325
Task 3 average acc: 81.56666666666666
Task 4 average acc: 75.1625
Task 5 average acc: 73.59
===Summary of experiment repeats: 5 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64 73.59  0.    0.    0.    0.    0.  ]
mean: 37.638 std: 37.65707763488823
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.460, Loss 0.568
 * robust loss: 0.386 robust error: 0.02000000
 *  Val Acc 81.900, time 0.64
Epoch:1
LR: 0.001
 * Train Acc 80.530, Loss 0.414
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.150, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 85.310, Loss 0.338
 * robust loss: 0.007 robust error: 0.01000000
 *  Val Acc 87.600, time 0.61
Epoch:3
LR: 0.001
 * Train Acc 86.880, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.350, time 0.63
Epoch:4
LR: 0.001
 * Train Acc 88.280, Loss 0.260
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 91.600, time 0.64
Epoch:5
LR: 0.001
 * Train Acc 89.770, Loss 0.229
 * robust loss: 1.538 robust error: 0.01000000
 *  Val Acc 90.900, time 0.63
Epoch:6
LR: 0.001
 * Train Acc 90.950, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.300, time 0.64
Epoch:7
LR: 0.001
 * Train Acc 92.170, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.500, time 0.63
Epoch:8
LR: 0.001
 * Train Acc 93.160, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 92.650, time 0.62
Epoch:9
LR: 0.001
 * Train Acc 93.200, Loss 0.163
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.200, time 0.63
Epoch:10
LR: 0.001
 * Train Acc 94.350, Loss 0.144
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.300, time 0.63
Epoch:11
LR: 0.001
 * Train Acc 94.780, Loss 0.128
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.65
Epoch:12
LR: 0.001
 * Train Acc 95.100, Loss 0.109
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.550, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 95.580, Loss 0.092
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.500, time 0.65
Epoch:14
LR: 0.001
 * Train Acc 95.960, Loss 0.091
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.64
Epoch:15
LR: 0.001
 * Train Acc 95.890, Loss 0.100
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.300, time 0.63
Epoch:16
LR: 0.001
 * Train Acc 96.100, Loss 0.366
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.61
Epoch:17
LR: 0.001
 * Train Acc 96.310, Loss 0.073
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.64
Epoch:18
LR: 0.001
 * Train Acc 96.900, Loss 0.145
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.63
Epoch:19
LR: 0.001
 * Train Acc 96.840, Loss 0.764
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.64
Epoch:20
LR: 0.001
 * Train Acc 97.300, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.700, time 0.62
Epoch:21
LR: 0.001
 * Train Acc 96.940, Loss 0.057
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.64
Epoch:22
LR: 0.001
 * Train Acc 97.440, Loss 0.163
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.65
Epoch:23
LR: 0.001
 * Train Acc 97.550, Loss 3.685
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.62
Epoch:24
LR: 0.001
 * Train Acc 97.260, Loss 0.099
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.400, time 0.63
Epoch:25
LR: 0.001
 * Train Acc 97.210, Loss 1.610
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.64
Epoch:26
LR: 0.001
 * Train Acc 97.410, Loss 0.046
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 97.500, Loss 1.335
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.650, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 97.610, Loss 0.065
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.68
Epoch:29
LR: 0.001
 * Train Acc 97.790, Loss 1.258
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.750, time 0.67
Epoch:30
LR: 0.001
 * Train Acc 97.900, Loss 4.869
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.64
Epoch:31
LR: 0.001
 * Train Acc 97.880, Loss 0.560
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 97.350, time 0.66
Epoch:32
LR: 0.001
 * Train Acc 97.610, Loss 3.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.66
Epoch:33
LR: 0.001
 * Train Acc 98.070, Loss 0.150
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.63
Epoch:34
LR: 0.001
 * Train Acc 97.830, Loss 0.031
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.68
Epoch:35
LR: 0.001
 * Train Acc 97.990, Loss 0.029
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.200, time 0.61
Epoch:36
LR: 0.001
 * Train Acc 98.190, Loss 6.005
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 0.61
Epoch:37
LR: 0.001
 * Train Acc 98.000, Loss 0.027
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.550, time 0.64
Epoch:38
LR: 0.001
 * Train Acc 98.160, Loss 1.081
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.67
Epoch:39
LR: 0.001
 * Train Acc 98.330, Loss 1.469
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.63
Epoch:40
LR: 0.001
 * Train Acc 98.380, Loss 329.789
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.65
Epoch:41
LR: 0.001
 * Train Acc 98.420, Loss 0.812
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.66
Epoch:42
LR: 0.001
 * Train Acc 98.400, Loss 0.034
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 98.120, Loss 0.024
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.64
Epoch:44
LR: 0.001
 * Train Acc 98.350, Loss 0.024
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.66
Epoch:45
LR: 0.001
 * Train Acc 98.450, Loss 1.072
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.66
Epoch:46
LR: 0.001
 * Train Acc 98.410, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.250, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 98.450, Loss 0.021
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.250, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 98.530, Loss 0.941
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.150, time 0.65
Epoch:49
LR: 0.001
 * Train Acc 98.620, Loss 1.772
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.62
after batch eps: 2.500000000000002, kappa: 0.5
sum: 12.638242721557617 - mean: 0.01462759543210268 - std: 0.004907726775854826
 * min 0.008585775271058083, max: 0.028595294803380966
sum: 259.29388427734375 - mean: 0.028135187923908234 - std: 0.012145694345235825
 * min 0.01093676220625639, max: 0.06893115490674973
sum: 183.17501831054688 - mean: 0.00993788056075573 - std: 0.003328301478177309
 * min 0.003701017005369067, max: 0.021248824894428253
sum: 504.6162414550781 - mean: 0.013688591308891773 - std: 0.0035471986047923565
 * min 0.0054452624171972275, max: 0.027692263945937157
sum: 2829.41064453125 - mean: 0.0383763387799263 - std: 0.01096068974584341
 * min 0.014656433835625648, max: 0.08779483288526535
sum: 5616.140625 - mean: 0.038086891174316406 - std: 0.007777558173984289
 * min 0.01594085805118084, max: 0.08503148704767227
sum: 6259.8984375 - mean: 0.042452652007341385 - std: 0.0074126324616372585
 * min 0.016048625111579895, max: 0.09616051614284515
sum: 106.52145385742188 - mean: 0.00013003106869291514 - std: 8.898609848984051e-07
 * min 0.00010762333840830252, max: 0.0001343334442935884
sum: 5.0 - mean: 0.009765625 - std: 0.00485539436340332
 * min 0.0026724091731011868, max: 0.03899379447102547
eps: tensor([0.1316, 0.2532, 0.0894, 0.1232, 0.3454, 0.3428, 0.3821, 0.4161, 0.4162],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.050, time 0.64
 * Lower 1 Val Acc 50.000, time 0.62
 * Upper 1 Val Acc 50.000, time 0.61
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.720, Loss 0.590
 * robust loss: 0.149 robust error: 0.01000000
 *  Val Acc 77.350, time 0.64
Epoch:1
LR: 0.001
 * Train Acc 79.400, Loss 0.443
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 80.900, Loss 0.406
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.63
Epoch:3
LR: 0.001
 * Train Acc 82.090, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.73
Epoch:4
LR: 0.001
 * Train Acc 82.780, Loss 0.375
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.65
Epoch:5
LR: 0.001
 * Train Acc 82.990, Loss 0.359
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.67
Epoch:6
LR: 0.001
 * Train Acc 83.420, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.64
Epoch:7
LR: 0.001
 * Train Acc 83.390, Loss 0.337
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.62
Epoch:8
LR: 0.001
 * Train Acc 84.190, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.65
Epoch:9
LR: 0.001
 * Train Acc 83.860, Loss 0.320
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 84.440, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.63
Epoch:11
LR: 0.001
 * Train Acc 84.550, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.62
Epoch:12
LR: 0.001
 * Train Acc 84.280, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.65
Epoch:13
LR: 0.001
 * Train Acc 85.160, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.62
Epoch:14
LR: 0.001
 * Train Acc 85.170, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.62
Epoch:15
LR: 0.001
 * Train Acc 85.500, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.65
Epoch:16
LR: 0.001
 * Train Acc 84.870, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.69
Epoch:17
LR: 0.001
 * Train Acc 85.570, Loss 0.261
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 0.64
Epoch:18
LR: 0.001
 * Train Acc 85.120, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.64
Epoch:19
LR: 0.001
 * Train Acc 85.340, Loss 0.256
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.250, time 0.65
Epoch:20
LR: 0.001
 * Train Acc 85.480, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.62
Epoch:21
LR: 0.001
 * Train Acc 85.160, Loss 1.093
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.300, time 0.65
Epoch:22
LR: 0.001
 * Train Acc 84.870, Loss 0.370
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.63
Epoch:23
LR: 0.001
 * Train Acc 85.540, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.64
Epoch:24
LR: 0.001
 * Train Acc 85.820, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.63
Epoch:25
LR: 0.001
 * Train Acc 85.810, Loss 0.227
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.650, time 0.63
Epoch:26
LR: 0.001
 * Train Acc 86.140, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 0.66
Epoch:27
LR: 0.001
 * Train Acc 85.470, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.65
Epoch:28
LR: 0.001
 * Train Acc 85.530, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.65
Epoch:29
LR: 0.001
 * Train Acc 85.960, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 85.720, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.650, time 0.64
Epoch:31
LR: 0.001
 * Train Acc 85.180, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.63
Epoch:32
LR: 0.001
 * Train Acc 84.860, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.62
Epoch:33
LR: 0.001
 * Train Acc 85.330, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.66
Epoch:34
LR: 0.001
 * Train Acc 85.410, Loss 0.195
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.64
Epoch:35
LR: 0.001
 * Train Acc 85.750, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.66
Epoch:36
LR: 0.001
 * Train Acc 85.200, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.150, time 0.63
Epoch:37
LR: 0.001
 * Train Acc 85.790, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.63
Epoch:38
LR: 0.001
 * Train Acc 85.790, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.62
Epoch:39
LR: 0.001
 * Train Acc 85.720, Loss 0.169
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.61
Epoch:40
LR: 0.001
 * Train Acc 85.390, Loss 0.171
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.64
Epoch:41
LR: 0.001
 * Train Acc 85.390, Loss 0.167
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.64
Epoch:42
LR: 0.001
 * Train Acc 85.460, Loss 0.171
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 85.160, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 85.550, Loss 0.167
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.64
Epoch:45
LR: 0.001
 * Train Acc 85.060, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.62
Epoch:46
LR: 0.001
 * Train Acc 84.830, Loss 0.176
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 84.850, time 0.66
Epoch:47
LR: 0.001
 * Train Acc 84.930, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 84.690, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 0.62
Epoch:49
LR: 0.001
 * Train Acc 84.490, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.64
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.268836975097656 - mean: 0.004940783604979515 - std: 0.0025826124474406242
 * min 0.002646449487656355, max: 0.012691536918282509
sum: 96.12629699707031 - mean: 0.010430370457470417 - std: 0.005992963910102844
 * min 0.0029130277689546347, max: 0.03425713628530502
sum: 49.47895050048828 - mean: 0.0026844048406928778 - std: 0.0010746951447799802
 * min 0.000717679038643837, max: 0.007213354576379061
sum: 133.87413024902344 - mean: 0.003631568280979991 - std: 0.001068195910193026
 * min 0.001118646003305912, max: 0.008734597824513912
sum: 903.203369140625 - mean: 0.012250480242073536 - std: 0.003971280995756388
 * min 0.003625173820182681, max: 0.0351763591170311
sum: 2051.7548828125 - mean: 0.013914353214204311 - std: 0.0032388009130954742
 * min 0.005015252158045769, max: 0.035367414355278015
sum: 2368.03662109375 - mean: 0.016059275716543198 - std: 0.0031687619630247355
 * min 0.005170532036572695, max: 0.040561776608228683
sum: 41.57101821899414 - mean: 5.074587170383893e-05 - std: 3.808369797297928e-07
 * min 4.1509949369356036e-05, max: 5.2622464863816276e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.002018408151343465
 * min 0.0008544593583792448, max: 0.015767617151141167
eps: tensor([0.0445, 0.0939, 0.0242, 0.0327, 0.1103, 0.1252, 0.1445, 0.1624, 0.1624],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.350, time 0.64
 * Lower 1 Val Acc 75.900, time 0.65
 * Upper 1 Val Acc 75.900, time 0.65
validation split name: 2
 *  Val Acc 84.100, time 0.65
 * Lower 1 Val Acc 73.150, time 0.66
 * Upper 1 Val Acc 73.150, time 0.65
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 81.160, Loss 0.421
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.64
Epoch:1
LR: 0.001
 * Train Acc 83.910, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.62
Epoch:2
LR: 0.001
 * Train Acc 84.970, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.500, time 0.63
Epoch:3
LR: 0.001
 * Train Acc 85.140, Loss 0.335
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.750, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 85.310, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.950, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 85.890, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.67
Epoch:6
LR: 0.001
 * Train Acc 86.030, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.050, time 0.69
Epoch:7
LR: 0.001
 * Train Acc 85.270, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.150, time 0.68
Epoch:8
LR: 0.001
 * Train Acc 85.570, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 0.63
Epoch:9
LR: 0.001
 * Train Acc 85.370, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.750, time 0.67
Epoch:10
LR: 0.001
 * Train Acc 85.780, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.700, time 0.65
Epoch:11
LR: 0.001
 * Train Acc 85.350, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.050, time 0.66
Epoch:12
LR: 0.001
 * Train Acc 85.670, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.400, time 0.65
Epoch:13
LR: 0.001
 * Train Acc 85.660, Loss 0.282
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.63
Epoch:14
LR: 0.001
 * Train Acc 85.560, Loss 0.280
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.250, time 0.63
Epoch:15
LR: 0.001
 * Train Acc 86.280, Loss 0.268
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 87.400, time 0.63
Epoch:16
LR: 0.001
 * Train Acc 85.150, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.64
Epoch:17
LR: 0.001
 * Train Acc 85.250, Loss 0.268
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.63
Epoch:18
LR: 0.001
 * Train Acc 85.700, Loss 0.257
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.700, time 0.65
Epoch:19
LR: 0.001
 * Train Acc 85.200, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.850, time 0.64
Epoch:20
LR: 0.001
 * Train Acc 85.770, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.750, time 0.64
Epoch:21
LR: 0.001
 * Train Acc 85.340, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.600, time 0.64
Epoch:22
LR: 0.001
 * Train Acc 85.690, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.700, time 0.62
Epoch:23
LR: 0.001
 * Train Acc 85.470, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.62
Epoch:24
LR: 0.001
 * Train Acc 85.240, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.950, time 0.63
Epoch:25
LR: 0.001
 * Train Acc 85.780, Loss 0.234
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.150, time 0.62
Epoch:26
LR: 0.001
 * Train Acc 85.550, Loss 0.228
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.600, time 0.65
Epoch:27
LR: 0.001
 * Train Acc 85.280, Loss 0.228
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.62
Epoch:28
LR: 0.001
 * Train Acc 85.370, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.050, time 0.67
Epoch:29
LR: 0.001
 * Train Acc 85.480, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.250, time 0.66
Epoch:30
LR: 0.001
 * Train Acc 85.270, Loss 0.216
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.600, time 0.62
Epoch:31
LR: 0.001
 * Train Acc 85.700, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.150, time 0.64
Epoch:32
LR: 0.001
 * Train Acc 85.100, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.700, time 0.64
Epoch:33
LR: 0.001
 * Train Acc 85.240, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.71
Epoch:34
LR: 0.001
 * Train Acc 85.320, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.64
Epoch:35
LR: 0.001
 * Train Acc 85.110, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.68
Epoch:36
LR: 0.001
 * Train Acc 85.040, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 0.64
Epoch:37
LR: 0.001
 * Train Acc 85.140, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.300, time 0.70
Epoch:38
LR: 0.001
 * Train Acc 84.920, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.64
Epoch:39
LR: 0.001
 * Train Acc 85.310, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.68
Epoch:40
LR: 0.001
 * Train Acc 84.860, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 0.65
Epoch:41
LR: 0.001
 * Train Acc 84.880, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.61
Epoch:42
LR: 0.001
 * Train Acc 85.100, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.64
Epoch:43
LR: 0.001
 * Train Acc 84.660, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.65
Epoch:44
LR: 0.001
 * Train Acc 84.970, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.64
Epoch:45
LR: 0.001
 * Train Acc 84.510, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.64
Epoch:46
LR: 0.001
 * Train Acc 85.140, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 84.530, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.64
Epoch:48
LR: 0.001
 * Train Acc 84.250, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.61
Epoch:49
LR: 0.001
 * Train Acc 84.770, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.66
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.3915190696716309 - mean: 0.0016105545219033957 - std: 0.0009772059274837375
 * min 0.0007955383043736219, max: 0.004545918665826321
sum: 31.521106719970703 - mean: 0.003420259105041623 - std: 0.0021576453000307083
 * min 0.0008196729468181729, max: 0.012284601107239723
sum: 14.478127479553223 - mean: 0.0007854886935092509 - std: 0.0003353820357006043
 * min 0.00017668851069174707, max: 0.002208844292908907
sum: 38.26795959472656 - mean: 0.0010380848543718457 - std: 0.0003191643627360463
 * min 0.0002563356247264892, max: 0.002898177597671747
sum: 259.9568786621094 - mean: 0.003525890875607729 - std: 0.0011857697973027825
 * min 0.0008900739485397935, max: 0.01112296897917986
sum: 681.8802490234375 - mean: 0.004624296445399523 - std: 0.0011205043410882354
 * min 0.0015396138187497854, max: 0.01240633986890316
sum: 841.2985229492188 - mean: 0.005705420859158039 - std: 0.0011776621686294675
 * min 0.0017130870837718248, max: 0.015203351154923439
sum: 14.541252136230469 - mean: 1.775055170583073e-05 - std: 1.3547905552968587e-07
 * min 1.4455143173108809e-05, max: 1.841522680479102e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0007222637068480253
 * min 0.0002656695432960987, max: 0.005577137228101492
eps: tensor([0.0145, 0.0308, 0.0071, 0.0093, 0.0317, 0.0416, 0.0513, 0.0568, 0.0568],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.600, time 0.65
 * Lower 1 Val Acc 82.950, time 0.65
 * Upper 1 Val Acc 82.950, time 0.67
validation split name: 2
 *  Val Acc 76.950, time 0.64
 * Lower 1 Val Acc 75.000, time 0.69
 * Upper 1 Val Acc 75.000, time 0.64
validation split name: 3
 *  Val Acc 83.800, time 0.67
 * Lower 1 Val Acc 83.400, time 0.64
 * Upper 1 Val Acc 83.400, time 0.64
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 76.000, Loss 0.504
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.63
Epoch:1
LR: 0.001
 * Train Acc 78.500, Loss 0.455
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.64
Epoch:2
LR: 0.001
 * Train Acc 79.180, Loss 0.442
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.65
Epoch:3
LR: 0.001
 * Train Acc 78.430, Loss 0.446
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.65
Epoch:4
LR: 0.001
 * Train Acc 78.960, Loss 0.437
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.66
Epoch:5
LR: 0.001
 * Train Acc 78.010, Loss 0.439
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.64
Epoch:6
LR: 0.001
 * Train Acc 77.900, Loss 0.436
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.64
Epoch:7
LR: 0.001
 * Train Acc 77.880, Loss 0.429
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.66
Epoch:8
LR: 0.001
 * Train Acc 77.950, Loss 0.423
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.65
Epoch:9
LR: 0.001
 * Train Acc 77.800, Loss 0.416
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 77.760, Loss 0.415
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.67
Epoch:11
LR: 0.001
 * Train Acc 77.380, Loss 0.412
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.64
Epoch:12
LR: 0.001
 * Train Acc 77.240, Loss 0.403
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.64
Epoch:13
LR: 0.001
 * Train Acc 77.070, Loss 0.404
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.66
Epoch:14
LR: 0.001
 * Train Acc 77.450, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.69
Epoch:15
LR: 0.001
 * Train Acc 76.640, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.63
Epoch:16
LR: 0.001
 * Train Acc 77.130, Loss 0.390
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.67
Epoch:17
LR: 0.001
 * Train Acc 76.310, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.65
Epoch:18
LR: 0.001
 * Train Acc 75.870, Loss 0.385
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 76.250, Loss 0.378
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.69
Epoch:20
LR: 0.001
 * Train Acc 75.720, Loss 0.374
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.68
Epoch:21
LR: 0.001
 * Train Acc 76.040, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.64
Epoch:22
LR: 0.001
 * Train Acc 74.650, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.68
Epoch:23
LR: 0.001
 * Train Acc 75.350, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.63
Epoch:24
LR: 0.001
 * Train Acc 75.280, Loss 0.363
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.65
Epoch:25
LR: 0.001
 * Train Acc 75.150, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.70
Epoch:26
LR: 0.001
 * Train Acc 74.370, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.450, time 0.68
Epoch:27
LR: 0.001
 * Train Acc 74.470, Loss 0.343
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.73
Epoch:28
LR: 0.001
 * Train Acc 74.250, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.250, time 0.66
Epoch:29
LR: 0.001
 * Train Acc 74.020, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.73
Epoch:30
LR: 0.001
 * Train Acc 73.400, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.68
Epoch:31
LR: 0.001
 * Train Acc 73.580, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.70
Epoch:32
LR: 0.001
 * Train Acc 73.310, Loss 0.325
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.350, time 0.78
Epoch:33
LR: 0.001
 * Train Acc 72.840, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 0.70
Epoch:34
LR: 0.001
 * Train Acc 72.560, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.79
Epoch:35
LR: 0.001
 * Train Acc 72.510, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.200, time 0.76
Epoch:36
LR: 0.001
 * Train Acc 72.850, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.74
Epoch:37
LR: 0.001
 * Train Acc 71.950, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.750, time 0.79
Epoch:38
LR: 0.001
 * Train Acc 72.310, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.100, time 0.68
Epoch:39
LR: 0.001
 * Train Acc 71.640, Loss 0.290
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.350, time 0.65
Epoch:40
LR: 0.001
 * Train Acc 71.580, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.650, time 0.69
Epoch:41
LR: 0.001
 * Train Acc 71.130, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.900, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 71.090, Loss 0.292
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.72
Epoch:43
LR: 0.001
 * Train Acc 70.970, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.600, time 0.73
Epoch:44
LR: 0.001
 * Train Acc 70.330, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.78
Epoch:45
LR: 0.001
 * Train Acc 69.830, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.450, time 0.72
Epoch:46
LR: 0.001
 * Train Acc 69.310, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.300, time 0.64
Epoch:47
LR: 0.001
 * Train Acc 70.080, Loss 0.297
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.250, time 0.74
Epoch:48
LR: 0.001
 * Train Acc 69.430, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.000, time 0.70
Epoch:49
LR: 0.001
 * Train Acc 69.650, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.800, time 0.68
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 0.9341133236885071 - mean: 0.0010811496758833528 - std: 0.0007758632418699563
 * min 0.00046568570542149246, max: 0.003422078210860491
sum: 20.749561309814453 - mean: 0.0022514716256409883 - std: 0.0015673928428441286
 * min 0.00046438915887847543, max: 0.009064694866538048
sum: 8.577903747558594 - mean: 0.0004653810756281018 - std: 0.00021236638713162392
 * min 9.248748392565176e-05, max: 0.0014342906652018428
sum: 22.40888786315918 - mean: 0.0006078800070099533 - std: 0.00019436798174865544
 * min 0.00013353126996662468, max: 0.0018480513244867325
sum: 150.8983612060547 - mean: 0.0020466900896281004 - std: 0.0007115307962521911
 * min 0.00045317047624848783, max: 0.006830551661550999
sum: 432.8362121582031 - mean: 0.0029353585559874773 - std: 0.0007367934449575841
 * min 0.0008962436113506556, max: 0.008278201334178448
sum: 585.0477905273438 - mean: 0.003967609256505966 - std: 0.0008564697345718741
 * min 0.0010905934032052755, max: 0.011274846270680428
sum: 10.21370792388916 - mean: 1.2467904525692575e-05 - std: 9.624623231729856e-08
 * min 1.0129022484761663e-05, max: 1.2940066881128587e-05
sum: 0.40000003576278687 - mean: 0.0007812500698491931 - std: 0.0005377145134843886
 * min 0.00015652480942662805, max: 0.004106926266103983
eps: tensor([0.0097, 0.0203, 0.0042, 0.0055, 0.0184, 0.0264, 0.0357, 0.0399, 0.0399],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 77.500, time 0.80
 * Lower 1 Val Acc 74.850, time 0.86
 * Upper 1 Val Acc 74.850, time 0.83
validation split name: 2
 *  Val Acc 72.750, time 0.68
 * Lower 1 Val Acc 70.850, time 0.71
 * Upper 1 Val Acc 70.850, time 0.69
validation split name: 3
 *  Val Acc 81.950, time 0.68
 * Lower 1 Val Acc 81.150, time 0.71
 * Upper 1 Val Acc 81.150, time 0.69
validation split name: 4
 *  Val Acc 73.800, time 0.66
 * Lower 1 Val Acc 73.650, time 0.78
 * Upper 1 Val Acc 73.650, time 0.78
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 80.590, Loss 0.417
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.71
Epoch:1
LR: 0.001
 * Train Acc 82.080, Loss 0.387
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.68
Epoch:2
LR: 0.001
 * Train Acc 82.320, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.93
Epoch:3
LR: 0.001
 * Train Acc 81.760, Loss 0.382
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 1.05
Epoch:4
LR: 0.001
 * Train Acc 82.170, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.67
Epoch:5
LR: 0.001
 * Train Acc 81.820, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.93
Epoch:6
LR: 0.001
 * Train Acc 81.890, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.94
Epoch:7
LR: 0.001
 * Train Acc 82.310, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.750, time 0.93
Epoch:8
LR: 0.001
 * Train Acc 81.730, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.93
Epoch:9
LR: 0.001
 * Train Acc 81.900, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.93
Epoch:10
LR: 0.001
 * Train Acc 81.510, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.82
Epoch:11
LR: 0.001
 * Train Acc 81.900, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 1.01
Epoch:12
LR: 0.001
 * Train Acc 81.880, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.94
Epoch:13
LR: 0.001
 * Train Acc 81.430, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.95
Epoch:14
LR: 0.001
 * Train Acc 81.480, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 1.11
Epoch:15
LR: 0.001
 * Train Acc 81.610, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 81.530, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.97
Epoch:17
LR: 0.001
 * Train Acc 81.090, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.74
Epoch:18
LR: 0.001
 * Train Acc 80.960, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.95
Epoch:19
LR: 0.001
 * Train Acc 81.580, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.90
Epoch:20
LR: 0.001
 * Train Acc 81.300, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.97
Epoch:21
LR: 0.001
 * Train Acc 80.950, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.81
Epoch:22
LR: 0.001
 * Train Acc 80.770, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.94
Epoch:23
LR: 0.001
 * Train Acc 80.790, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.94
Epoch:24
LR: 0.001
 * Train Acc 80.890, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.96
Epoch:25
LR: 0.001
 * Train Acc 80.660, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.96
Epoch:26
LR: 0.001
 * Train Acc 80.840, Loss 0.285
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.900, time 0.93
Epoch:27
LR: 0.001
 * Train Acc 80.790, Loss 0.277
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 0.98
Epoch:28
LR: 0.001
 * Train Acc 80.810, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.78
Epoch:29
LR: 0.001
 * Train Acc 80.370, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.94
Epoch:30
LR: 0.001
 * Train Acc 80.680, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 1.01
Epoch:31
LR: 0.001
 * Train Acc 79.880, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.95
Epoch:32
LR: 0.001
 * Train Acc 80.140, Loss 0.256
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.96
Epoch:33
LR: 0.001
 * Train Acc 80.130, Loss 0.251
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.90
Epoch:34
LR: 0.001
 * Train Acc 79.950, Loss 0.248
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.98
Epoch:35
LR: 0.001
 * Train Acc 79.710, Loss 0.241
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.72
Epoch:36
LR: 0.001
 * Train Acc 79.960, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 0.74
Epoch:37
LR: 0.001
 * Train Acc 79.520, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.91
Epoch:38
LR: 0.001
 * Train Acc 79.410, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.450, time 0.90
Epoch:39
LR: 0.001
 * Train Acc 79.570, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.950, time 0.91
Epoch:40
LR: 0.001
 * Train Acc 79.610, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.800, time 0.98
Epoch:41
LR: 0.001
 * Train Acc 79.670, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.650, time 0.96
Epoch:42
LR: 0.001
 * Train Acc 79.040, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.96
Epoch:43
LR: 0.001
 * Train Acc 78.440, Loss 0.226
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.81
Epoch:44
LR: 0.001
 * Train Acc 79.020, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.400, time 0.93
Epoch:45
LR: 0.001
 * Train Acc 79.240, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.800, time 0.88
Epoch:46
LR: 0.001
 * Train Acc 78.700, Loss 0.226
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 0.80
Epoch:47
LR: 0.001
 * Train Acc 78.710, Loss 0.226
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 0.69
Epoch:48
LR: 0.001
 * Train Acc 78.620, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.900, time 0.86
Epoch:49
LR: 0.001
 * Train Acc 78.380, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 1.14
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.4550071656703949 - mean: 0.000526628689840436 - std: 0.00039856633520685136
 * min 0.00019631687609944493, max: 0.0017473683692514896
sum: 9.927353858947754 - mean: 0.001077186898328364 - std: 0.0007883539074100554
 * min 0.00020558676624204963, max: 0.00465280981734395
sum: 3.6484944820404053 - mean: 0.00019794348918367177 - std: 9.484385373070836e-05
 * min 3.7709676689701155e-05, max: 0.0006198455230332911
sum: 9.502607345581055 - mean: 0.0002577747218310833 - std: 8.523868018528447e-05
 * min 5.197177961235866e-05, max: 0.00080796581460163
sum: 65.37591552734375 - mean: 0.000886717636603862 - std: 0.00031551599386148155
 * min 0.0001797300501493737, max: 0.003072621999308467
sum: 207.2340850830078 - mean: 0.001405396149493754 - std: 0.00035924819530919194
 * min 0.0004191214684396982, max: 0.004217915236949921
sum: 307.88409423828125 - mean: 0.002087972592562437 - std: 0.0004597627557814121
 * min 0.000541266577783972, max: 0.0062302700243890285
sum: 5.3812456130981445 - mean: 6.568903245351976e-06 - std: 5.083278864503882e-08
 * min 5.3342168939707335e-06, max: 6.818238034611568e-06
sum: 0.19999998807907104 - mean: 0.00039062497671693563 - std: 0.0002925877633970231
 * min 7.184163405327126e-05, max: 0.0022373595274984837
eps: tensor([0.0047, 0.0097, 0.0018, 0.0023, 0.0080, 0.0126, 0.0188, 0.0210, 0.0210],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 85.300, time 1.07
 * Lower 1 Val Acc 84.900, time 1.07
 * Upper 1 Val Acc 84.900, time 1.04
validation split name: 2
 *  Val Acc 76.000, time 1.01
 * Lower 1 Val Acc 75.600, time 1.01
 * Upper 1 Val Acc 75.600, time 1.02
validation split name: 3
 *  Val Acc 78.950, time 1.02
 * Lower 1 Val Acc 79.450, time 1.05
 * Upper 1 Val Acc 79.450, time 1.08
validation split name: 4
 *  Val Acc 70.400, time 1.05
 * Lower 1 Val Acc 71.100, time 1.09
 * Upper 1 Val Acc 71.100, time 1.09
validation split name: 5
 *  Val Acc 79.000, time 1.01
 * Lower 1 Val Acc 78.550, time 1.06
 * Upper 1 Val Acc 78.550, time 1.07
Task 1 average acc: 97.05
Task 2 average acc: 84.725
Task 3 average acc: 82.11666666666667
Task 4 average acc: 76.5
Task 5 average acc: 77.92999999999999
===Summary of experiment repeats: 6 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64 73.59 77.93  0.    0.    0.    0.  ]
mean: 45.431 std: 37.12152029483706
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.720, Loss 0.563
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.000, time 0.98
Epoch:1
LR: 0.001
 * Train Acc 81.110, Loss 0.412
 * robust loss: 0.248 robust error: 0.01000000
 *  Val Acc 85.750, time 0.94
Epoch:2
LR: 0.001
 * Train Acc 84.810, Loss 0.342
 * robust loss: 0.400 robust error: 0.02000000
 *  Val Acc 88.250, time 0.88
Epoch:3
LR: 0.001
 * Train Acc 86.590, Loss 0.307
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 88.450, time 0.89
Epoch:4
LR: 0.001
 * Train Acc 88.780, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.950, time 1.04
Epoch:5
LR: 0.001
 * Train Acc 90.000, Loss 0.233
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.550, time 0.99
Epoch:6
LR: 0.001
 * Train Acc 90.470, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 91.800, time 1.11
Epoch:7
LR: 0.001
 * Train Acc 91.580, Loss 0.193
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.150, time 0.95
Epoch:8
LR: 0.001
 * Train Acc 92.560, Loss 0.167
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.500, time 1.05
Epoch:9
LR: 0.001
 * Train Acc 93.890, Loss 0.144
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 93.350, time 0.94
Epoch:10
LR: 0.001
 * Train Acc 94.050, Loss 0.133
 * robust loss: 0.035 robust error: 0.00000000
 *  Val Acc 93.900, time 0.72
Epoch:11
LR: 0.001
 * Train Acc 94.980, Loss 0.168
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 95.550, time 1.13
Epoch:12
LR: 0.001
 * Train Acc 94.880, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.050, time 0.99
Epoch:13
LR: 0.001
 * Train Acc 95.660, Loss 0.097
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.750, time 1.10
Epoch:14
LR: 0.001
 * Train Acc 95.710, Loss 0.094
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.800, time 1.00
Epoch:15
LR: 0.001
 * Train Acc 95.840, Loss 0.624
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.150, time 0.86
Epoch:16
LR: 0.001
 * Train Acc 95.940, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 0.97
Epoch:17
LR: 0.001
 * Train Acc 96.450, Loss 0.149
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.250, time 0.80
Epoch:18
LR: 0.001
 * Train Acc 96.780, Loss 0.068
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.89
Epoch:19
LR: 0.001
 * Train Acc 96.950, Loss 0.081
 * robust loss: 9.591 robust error: 0.01000000
 *  Val Acc 96.850, time 0.89
Epoch:20
LR: 0.001
 * Train Acc 96.790, Loss 0.063
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 1.03
Epoch:21
LR: 0.001
 * Train Acc 97.110, Loss 0.471
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.650, time 0.95
Epoch:22
LR: 0.001
 * Train Acc 97.080, Loss 0.061
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.300, time 1.03
Epoch:23
LR: 0.001
 * Train Acc 97.180, Loss 0.518
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 1.04
Epoch:24
LR: 0.001
 * Train Acc 97.610, Loss 11.057
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.71
Epoch:25
LR: 0.001
 * Train Acc 97.450, Loss 0.045
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 1.04
Epoch:26
LR: 0.001
 * Train Acc 97.360, Loss 9.678
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 0.90
Epoch:27
LR: 0.001
 * Train Acc 97.700, Loss 8.672
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.400, time 1.01
Epoch:28
LR: 0.001
 * Train Acc 97.780, Loss 2.728
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.450, time 0.95
Epoch:29
LR: 0.001
 * Train Acc 97.520, Loss 48.454
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.000, time 0.95
Epoch:30
LR: 0.001
 * Train Acc 97.660, Loss 0.038
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 0.88
Epoch:31
LR: 0.001
 * Train Acc 97.770, Loss 0.139
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 97.990, Loss 1.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 1.05
Epoch:33
LR: 0.001
 * Train Acc 98.090, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 0.91
Epoch:34
LR: 0.001
 * Train Acc 98.120, Loss 2.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 0.79
Epoch:35
LR: 0.001
 * Train Acc 98.200, Loss 0.027
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.96
Epoch:36
LR: 0.001
 * Train Acc 98.030, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.91
Epoch:37
LR: 0.001
 * Train Acc 98.480, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.98
Epoch:38
LR: 0.001
 * Train Acc 98.540, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.950, time 0.98
Epoch:39
LR: 0.001
 * Train Acc 98.280, Loss 0.023
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.72
Epoch:40
LR: 0.001
 * Train Acc 98.250, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 1.00
Epoch:41
LR: 0.001
 * Train Acc 98.470, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 0.97
Epoch:42
LR: 0.001
 * Train Acc 98.440, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.200, time 0.99
Epoch:43
LR: 0.001
 * Train Acc 98.720, Loss 25.588
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.500, time 0.99
Epoch:44
LR: 0.001
 * Train Acc 98.140, Loss 85.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.92
Epoch:45
LR: 0.001
 * Train Acc 98.670, Loss 3.959
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.99
Epoch:46
LR: 0.001
 * Train Acc 98.430, Loss 0.430
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.98
Epoch:47
LR: 0.001
 * Train Acc 98.340, Loss 0.697
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.96
Epoch:48
LR: 0.001
 * Train Acc 98.580, Loss 1.145
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.950, time 1.01
Epoch:49
LR: 0.001
 * Train Acc 98.400, Loss 0.021
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.99
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.79163932800293 - mean: 0.015962544828653336 - std: 0.004693187773227692
 * min 0.010428179055452347, max: 0.02820977009832859
sum: 289.6142578125 - mean: 0.03142515942454338 - std: 0.01257831696420908
 * min 0.013360263779759407, max: 0.06974521279335022
sum: 203.88841247558594 - mean: 0.011061654426157475 - std: 0.0033936817198991776
 * min 0.004486935678869486, max: 0.022791696712374687
sum: 518.6408081054688 - mean: 0.014069032855331898 - std: 0.003531722817569971
 * min 0.005201621446758509, max: 0.03072941116988659
sum: 2360.260498046875 - mean: 0.03201308101415634 - std: 0.009097758680582047
 * min 0.01047123409807682, max: 0.07686077803373337
sum: 5128.85009765625 - mean: 0.03478224202990532 - std: 0.008187856525182724
 * min 0.012718930840492249, max: 0.08384975790977478
sum: 6164.7158203125 - mean: 0.041807156056165695 - std: 0.008447063155472279
 * min 0.014230212196707726, max: 0.09349562972784042
sum: 111.32722473144531 - mean: 0.00013589748414233327 - std: 1.0117672673004563e-06
 * min 0.00010827060759766027, max: 0.00014141348947305232
sum: 5.0 - mean: 0.009765625 - std: 0.005400517489761114
 * min 0.0027294191531836987, max: 0.035102225840091705
eps: tensor([0.1437, 0.2828, 0.0996, 0.1266, 0.2881, 0.3130, 0.3763, 0.4349, 0.4350],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.450, time 1.06
 * Lower 1 Val Acc 50.050, time 1.06
 * Upper 1 Val Acc 50.050, time 1.03
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 69.350, Loss 0.594
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.100, time 1.01
Epoch:1
LR: 0.001
 * Train Acc 78.170, Loss 0.464
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 0.82
Epoch:2
LR: 0.001
 * Train Acc 80.270, Loss 0.422
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.71
Epoch:3
LR: 0.001
 * Train Acc 81.390, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.000, time 0.77
Epoch:4
LR: 0.001
 * Train Acc 81.910, Loss 0.382
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 1.13
Epoch:5
LR: 0.001
 * Train Acc 82.570, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 1.00
Epoch:6
LR: 0.001
 * Train Acc 82.200, Loss 0.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.87
Epoch:7
LR: 0.001
 * Train Acc 83.080, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 1.12
Epoch:8
LR: 0.001
 * Train Acc 83.890, Loss 0.337
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.91
Epoch:9
LR: 0.001
 * Train Acc 83.750, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 1.16
Epoch:10
LR: 0.001
 * Train Acc 83.950, Loss 0.320
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.96
Epoch:11
LR: 0.001
 * Train Acc 84.260, Loss 0.313
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.100, time 0.96
Epoch:12
LR: 0.001
 * Train Acc 84.010, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 1.04
Epoch:13
LR: 0.001
 * Train Acc 84.700, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.93
Epoch:14
LR: 0.001
 * Train Acc 84.380, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.250, time 0.99
Epoch:15
LR: 0.001
 * Train Acc 84.750, Loss 0.283
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.350, time 1.09
Epoch:16
LR: 0.001
 * Train Acc 84.740, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.88
Epoch:17
LR: 0.001
 * Train Acc 84.940, Loss 0.269
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 1.12
Epoch:18
LR: 0.001
 * Train Acc 84.960, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.93
Epoch:19
LR: 0.001
 * Train Acc 84.460, Loss 0.268
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.96
Epoch:20
LR: 0.001
 * Train Acc 85.270, Loss 0.257
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.66
Epoch:21
LR: 0.001
 * Train Acc 84.630, Loss 0.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 0.88
Epoch:22
LR: 0.001
 * Train Acc 85.100, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 0.72
Epoch:23
LR: 0.001
 * Train Acc 85.200, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 1.03
Epoch:24
LR: 0.001
 * Train Acc 85.440, Loss 0.235
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.96
Epoch:25
LR: 0.001
 * Train Acc 85.590, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.91
Epoch:26
LR: 0.001
 * Train Acc 84.970, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.92
Epoch:27
LR: 0.001
 * Train Acc 85.160, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.99
Epoch:28
LR: 0.001
 * Train Acc 85.170, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.97
Epoch:29
LR: 0.001
 * Train Acc 85.150, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.98
Epoch:30
LR: 0.001
 * Train Acc 85.250, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.80
Epoch:31
LR: 0.001
 * Train Acc 84.760, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.76
Epoch:32
LR: 0.001
 * Train Acc 85.050, Loss 0.262
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.72
Epoch:33
LR: 0.001
 * Train Acc 85.100, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.96
Epoch:34
LR: 0.001
 * Train Acc 85.510, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.95
Epoch:35
LR: 0.001
 * Train Acc 84.920, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.98
Epoch:36
LR: 0.001
 * Train Acc 84.880, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.96
Epoch:37
LR: 0.001
 * Train Acc 85.000, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.92
Epoch:38
LR: 0.001
 * Train Acc 85.610, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.74
Epoch:39
LR: 0.001
 * Train Acc 84.590, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.91
Epoch:40
LR: 0.001
 * Train Acc 84.590, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.95
Epoch:41
LR: 0.001
 * Train Acc 85.090, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.98
Epoch:42
LR: 0.001
 * Train Acc 85.140, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.68
Epoch:43
LR: 0.001
 * Train Acc 84.380, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.81
Epoch:44
LR: 0.001
 * Train Acc 84.940, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.95
Epoch:45
LR: 0.001
 * Train Acc 84.250, Loss 0.178
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 0.96
Epoch:46
LR: 0.001
 * Train Acc 84.950, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.93
Epoch:47
LR: 0.001
 * Train Acc 84.440, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.95
Epoch:48
LR: 0.001
 * Train Acc 85.030, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.99
Epoch:49
LR: 0.001
 * Train Acc 84.320, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.66
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.549970626831055 - mean: 0.005266169551759958 - std: 0.0020372746512293816
 * min 0.003096967004239559, max: 0.010920663364231586
sum: 114.41217041015625 - mean: 0.012414515018463135 - std: 0.006068340037018061
 * min 0.004299220163375139, max: 0.0332624576985836
sum: 61.17792510986328 - mean: 0.003319114912301302 - std: 0.0011619925498962402
 * min 0.0011658204020932317, max: 0.007670185994356871
sum: 157.88400268554688 - mean: 0.004282877780497074 - std: 0.0011474054772406816
 * min 0.001515704789198935, max: 0.010137842036783695
sum: 773.0820922851562 - mean: 0.01048559695482254 - std: 0.0031954539008438587
 * min 0.003135285573080182, max: 0.02811039239168167
sum: 1840.9818115234375 - mean: 0.012484957464039326 - std: 0.0031996951438486576
 * min 0.004094085190445185, max: 0.03170311078429222
sum: 2227.5458984375 - mean: 0.015106512233614922 - std: 0.00331152998842299
 * min 0.004602929577231407, max: 0.03739731386303902
sum: 42.1983642578125 - mean: 5.151167351868935e-05 - std: 4.020066626253538e-07
 * min 4.086270200787112e-05, max: 5.364488606574014e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.00203392724506557
 * min 0.0009430728387087584, max: 0.013245271518826485
eps: tensor([0.0474, 0.1117, 0.0299, 0.0385, 0.0944, 0.1124, 0.1360, 0.1648, 0.1649],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.350, time 0.67
 * Lower 1 Val Acc 76.750, time 0.92
 * Upper 1 Val Acc 76.750, time 0.91
validation split name: 2
 *  Val Acc 83.850, time 0.92
 * Lower 1 Val Acc 70.100, time 1.01
 * Upper 1 Val Acc 70.100, time 0.93
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 80.180, Loss 0.440
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.95
Epoch:1
LR: 0.001
 * Train Acc 84.030, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.89
Epoch:2
LR: 0.001
 * Train Acc 85.380, Loss 0.340
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.85
Epoch:3
LR: 0.001
 * Train Acc 84.830, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.900, time 0.66
Epoch:4
LR: 0.001
 * Train Acc 85.620, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 1.02
Epoch:5
LR: 0.001
 * Train Acc 86.240, Loss 0.309
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 82.900, time 0.89
Epoch:6
LR: 0.001
 * Train Acc 85.990, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 1.01
Epoch:7
LR: 0.001
 * Train Acc 85.470, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 1.09
Epoch:8
LR: 0.001
 * Train Acc 85.580, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 1.03
Epoch:9
LR: 0.001
 * Train Acc 86.020, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 1.07
Epoch:10
LR: 0.001
 * Train Acc 85.990, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.92
Epoch:11
LR: 0.001
 * Train Acc 85.780, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 1.00
Epoch:12
LR: 0.001
 * Train Acc 85.920, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.76
Epoch:13
LR: 0.001
 * Train Acc 85.940, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.100, time 0.85
Epoch:14
LR: 0.001
 * Train Acc 85.920, Loss 0.274
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.150, time 0.90
Epoch:15
LR: 0.001
 * Train Acc 85.640, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.700, time 1.06
Epoch:16
LR: 0.001
 * Train Acc 86.150, Loss 0.266
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.96
Epoch:17
LR: 0.001
 * Train Acc 85.780, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 1.01
Epoch:18
LR: 0.001
 * Train Acc 85.770, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 1.13
Epoch:19
LR: 0.001
 * Train Acc 86.030, Loss 0.251
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.050, time 0.98
Epoch:20
LR: 0.001
 * Train Acc 85.930, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 0.98
Epoch:21
LR: 0.001
 * Train Acc 85.600, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.450, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 85.440, Loss 0.240
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.500, time 0.90
Epoch:23
LR: 0.001
 * Train Acc 85.560, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.150, time 1.11
Epoch:24
LR: 0.001
 * Train Acc 85.670, Loss 0.235
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.92
Epoch:25
LR: 0.001
 * Train Acc 85.950, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 0.99
Epoch:26
LR: 0.001
 * Train Acc 85.970, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.650, time 0.96
Epoch:27
LR: 0.001
 * Train Acc 85.780, Loss 0.226
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 1.03
Epoch:28
LR: 0.001
 * Train Acc 85.240, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 1.01
Epoch:29
LR: 0.001
 * Train Acc 85.330, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.91
Epoch:30
LR: 0.001
 * Train Acc 84.840, Loss 0.214
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.90
Epoch:31
LR: 0.001
 * Train Acc 85.320, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.99
Epoch:32
LR: 0.001
 * Train Acc 85.990, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.950, time 1.00
Epoch:33
LR: 0.001
 * Train Acc 85.240, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.97
Epoch:34
LR: 0.001
 * Train Acc 85.380, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 1.02
Epoch:35
LR: 0.001
 * Train Acc 84.900, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 1.01
Epoch:36
LR: 0.001
 * Train Acc 85.290, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.97
Epoch:37
LR: 0.001
 * Train Acc 85.000, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.95
Epoch:38
LR: 0.001
 * Train Acc 85.110, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.98
Epoch:39
LR: 0.001
 * Train Acc 84.980, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.91
Epoch:40
LR: 0.001
 * Train Acc 85.060, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.88
Epoch:41
LR: 0.001
 * Train Acc 84.760, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.550, time 0.85
Epoch:42
LR: 0.001
 * Train Acc 84.990, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.96
Epoch:43
LR: 0.001
 * Train Acc 84.910, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.97
Epoch:44
LR: 0.001
 * Train Acc 84.430, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.99
Epoch:45
LR: 0.001
 * Train Acc 84.940, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.92
Epoch:46
LR: 0.001
 * Train Acc 84.460, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.97
Epoch:47
LR: 0.001
 * Train Acc 84.230, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 1.00
Epoch:48
LR: 0.001
 * Train Acc 84.880, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.800, time 0.96
Epoch:49
LR: 0.001
 * Train Acc 84.520, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.89
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.4898405075073242 - mean: 0.0017243524780496955 - std: 0.0007335498812608421
 * min 0.0009478307329118252, max: 0.00379113364033401
sum: 37.713356018066406 - mean: 0.004092161078006029 - std: 0.0021584047935903072
 * min 0.0013071130961179733, max: 0.011808733455836773
sum: 18.766559600830078 - mean: 0.0010181510588154197 - std: 0.0003709293087013066
 * min 0.00033673757570795715, max: 0.0024986094795167446
sum: 48.71499252319336 - mean: 0.001321478746831417 - std: 0.00036084343446418643
 * min 0.00045986397890374064, max: 0.0032860792707651854
sum: 238.65023803710938 - mean: 0.0032369010150432587 - std: 0.001001675147563219
 * min 0.0009145408403128386, max: 0.009068290702998638
sum: 626.9874267578125 - mean: 0.00425203051418066 - std: 0.00110203109215945
 * min 0.0013789106160402298, max: 0.010815131478011608
sum: 766.0064086914062 - mean: 0.0051948134787380695 - std: 0.0011530857300385833
 * min 0.0015350761823356152, max: 0.013023079372942448
sum: 14.388856887817383 - mean: 1.7564521840540692e-05 - std: 1.3743972715474229e-07
 * min 1.3931886314821895e-05, max: 1.8292814274900593e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0006939801387488842
 * min 0.00031332948128692806, max: 0.00449797511100769
eps: tensor([0.0155, 0.0368, 0.0092, 0.0119, 0.0291, 0.0383, 0.0468, 0.0562, 0.0562],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 89.350, time 0.91
 * Lower 1 Val Acc 84.600, time 0.90
 * Upper 1 Val Acc 84.600, time 0.91
validation split name: 2
 *  Val Acc 77.950, time 0.90
 * Lower 1 Val Acc 77.150, time 0.91
 * Upper 1 Val Acc 77.150, time 0.91
validation split name: 3
 *  Val Acc 83.750, time 0.88
 * Lower 1 Val Acc 84.500, time 0.91
 * Upper 1 Val Acc 84.500, time 0.91
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.120, Loss 0.473
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 84.150, time 0.81
Epoch:1
LR: 0.001
 * Train Acc 80.660, Loss 0.427
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 81.660, Loss 0.409
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.88
Epoch:3
LR: 0.001
 * Train Acc 81.270, Loss 0.410
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.73
Epoch:4
LR: 0.001
 * Train Acc 80.970, Loss 0.409
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.65
Epoch:5
LR: 0.001
 * Train Acc 81.050, Loss 0.400
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.72
Epoch:6
LR: 0.001
 * Train Acc 80.930, Loss 0.394
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.73
Epoch:7
LR: 0.001
 * Train Acc 80.370, Loss 0.399
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.67
Epoch:8
LR: 0.001
 * Train Acc 80.180, Loss 0.391
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.67
Epoch:9
LR: 0.001
 * Train Acc 80.160, Loss 0.389
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.66
Epoch:10
LR: 0.001
 * Train Acc 80.870, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.82
Epoch:11
LR: 0.001
 * Train Acc 79.910, Loss 0.385
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.66
Epoch:12
LR: 0.001
 * Train Acc 79.400, Loss 0.384
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.66
Epoch:13
LR: 0.001
 * Train Acc 80.060, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.70
Epoch:14
LR: 0.001
 * Train Acc 79.280, Loss 0.380
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.93
Epoch:15
LR: 0.001
 * Train Acc 79.350, Loss 0.370
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.92
Epoch:16
LR: 0.001
 * Train Acc 79.040, Loss 0.365
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.91
Epoch:17
LR: 0.001
 * Train Acc 79.180, Loss 0.368
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.92
Epoch:18
LR: 0.001
 * Train Acc 78.230, Loss 0.363
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 1.00
Epoch:19
LR: 0.001
 * Train Acc 78.980, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.88
Epoch:20
LR: 0.001
 * Train Acc 77.770, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 0.92
Epoch:21
LR: 0.001
 * Train Acc 77.830, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 1.04
Epoch:22
LR: 0.001
 * Train Acc 77.430, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.90
Epoch:23
LR: 0.001
 * Train Acc 76.890, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.88
Epoch:24
LR: 0.001
 * Train Acc 76.880, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 1.06
Epoch:25
LR: 0.001
 * Train Acc 77.140, Loss 0.338
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.95
Epoch:26
LR: 0.001
 * Train Acc 76.810, Loss 0.333
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.92
Epoch:27
LR: 0.001
 * Train Acc 76.160, Loss 0.330
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 1.06
Epoch:28
LR: 0.001
 * Train Acc 76.080, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 1.01
Epoch:29
LR: 0.001
 * Train Acc 75.440, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.750, time 0.92
Epoch:30
LR: 0.001
 * Train Acc 75.720, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.99
Epoch:31
LR: 0.001
 * Train Acc 75.540, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.200, time 0.99
Epoch:32
LR: 0.001
 * Train Acc 75.190, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.950, time 0.99
Epoch:33
LR: 0.001
 * Train Acc 74.340, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.100, time 0.98
Epoch:34
LR: 0.001
 * Train Acc 74.660, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.050, time 0.95
Epoch:35
LR: 0.001
 * Train Acc 74.110, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.750, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 74.250, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.550, time 1.02
Epoch:37
LR: 0.001
 * Train Acc 73.440, Loss 0.290
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.050, time 0.71
Epoch:38
LR: 0.001
 * Train Acc 73.120, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.250, time 0.94
Epoch:39
LR: 0.001
 * Train Acc 72.180, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.250, time 0.94
Epoch:40
LR: 0.001
 * Train Acc 72.290, Loss 0.282
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.950, time 0.96
Epoch:41
LR: 0.001
 * Train Acc 71.640, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.93
Epoch:42
LR: 0.001
 * Train Acc 71.190, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.91
Epoch:43
LR: 0.001
 * Train Acc 70.800, Loss 0.289
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.050, time 0.75
Epoch:44
LR: 0.001
 * Train Acc 71.580, Loss 0.290
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.100, time 0.86
Epoch:45
LR: 0.001
 * Train Acc 70.290, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.150, time 0.93
Epoch:46
LR: 0.001
 * Train Acc 69.810, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.500, time 0.94
Epoch:47
LR: 0.001
 * Train Acc 69.830, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.150, time 0.97
Epoch:48
LR: 0.001
 * Train Acc 69.390, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.550, time 0.97
Epoch:49
LR: 0.001
 * Train Acc 69.200, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.800, time 0.91
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.006825566291809 - mean: 0.0011653073597699404 - std: 0.0005530734197236598
 * min 0.0005999417626298964, max: 0.0027063561137765646
sum: 24.59015464782715 - mean: 0.0026682026218622923 - std: 0.001511355978436768
 * min 0.0007912925211712718, max: 0.008126583881676197
sum: 11.82378101348877 - mean: 0.0006414811941795051 - std: 0.00024599157040938735
 * min 0.00019179005175828934, max: 0.0016429852694272995
sum: 30.517112731933594 - mean: 0.000827829644549638 - std: 0.00023117652744986117
 * min 0.00027286820113658905, max: 0.002136423485353589
sum: 148.23846435546875 - mean: 0.0020106129813939333 - std: 0.0006336912629194558
 * min 0.0005458766245283186, max: 0.005903868470340967
sum: 420.88214111328125 - mean: 0.0028542897198349237 - std: 0.0007501696236431599
 * min 0.000914997945073992, max: 0.007395076099783182
sum: 520.3841552734375 - mean: 0.0035290808882564306 - std: 0.0007959333015605807
 * min 0.0010131445014849305, max: 0.009033458307385445
sum: 9.818748474121094 - mean: 1.1985776836809237e-05 - std: 9.405053447153477e-08
 * min 9.507365575700533e-06, max: 1.2483598766266368e-05
sum: 0.40000003576278687 - mean: 0.0007812500698491931 - std: 0.00048468916793353856
 * min 0.00019718575640581548, max: 0.0031373349484056234
eps: tensor([0.0105, 0.0240, 0.0058, 0.0075, 0.0181, 0.0257, 0.0318, 0.0384, 0.0384],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.900, time 0.93
 * Lower 1 Val Acc 78.600, time 0.93
 * Upper 1 Val Acc 78.600, time 0.98
validation split name: 2
 *  Val Acc 74.700, time 0.98
 * Lower 1 Val Acc 74.350, time 0.97
 * Upper 1 Val Acc 74.350, time 0.89
validation split name: 3
 *  Val Acc 81.350, time 0.90
 * Lower 1 Val Acc 82.000, time 0.94
 * Upper 1 Val Acc 82.000, time 0.91
validation split name: 4
 *  Val Acc 72.800, time 0.93
 * Lower 1 Val Acc 72.150, time 0.92
 * Upper 1 Val Acc 72.150, time 0.89
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 82.120, Loss 0.401
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 1.03
Epoch:1
LR: 0.001
 * Train Acc 83.780, Loss 0.368
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.97
Epoch:2
LR: 0.001
 * Train Acc 83.610, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.95
Epoch:3
LR: 0.001
 * Train Acc 83.510, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 1.01
Epoch:4
LR: 0.001
 * Train Acc 83.780, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.95
Epoch:5
LR: 0.001
 * Train Acc 83.580, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.99
Epoch:6
LR: 0.001
 * Train Acc 83.600, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.97
Epoch:7
LR: 0.001
 * Train Acc 83.560, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.99
Epoch:8
LR: 0.001
 * Train Acc 82.890, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 1.01
Epoch:9
LR: 0.001
 * Train Acc 83.290, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.73
Epoch:10
LR: 0.001
 * Train Acc 83.280, Loss 0.336
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.79
Epoch:11
LR: 0.001
 * Train Acc 83.370, Loss 0.330
 * robust loss: 0.001 robust error: 0.00000000
 *  Val Acc 83.900, time 0.82
Epoch:12
LR: 0.001
 * Train Acc 83.100, Loss 0.325
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.71
Epoch:13
LR: 0.001
 * Train Acc 83.580, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.96
Epoch:14
LR: 0.001
 * Train Acc 83.140, Loss 0.315
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 1.06
Epoch:15
LR: 0.001
 * Train Acc 83.420, Loss 0.316
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 83.080, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.98
Epoch:17
LR: 0.001
 * Train Acc 83.120, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 1.07
Epoch:18
LR: 0.001
 * Train Acc 82.880, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 1.04
Epoch:19
LR: 0.001
 * Train Acc 82.750, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 1.08
Epoch:20
LR: 0.001
 * Train Acc 83.210, Loss 0.292
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 1.05
Epoch:21
LR: 0.001
 * Train Acc 82.790, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 82.930, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 1.09
Epoch:23
LR: 0.001
 * Train Acc 82.960, Loss 0.276
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.98
Epoch:24
LR: 0.001
 * Train Acc 82.270, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.98
Epoch:25
LR: 0.001
 * Train Acc 82.080, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 1.06
Epoch:26
LR: 0.001
 * Train Acc 82.280, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.95
Epoch:27
LR: 0.001
 * Train Acc 82.570, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.97
Epoch:28
LR: 0.001
 * Train Acc 82.150, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 1.04
Epoch:29
LR: 0.001
 * Train Acc 81.910, Loss 0.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.95
Epoch:30
LR: 0.001
 * Train Acc 82.170, Loss 0.251
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.94
Epoch:31
LR: 0.001
 * Train Acc 82.400, Loss 0.248
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.70
Epoch:32
LR: 0.001
 * Train Acc 82.090, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.98
Epoch:33
LR: 0.001
 * Train Acc 82.110, Loss 0.239
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.95
Epoch:34
LR: 0.001
 * Train Acc 82.170, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 1.02
Epoch:35
LR: 0.001
 * Train Acc 81.810, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.95
Epoch:36
LR: 0.001
 * Train Acc 81.560, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.95
Epoch:37
LR: 0.001
 * Train Acc 81.630, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.93
Epoch:38
LR: 0.001
 * Train Acc 81.740, Loss 0.214
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 1.00
Epoch:39
LR: 0.001
 * Train Acc 81.330, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.94
Epoch:40
LR: 0.001
 * Train Acc 81.240, Loss 0.210
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 1.01
Epoch:41
LR: 0.001
 * Train Acc 81.630, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 1.00
Epoch:42
LR: 0.001
 * Train Acc 81.120, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.70
Epoch:43
LR: 0.001
 * Train Acc 81.470, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.94
Epoch:44
LR: 0.001
 * Train Acc 80.890, Loss 0.216
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 1.00
Epoch:45
LR: 0.001
 * Train Acc 81.220, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.98
Epoch:46
LR: 0.001
 * Train Acc 81.240, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.96
Epoch:47
LR: 0.001
 * Train Acc 81.120, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 1.01
Epoch:48
LR: 0.001
 * Train Acc 80.930, Loss 0.214
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 1.01
Epoch:49
LR: 0.001
 * Train Acc 81.100, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.81
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5021095275878906 - mean: 0.0005811452865600586 - std: 0.00028580520302057266
 * min 0.00027785790734924376, max: 0.001384632894769311
sum: 12.195919036865234 - mean: 0.0013233419740572572 - std: 0.0007767542847432196
 * min 0.0003646001860033721, max: 0.004192305263131857
sum: 5.528158664703369 - mean: 0.00029992181225679815 - std: 0.00011779629130614921
 * min 8.767626422923058e-05, max: 0.0007682759314775467
sum: 13.825429916381836 - mean: 0.00037503879866562784 - std: 0.000105823710327968
 * min 0.00011873402399942279, max: 0.000975999457295984
sum: 68.10689544677734 - mean: 0.0009237588965333998 - std: 0.00029334676219150424
 * min 0.0002498167159501463, max: 0.0027176253497600555
sum: 207.80735778808594 - mean: 0.0014092838391661644 - std: 0.00037340293056331575
 * min 0.00044513578177429736, max: 0.003808424575254321
sum: 266.1374206542969 - mean: 0.0018048599595203996 - std: 0.0004119600635021925
 * min 0.0004993270849809051, max: 0.004654689691960812
sum: 5.060293197631836 - mean: 6.177115665195743e-06 - std: 4.8554820608615046e-08
 * min 4.89820422444609e-06, max: 6.433905582525767e-06
sum: 0.20000001788139343 - mean: 0.00039062503492459655 - std: 0.0002547138137742877
 * min 9.282978135161102e-05, max: 0.0016452803974971175
eps: tensor([0.0052, 0.0119, 0.0027, 0.0034, 0.0083, 0.0127, 0.0162, 0.0198, 0.0198],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.100, time 0.81
 * Lower 1 Val Acc 87.150, time 0.90
 * Upper 1 Val Acc 87.150, time 0.96
validation split name: 2
 *  Val Acc 74.850, time 0.97
 * Lower 1 Val Acc 75.000, time 0.91
 * Upper 1 Val Acc 75.000, time 0.97
validation split name: 3
 *  Val Acc 70.150, time 0.91
 * Lower 1 Val Acc 71.250, time 0.95
 * Upper 1 Val Acc 71.250, time 0.91
validation split name: 4
 *  Val Acc 68.450, time 0.91
 * Lower 1 Val Acc 68.500, time 0.88
 * Upper 1 Val Acc 68.500, time 0.89
validation split name: 5
 *  Val Acc 82.100, time 0.92
 * Lower 1 Val Acc 81.850, time 0.94
 * Upper 1 Val Acc 81.850, time 0.95
Task 1 average acc: 97.45
Task 2 average acc: 86.1
Task 3 average acc: 83.68333333333334
Task 4 average acc: 77.9375
Task 5 average acc: 76.72999999999999
===Summary of experiment repeats: 7 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64 73.59 77.93 76.73  0.    0.    0.  ]
mean: 53.104 std: 34.79507873248744
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 51.810, Loss 0.676
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.800, time 0.90
Epoch:1
LR: 0.001
 * Train Acc 74.210, Loss 0.527
 * robust loss: 0.672 robust error: 0.01000000
 *  Val Acc 79.400, time 0.72
Epoch:2
LR: 0.001
 * Train Acc 80.370, Loss 0.432
 * robust loss: 0.027 robust error: 0.01000000
 *  Val Acc 83.850, time 0.96
Epoch:3
LR: 0.001
 * Train Acc 83.490, Loss 0.365
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.100, time 0.97
Epoch:4
LR: 0.001
 * Train Acc 84.820, Loss 0.338
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.400, time 0.93
Epoch:5
LR: 0.001
 * Train Acc 86.220, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 0.89
Epoch:6
LR: 0.001
 * Train Acc 87.630, Loss 0.278
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.700, time 0.94
Epoch:7
LR: 0.001
 * Train Acc 88.150, Loss 0.272
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 89.450, time 0.90
Epoch:8
LR: 0.001
 * Train Acc 90.290, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 90.750, time 0.78
Epoch:9
LR: 0.001
 * Train Acc 90.620, Loss 0.202
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 92.550, time 0.73
Epoch:10
LR: 0.001
 * Train Acc 90.750, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 88.150, time 0.76
Epoch:11
LR: 0.001
 * Train Acc 91.890, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.050, time 1.03
Epoch:12
LR: 0.001
 * Train Acc 92.770, Loss 0.154
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.400, time 0.97
Epoch:13
LR: 0.001
 * Train Acc 93.930, Loss 0.139
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.550, time 1.10
Epoch:14
LR: 0.001
 * Train Acc 93.690, Loss 0.131
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.050, time 0.88
Epoch:15
LR: 0.001
 * Train Acc 94.310, Loss 0.127
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 94.000, time 0.77
Epoch:16
LR: 0.001
 * Train Acc 95.110, Loss 0.114
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.250, time 0.95
Epoch:17
LR: 0.001
 * Train Acc 95.200, Loss 0.106
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.050, time 0.94
Epoch:18
LR: 0.001
 * Train Acc 95.630, Loss 0.132
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.550, time 0.70
Epoch:19
LR: 0.001
 * Train Acc 95.800, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.94
Epoch:20
LR: 0.001
 * Train Acc 96.060, Loss 0.079
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 94.750, time 1.04
Epoch:21
LR: 0.001
 * Train Acc 95.670, Loss 0.115
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.650, time 0.93
Epoch:22
LR: 0.001
 * Train Acc 96.390, Loss 1.718
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.800, time 1.09
Epoch:23
LR: 0.001
 * Train Acc 96.270, Loss 0.071
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.200, time 1.00
Epoch:24
LR: 0.001
 * Train Acc 96.150, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 1.04
Epoch:25
LR: 0.001
 * Train Acc 96.570, Loss 0.068
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.400, time 1.03
Epoch:26
LR: 0.001
 * Train Acc 96.950, Loss 0.054
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.250, time 0.90
Epoch:27
LR: 0.001
 * Train Acc 96.950, Loss 0.052
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.400, time 0.98
Epoch:28
LR: 0.001
 * Train Acc 97.320, Loss 0.050
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.150, time 0.98
Epoch:29
LR: 0.001
 * Train Acc 97.050, Loss 0.051
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.69
Epoch:30
LR: 0.001
 * Train Acc 97.440, Loss 0.043
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 0.91
Epoch:31
LR: 0.001
 * Train Acc 97.180, Loss 0.053
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 97.380, Loss 50.535
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 97.420, Loss 2.936
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.900, time 0.92
Epoch:34
LR: 0.001
 * Train Acc 97.500, Loss 0.078
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.750, time 0.99
Epoch:35
LR: 0.001
 * Train Acc 97.430, Loss 0.036
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 97.500, Loss 0.040
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.95
Epoch:37
LR: 0.001
 * Train Acc 97.950, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 0.96
Epoch:38
LR: 0.001
 * Train Acc 98.090, Loss 0.029
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.99
Epoch:39
LR: 0.001
 * Train Acc 98.060, Loss 0.029
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.67
Epoch:40
LR: 0.001
 * Train Acc 97.870, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.92
Epoch:41
LR: 0.001
 * Train Acc 98.030, Loss 0.026
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.150, time 0.98
Epoch:42
LR: 0.001
 * Train Acc 98.100, Loss 0.028
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.97
Epoch:43
LR: 0.001
 * Train Acc 98.140, Loss 0.025
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 0.96
Epoch:44
LR: 0.001
 * Train Acc 98.280, Loss 9.164
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.850, time 0.98
Epoch:45
LR: 0.001
 * Train Acc 98.020, Loss 0.195
 * robust loss: 0.083 robust error: 0.00000000
 *  Val Acc 94.800, time 0.95
Epoch:46
LR: 0.001
 * Train Acc 98.060, Loss 0.166
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.600, time 0.66
Epoch:47
LR: 0.001
 * Train Acc 97.980, Loss 0.094
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 0.96
Epoch:48
LR: 0.001
 * Train Acc 98.070, Loss 0.026
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 1.02
Epoch:49
LR: 0.001
 * Train Acc 98.160, Loss 0.084
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.050, time 1.03
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.872754096984863 - mean: 0.016056427732110023 - std: 0.004922658205032349
 * min 0.010147306136786938, max: 0.029552888125181198
sum: 338.02349853515625 - mean: 0.03667789697647095 - std: 0.01596377231180668
 * min 0.01361639704555273, max: 0.08560923486948013
sum: 215.83010864257812 - mean: 0.01170953270047903 - std: 0.00417771702632308
 * min 0.004268542397767305, max: 0.025008216500282288
sum: 531.998046875 - mean: 0.014431370422244072 - std: 0.0037300598341971636
 * min 0.0054382821545004845, max: 0.03121192939579487
sum: 2159.8212890625 - mean: 0.02929445169866085 - std: 0.008021442219614983
 * min 0.010520976036787033, max: 0.0680706575512886
sum: 4688.2998046875 - mean: 0.03179456666111946 - std: 0.004253082908689976
 * min 0.011305799707770348, max: 0.07885691523551941
sum: 5862.87451171875 - mean: 0.03976016119122505 - std: 0.004195238463580608
 * min 0.012497412972152233, max: 0.1098543182015419
sum: 112.93740844726562 - mean: 0.00013786304043605924 - std: 8.395043096243171e-07
 * min 0.00010818502778420225, max: 0.00014430317969527096
sum: 5.0 - mean: 0.009765625 - std: 0.0037757637910544872
 * min 0.0027156067080795765, max: 0.05885973200201988
eps: tensor([0.1445, 0.3301, 0.1054, 0.1299, 0.2637, 0.2862, 0.3578, 0.4412, 0.4413],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.050, time 0.96
 * Lower 1 Val Acc 49.150, time 0.95
 * Upper 1 Val Acc 49.150, time 1.03
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 70.920, Loss 0.620
 * robust loss: 0.050 robust error: 0.01000000
 *  Val Acc 77.550, time 1.00
Epoch:1
LR: 0.001
 * Train Acc 78.820, Loss 0.451
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 1.04
Epoch:2
LR: 0.001
 * Train Acc 80.100, Loss 0.427
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.97
Epoch:3
LR: 0.001
 * Train Acc 80.510, Loss 0.411
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 0.96
Epoch:4
LR: 0.001
 * Train Acc 81.550, Loss 0.397
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.97
Epoch:5
LR: 0.001
 * Train Acc 82.190, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.92
Epoch:6
LR: 0.001
 * Train Acc 82.200, Loss 0.369
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.050, time 0.79
Epoch:7
LR: 0.001
 * Train Acc 82.300, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.94
Epoch:8
LR: 0.001
 * Train Acc 82.220, Loss 0.355
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.94
Epoch:9
LR: 0.001
 * Train Acc 82.980, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.92
Epoch:10
LR: 0.001
 * Train Acc 82.610, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.91
Epoch:11
LR: 0.001
 * Train Acc 83.230, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.93
Epoch:12
LR: 0.001
 * Train Acc 83.530, Loss 0.320
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 0.82
Epoch:13
LR: 0.001
 * Train Acc 83.330, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.85
Epoch:14
LR: 0.001
 * Train Acc 83.550, Loss 0.322
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 82.800, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 83.160, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.500, time 0.83
Epoch:16
LR: 0.001
 * Train Acc 83.070, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 1.02
Epoch:17
LR: 0.001
 * Train Acc 83.480, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.91
Epoch:18
LR: 0.001
 * Train Acc 83.770, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 1.05
Epoch:19
LR: 0.001
 * Train Acc 83.650, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.90
Epoch:20
LR: 0.001
 * Train Acc 83.830, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.91
Epoch:21
LR: 0.001
 * Train Acc 84.100, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 1.04
Epoch:22
LR: 0.001
 * Train Acc 83.970, Loss 0.269
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.93
Epoch:23
LR: 0.001
 * Train Acc 84.350, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.92
Epoch:24
LR: 0.001
 * Train Acc 83.960, Loss 0.257
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 1.05
Epoch:25
LR: 0.001
 * Train Acc 83.750, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 0.92
Epoch:26
LR: 0.001
 * Train Acc 83.480, Loss 0.251
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.93
Epoch:27
LR: 0.001
 * Train Acc 83.490, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 1.03
Epoch:28
LR: 0.001
 * Train Acc 83.520, Loss 0.240
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.92
Epoch:29
LR: 0.001
 * Train Acc 84.310, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.73
Epoch:30
LR: 0.001
 * Train Acc 84.090, Loss 0.228
 * robust loss: 0.035 robust error: 0.01000000
 *  Val Acc 82.650, time 0.93
Epoch:31
LR: 0.001
 * Train Acc 83.210, Loss 0.531
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.94
Epoch:32
LR: 0.001
 * Train Acc 84.000, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.67
Epoch:33
LR: 0.001
 * Train Acc 83.860, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.67
Epoch:34
LR: 0.001
 * Train Acc 83.450, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.93
Epoch:35
LR: 0.001
 * Train Acc 83.990, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.96
Epoch:36
LR: 0.001
 * Train Acc 83.770, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 83.520, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.90
Epoch:38
LR: 0.001
 * Train Acc 83.820, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.96
Epoch:39
LR: 0.001
 * Train Acc 83.950, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.72
Epoch:40
LR: 0.001
 * Train Acc 82.800, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.92
Epoch:41
LR: 0.001
 * Train Acc 83.600, Loss 0.189
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.92
Epoch:42
LR: 0.001
 * Train Acc 83.310, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.96
Epoch:43
LR: 0.001
 * Train Acc 83.690, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.94
Epoch:44
LR: 0.001
 * Train Acc 83.010, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.94
Epoch:45
LR: 0.001
 * Train Acc 83.090, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.94
Epoch:46
LR: 0.001
 * Train Acc 83.330, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.92
Epoch:47
LR: 0.001
 * Train Acc 83.000, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.94
Epoch:48
LR: 0.001
 * Train Acc 83.500, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.95
Epoch:49
LR: 0.001
 * Train Acc 83.280, Loss 0.190
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.94
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.594577312469482 - mean: 0.005317797884345055 - std: 0.0022514539305120707
 * min 0.0027712432201951742, max: 0.012054463848471642
sum: 132.1126251220703 - mean: 0.01433513779193163 - std: 0.007784159388393164
 * min 0.004060049075633287, max: 0.04011720418930054
sum: 59.61460876464844 - mean: 0.003234299598261714 - std: 0.0013467572862282395
 * min 0.000943128252401948, max: 0.008250853978097439
sum: 144.32766723632812 - mean: 0.00391513854265213 - std: 0.0010941973887383938
 * min 0.0013372403336688876, max: 0.0091105867177248
sum: 662.6825561523438 - mean: 0.008988207206130028 - std: 0.0027074944227933884
 * min 0.0026791798882186413, max: 0.025308510288596153
sum: 1712.2099609375 - mean: 0.011611667461693287 - std: 0.00171838013920933
 * min 0.003479568986222148, max: 0.03262795880436897
sum: 2181.158935546875 - mean: 0.01479193102568388 - std: 0.0017277909209951758
 * min 0.004174438305199146, max: 0.04289046674966812
sum: 43.542449951171875 - mean: 5.315240196068771e-05 - std: 3.443235527811339e-07
 * min 4.1187206079484895e-05, max: 5.586486804531887e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0014866647543385625
 * min 0.0009267730638384819, max: 0.02312631532549858
eps: tensor([0.0479, 0.1290, 0.0291, 0.0352, 0.0809, 0.1045, 0.1331, 0.1701, 0.1702],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 82.200, time 0.95
 * Lower 1 Val Acc 74.800, time 0.97
 * Upper 1 Val Acc 74.800, time 0.94
validation split name: 2
 *  Val Acc 79.250, time 1.02
 * Lower 1 Val Acc 69.400, time 0.87
 * Upper 1 Val Acc 69.400, time 0.89
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 78.780, Loss 0.467
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.89
Epoch:1
LR: 0.001
 * Train Acc 81.490, Loss 0.416
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 82.090, Loss 0.396
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.050, time 1.01
Epoch:3
LR: 0.001
 * Train Acc 82.770, Loss 0.379
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.97
Epoch:4
LR: 0.001
 * Train Acc 82.430, Loss 0.384
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.300, time 0.99
Epoch:5
LR: 0.001
 * Train Acc 82.250, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.86
Epoch:6
LR: 0.001
 * Train Acc 82.360, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.95
Epoch:7
LR: 0.001
 * Train Acc 82.930, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 0.94
Epoch:8
LR: 0.001
 * Train Acc 82.400, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.94
Epoch:9
LR: 0.001
 * Train Acc 83.330, Loss 0.343
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.92
Epoch:10
LR: 0.001
 * Train Acc 82.700, Loss 0.348
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.77
Epoch:11
LR: 0.001
 * Train Acc 83.270, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.83
Epoch:12
LR: 0.001
 * Train Acc 82.760, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.69
Epoch:13
LR: 0.001
 * Train Acc 82.790, Loss 0.325
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.63
Epoch:14
LR: 0.001
 * Train Acc 83.040, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.67
Epoch:15
LR: 0.001
 * Train Acc 82.380, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.67
Epoch:16
LR: 0.001
 * Train Acc 82.490, Loss 0.317
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.94
Epoch:17
LR: 0.001
 * Train Acc 82.580, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.94
Epoch:18
LR: 0.001
 * Train Acc 82.790, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.94
Epoch:19
LR: 0.001
 * Train Acc 82.370, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.950, time 0.98
Epoch:20
LR: 0.001
 * Train Acc 82.570, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.96
Epoch:21
LR: 0.001
 * Train Acc 83.220, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.91
Epoch:22
LR: 0.001
 * Train Acc 82.690, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.94
Epoch:23
LR: 0.001
 * Train Acc 82.660, Loss 0.282
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.94
Epoch:24
LR: 0.001
 * Train Acc 82.100, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 1.12
Epoch:25
LR: 0.001
 * Train Acc 82.820, Loss 0.269
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.93
Epoch:26
LR: 0.001
 * Train Acc 82.430, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.92
Epoch:27
LR: 0.001
 * Train Acc 82.220, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 1.10
Epoch:28
LR: 0.001
 * Train Acc 82.550, Loss 0.258
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 1.03
Epoch:29
LR: 0.001
 * Train Acc 82.450, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 1.03
Epoch:30
LR: 0.001
 * Train Acc 81.870, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 1.09
Epoch:31
LR: 0.001
 * Train Acc 82.020, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 82.420, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.93
Epoch:33
LR: 0.001
 * Train Acc 82.240, Loss 0.235
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.95
Epoch:34
LR: 0.001
 * Train Acc 82.370, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.93
Epoch:35
LR: 0.001
 * Train Acc 82.100, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.96
Epoch:36
LR: 0.001
 * Train Acc 82.200, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.90
Epoch:37
LR: 0.001
 * Train Acc 82.070, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.93
Epoch:38
LR: 0.001
 * Train Acc 82.330, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.92
Epoch:39
LR: 0.001
 * Train Acc 82.160, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.94
Epoch:40
LR: 0.001
 * Train Acc 81.530, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.700, time 0.97
Epoch:41
LR: 0.001
 * Train Acc 81.660, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.92
Epoch:42
LR: 0.001
 * Train Acc 81.450, Loss 0.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 0.91
Epoch:43
LR: 0.001
 * Train Acc 81.230, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.90
Epoch:44
LR: 0.001
 * Train Acc 81.630, Loss 0.205
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.81
Epoch:45
LR: 0.001
 * Train Acc 82.090, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.98
Epoch:46
LR: 0.001
 * Train Acc 81.740, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.600, time 0.96
Epoch:47
LR: 0.001
 * Train Acc 81.820, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.350, time 0.98
Epoch:48
LR: 0.001
 * Train Acc 81.500, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.050, time 0.96
Epoch:49
LR: 0.001
 * Train Acc 81.740, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.750, time 0.72
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.518143653869629 - mean: 0.0017571107018738985 - std: 0.0009025477338582277
 * min 0.0007751430384814739, max: 0.0045578195713460445
sum: 42.545021057128906 - mean: 0.0046164304949343204 - std: 0.0028967622201889753
 * min 0.0010721770813688636, max: 0.015503546223044395
sum: 16.33307456970215 - mean: 0.0008861260139383376 - std: 0.00041183552821166813
 * min 0.00021000423294026405, max: 0.002628423972055316
sum: 40.227115631103516 - mean: 0.0010912303114309907 - std: 0.0003193526354152709
 * min 0.00032967544393613935, max: 0.0028717757668346167
sum: 188.10147094726562 - mean: 0.0025512895081192255 - std: 0.0008020397508516908
 * min 0.0007049672421999276, max: 0.007876989431679249
sum: 578.2560424804688 - mean: 0.003921549767255783 - std: 0.0006086251814849675
 * min 0.0010359878651797771, max: 0.012384510599076748
sum: 756.975830078125 - mean: 0.005133571103215218 - std: 0.0006351621123030782
 * min 0.0012771106557920575, max: 0.016439227387309074
sum: 15.405181884765625 - mean: 1.8805152649292722e-05 - std: 1.2327203080531035e-07
 * min 1.4504557839245535e-05, max: 1.977994543267414e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0005556109244935215
 * min 0.000283018802292645, max: 0.008619915693998337
eps: tensor([0.0158, 0.0415, 0.0080, 0.0098, 0.0230, 0.0353, 0.0462, 0.0602, 0.0602],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 83.150, time 0.70
 * Lower 1 Val Acc 80.750, time 0.98
 * Upper 1 Val Acc 80.750, time 0.89
validation split name: 2
 *  Val Acc 77.000, time 0.92
 * Lower 1 Val Acc 78.350, time 0.93
 * Upper 1 Val Acc 78.350, time 0.88
validation split name: 3
 *  Val Acc 77.750, time 0.92
 * Lower 1 Val Acc 79.200, time 0.92
 * Upper 1 Val Acc 79.200, time 0.93
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 67.030, Loss 0.615
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.200, time 0.86
Epoch:1
LR: 0.001
 * Train Acc 68.640, Loss 0.581
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.200, time 0.95
Epoch:2
LR: 0.001
 * Train Acc 69.150, Loss 0.569
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.150, time 0.89
Epoch:3
LR: 0.001
 * Train Acc 68.600, Loss 0.566
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.650, time 0.96
Epoch:4
LR: 0.001
 * Train Acc 68.440, Loss 0.561
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.700, time 0.93
Epoch:5
LR: 0.001
 * Train Acc 68.650, Loss 0.556
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.350, time 0.90
Epoch:6
LR: 0.001
 * Train Acc 68.970, Loss 0.546
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.250, time 0.80
Epoch:7
LR: 0.001
 * Train Acc 68.430, Loss 0.542
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.500, time 0.74
Epoch:8
LR: 0.001
 * Train Acc 68.750, Loss 0.537
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.250, time 0.94
Epoch:9
LR: 0.001
 * Train Acc 68.130, Loss 0.531
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.550, time 0.93
Epoch:10
LR: 0.001
 * Train Acc 67.940, Loss 0.524
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.650, time 0.87
Epoch:11
LR: 0.001
 * Train Acc 67.460, Loss 0.525
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.600, time 0.63
Epoch:12
LR: 0.001
 * Train Acc 67.210, Loss 0.514
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.800, time 0.80
Epoch:13
LR: 0.001
 * Train Acc 67.450, Loss 0.503
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.650, time 1.10
Epoch:14
LR: 0.001
 * Train Acc 67.130, Loss 0.504
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.550, time 0.89
Epoch:15
LR: 0.001
 * Train Acc 67.230, Loss 0.490
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.300, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 67.230, Loss 0.488
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.500, time 1.03
Epoch:17
LR: 0.001
 * Train Acc 66.700, Loss 0.482
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.850, time 0.92
Epoch:18
LR: 0.001
 * Train Acc 66.780, Loss 0.473
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.950, time 1.07
Epoch:19
LR: 0.001
 * Train Acc 66.930, Loss 0.472
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.300, time 0.72
Epoch:20
LR: 0.001
 * Train Acc 66.700, Loss 0.464
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.850, time 0.98
Epoch:21
LR: 0.001
 * Train Acc 66.510, Loss 0.458
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.650, time 1.06
Epoch:22
LR: 0.001
 * Train Acc 65.710, Loss 0.455
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.300, time 0.96
Epoch:23
LR: 0.001
 * Train Acc 65.160, Loss 0.448
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.400, time 0.96
Epoch:24
LR: 0.001
 * Train Acc 65.600, Loss 0.439
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.700, time 1.07
Epoch:25
LR: 0.001
 * Train Acc 65.220, Loss 0.436
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.050, time 0.91
Epoch:26
LR: 0.001
 * Train Acc 65.090, Loss 0.431
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 68.300, time 0.97
Epoch:27
LR: 0.001
 * Train Acc 64.790, Loss 0.422
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.350, time 1.03
Epoch:28
LR: 0.001
 * Train Acc 64.650, Loss 0.419
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 67.250, time 0.91
Epoch:29
LR: 0.001
 * Train Acc 63.900, Loss 0.413
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.800, time 0.93
Epoch:30
LR: 0.001
 * Train Acc 62.990, Loss 0.409
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.550, time 0.95
Epoch:31
LR: 0.001
 * Train Acc 63.930, Loss 0.400
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.800, time 1.00
Epoch:32
LR: 0.001
 * Train Acc 63.730, Loss 0.392
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.700, time 0.91
Epoch:33
LR: 0.001
 * Train Acc 63.170, Loss 0.384
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.200, time 0.93
Epoch:34
LR: 0.001
 * Train Acc 62.990, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.100, time 0.94
Epoch:35
LR: 0.001
 * Train Acc 62.900, Loss 0.375
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 65.600, time 0.99
Epoch:36
LR: 0.001
 * Train Acc 62.170, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 66.050, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 62.160, Loss 0.361
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.950, time 0.96
Epoch:38
LR: 0.001
 * Train Acc 61.830, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.700, time 1.05
Epoch:39
LR: 0.001
 * Train Acc 61.150, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.850, time 0.94
Epoch:40
LR: 0.001
 * Train Acc 61.520, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.400, time 0.88
Epoch:41
LR: 0.001
 * Train Acc 60.960, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.250, time 0.99
Epoch:42
LR: 0.001
 * Train Acc 61.220, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.650, time 0.97
Epoch:43
LR: 0.001
 * Train Acc 61.020, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 64.050, time 0.96
Epoch:44
LR: 0.001
 * Train Acc 60.440, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.350, time 1.02
Epoch:45
LR: 0.001
 * Train Acc 60.100, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.300, time 0.99
Epoch:46
LR: 0.001
 * Train Acc 60.570, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 63.250, time 1.00
Epoch:47
LR: 0.001
 * Train Acc 60.090, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.450, time 0.75
Epoch:48
LR: 0.001
 * Train Acc 59.630, Loss 0.359
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.450, time 0.95
Epoch:49
LR: 0.001
 * Train Acc 59.620, Loss 0.361
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 62.200, time 0.90
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.000146746635437 - mean: 0.00115757726598531 - std: 0.0006219887873157859
 * min 0.0004909061244688928, max: 0.003128030337393284
sum: 28.628427505493164 - mean: 0.003106383141130209 - std: 0.0020103538408875465
 * min 0.0006949712987989187, max: 0.0107638044282794
sum: 10.40955924987793 - mean: 0.000564754765946418 - std: 0.0002679830649867654
 * min 0.00012767895532306284, max: 0.001712392084300518
sum: 25.432594299316406 - mean: 0.0006899032741785049 - std: 0.00020361710630822927
 * min 0.00020219016005285084, max: 0.0018551608081907034
sum: 118.17208099365234 - mean: 0.0016028113896027207 - std: 0.0005079254042357206
 * min 0.0004362535837572068, max: 0.00513914879411459
sum: 382.9690856933594 - mean: 0.002597175305709243 - std: 0.0004068185808137059
 * min 0.0006692332099191844, max: 0.008351732976734638
sum: 506.10003662109375 - mean: 0.00343221053481102 - std: 0.0004309554351493716
 * min 0.0008237209985964, max: 0.01144568994641304
sum: 10.447315216064453 - mean: 1.2753070222970564e-05 - std: 8.366981063545609e-08
 * min 9.835081982600968e-06, max: 1.3414631212071981e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 0.0003848385822493583
 * min 0.00018373303464613855, max: 0.005948397796601057
eps: tensor([0.0104, 0.0280, 0.0051, 0.0062, 0.0144, 0.0234, 0.0309, 0.0408, 0.0408],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 79.750, time 0.90
 * Lower 1 Val Acc 80.150, time 0.90
 * Upper 1 Val Acc 80.150, time 0.90
validation split name: 2
 *  Val Acc 75.600, time 0.96
 * Lower 1 Val Acc 77.200, time 0.93
 * Upper 1 Val Acc 77.200, time 0.97
validation split name: 3
 *  Val Acc 78.150, time 0.89
 * Lower 1 Val Acc 78.500, time 0.93
 * Upper 1 Val Acc 78.500, time 0.96
validation split name: 4
 *  Val Acc 62.200, time 0.89
 * Lower 1 Val Acc 61.800, time 0.90
 * Upper 1 Val Acc 61.800, time 0.93
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 81.360, Loss 0.423
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 0.69
Epoch:1
LR: 0.001
 * Train Acc 82.170, Loss 0.394
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.92
Epoch:2
LR: 0.001
 * Train Acc 82.510, Loss 0.386
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 81.790, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.97
Epoch:4
LR: 0.001
 * Train Acc 81.780, Loss 0.384
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.91
Epoch:5
LR: 0.001
 * Train Acc 82.040, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 1.00
Epoch:6
LR: 0.001
 * Train Acc 81.650, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.96
Epoch:7
LR: 0.001
 * Train Acc 82.510, Loss 0.363
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.92
Epoch:8
LR: 0.001
 * Train Acc 82.200, Loss 0.360
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.71
Epoch:9
LR: 0.001
 * Train Acc 81.690, Loss 0.361
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.65
Epoch:10
LR: 0.001
 * Train Acc 81.980, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 0.88
Epoch:11
LR: 0.001
 * Train Acc 81.850, Loss 0.355
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 0.94
Epoch:12
LR: 0.001
 * Train Acc 81.610, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.98
Epoch:13
LR: 0.001
 * Train Acc 81.580, Loss 0.342
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.83
Epoch:14
LR: 0.001
 * Train Acc 81.590, Loss 0.338
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.86
Epoch:15
LR: 0.001
 * Train Acc 81.280, Loss 0.333
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.96
Epoch:16
LR: 0.001
 * Train Acc 81.620, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 1.08
Epoch:17
LR: 0.001
 * Train Acc 81.760, Loss 0.320
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.93
Epoch:18
LR: 0.001
 * Train Acc 81.190, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.000, time 0.95
Epoch:19
LR: 0.001
 * Train Acc 81.430, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 1.05
Epoch:20
LR: 0.001
 * Train Acc 81.470, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.94
Epoch:21
LR: 0.001
 * Train Acc 81.870, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.95
Epoch:22
LR: 0.001
 * Train Acc 81.230, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 1.07
Epoch:23
LR: 0.001
 * Train Acc 81.170, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.95
Epoch:24
LR: 0.001
 * Train Acc 80.710, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 1.10
Epoch:25
LR: 0.001
 * Train Acc 80.770, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 0.90
Epoch:26
LR: 0.001
 * Train Acc 80.900, Loss 0.282
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.91
Epoch:27
LR: 0.001
 * Train Acc 81.260, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.76
Epoch:28
LR: 0.001
 * Train Acc 80.790, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.92
Epoch:29
LR: 0.001
 * Train Acc 80.600, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 1.03
Epoch:30
LR: 0.001
 * Train Acc 80.680, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 0.79
Epoch:31
LR: 0.001
 * Train Acc 80.710, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.96
Epoch:32
LR: 0.001
 * Train Acc 80.410, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.94
Epoch:33
LR: 0.001
 * Train Acc 80.260, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.96
Epoch:34
LR: 0.001
 * Train Acc 80.120, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.92
Epoch:35
LR: 0.001
 * Train Acc 79.960, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.83
Epoch:36
LR: 0.001
 * Train Acc 80.250, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.96
Epoch:37
LR: 0.001
 * Train Acc 79.500, Loss 0.232
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.97
Epoch:38
LR: 0.001
 * Train Acc 80.290, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.95
Epoch:39
LR: 0.001
 * Train Acc 79.990, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.90
Epoch:40
LR: 0.001
 * Train Acc 80.320, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.89
Epoch:41
LR: 0.001
 * Train Acc 79.450, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.67
Epoch:42
LR: 0.001
 * Train Acc 79.820, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.91
Epoch:43
LR: 0.001
 * Train Acc 79.820, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.900, time 0.89
Epoch:44
LR: 0.001
 * Train Acc 79.450, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.95
Epoch:45
LR: 0.001
 * Train Acc 79.560, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 0.94
Epoch:46
LR: 0.001
 * Train Acc 79.630, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.80
Epoch:47
LR: 0.001
 * Train Acc 79.080, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.98
Epoch:48
LR: 0.001
 * Train Acc 79.010, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.89
Epoch:49
LR: 0.001
 * Train Acc 79.470, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.88
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5457873344421387 - mean: 0.0006316982908174396 - std: 0.0003506831999402493
 * min 0.0002637180150486529, max: 0.00174690259154886
sum: 15.123509407043457 - mean: 0.0016410058597102761 - std: 0.0010903208749368787
 * min 0.0003541748155839741, max: 0.0057922787964344025
sum: 5.500025272369385 - mean: 0.00029839546186849475 - std: 0.0001429101830581203
 * min 6.715223571518436e-05, max: 0.0009112691623158753
sum: 13.256634712219238 - mean: 0.0003596092283260077 - std: 0.00010634279897203669
 * min 0.00010512641165405512, max: 0.0009700326481834054
sum: 60.409568786621094 - mean: 0.0008193572284653783 - std: 0.00025988282868638635
 * min 0.00022264537983573973, max: 0.002631255192682147
sum: 185.19131469726562 - mean: 0.0012559089809656143 - std: 0.00019679285469464958
 * min 0.00032348616514354944, max: 0.004046919289976358
sum: 245.8468017578125 - mean: 0.0016672553028911352 - std: 0.00020944973221048713
 * min 0.0003993964346591383, max: 0.005572364665567875
sum: 5.110876083374023 - mean: 6.2388621699938085e-06 - std: 4.0932018663397685e-08
 * min 4.811368398804916e-06, max: 6.562504950125003e-06
sum: 0.20000000298023224 - mean: 0.0003906250058207661 - std: 0.00019206512661185116
 * min 9.194672747980803e-05, max: 0.0029692056123167276
eps: tensor([0.0057, 0.0148, 0.0027, 0.0032, 0.0074, 0.0113, 0.0150, 0.0200, 0.0200],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 88.250, time 0.91
 * Lower 1 Val Acc 87.600, time 0.92
 * Upper 1 Val Acc 87.600, time 0.92
validation split name: 2
 *  Val Acc 77.300, time 0.91
 * Lower 1 Val Acc 77.350, time 0.96
 * Upper 1 Val Acc 77.350, time 0.66
validation split name: 3
 *  Val Acc 72.250, time 0.97
 * Lower 1 Val Acc 72.200, time 0.95
 * Upper 1 Val Acc 72.200, time 0.98
validation split name: 4
 *  Val Acc 58.700, time 0.97
 * Lower 1 Val Acc 59.150, time 0.91
 * Upper 1 Val Acc 59.150, time 0.95
validation split name: 5
 *  Val Acc 80.500, time 0.93
 * Lower 1 Val Acc 80.600, time 0.94
 * Upper 1 Val Acc 80.600, time 0.94
Task 1 average acc: 98.05
Task 2 average acc: 80.725
Task 3 average acc: 79.3
Task 4 average acc: 73.925
Task 5 average acc: 75.4
===Summary of experiment repeats: 8 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64 73.59 77.93 76.73 75.4   0.    0.  ]
mean: 60.64399999999999 std: 30.357101047366168
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 68.550, Loss 0.580
 * robust loss: 0.009 robust error: 0.00000000
 *  Val Acc 80.700, time 0.93
Epoch:1
LR: 0.001
 * Train Acc 80.330, Loss 0.424
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.750, time 0.89
Epoch:2
LR: 0.001
 * Train Acc 84.640, Loss 0.344
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 88.800, time 0.92
Epoch:3
LR: 0.001
 * Train Acc 87.520, Loss 0.290
 * robust loss: 0.033 robust error: 0.01000000
 *  Val Acc 86.300, time 0.91
Epoch:4
LR: 0.001
 * Train Acc 88.750, Loss 0.252
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 92.050, time 0.99
Epoch:5
LR: 0.001
 * Train Acc 89.840, Loss 0.296
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 90.150, time 0.73
Epoch:6
LR: 0.001
 * Train Acc 91.550, Loss 0.202
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.700, time 1.07
Epoch:7
LR: 0.001
 * Train Acc 93.010, Loss 0.160
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.050, time 0.85
Epoch:8
LR: 0.001
 * Train Acc 93.870, Loss 0.155
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.500, time 0.83
Epoch:9
LR: 0.001
 * Train Acc 94.510, Loss 0.134
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 95.800, time 0.62
Epoch:10
LR: 0.001
 * Train Acc 94.580, Loss 0.128
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.71
Epoch:11
LR: 0.001
 * Train Acc 95.410, Loss 0.170
 * robust loss: 5.208 robust error: 0.01000000
 *  Val Acc 95.000, time 0.88
Epoch:12
LR: 0.001
 * Train Acc 95.520, Loss 0.104
 * robust loss: 0.431 robust error: 0.01000000
 *  Val Acc 95.700, time 0.91
Epoch:13
LR: 0.001
 * Train Acc 96.010, Loss 0.134
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.900, time 0.72
Epoch:14
LR: 0.001
 * Train Acc 96.210, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.100, time 0.91
Epoch:15
LR: 0.001
 * Train Acc 96.290, Loss 0.297
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.950, time 1.03
Epoch:16
LR: 0.001
 * Train Acc 96.340, Loss 0.127
 * robust loss: 0.028 robust error: 0.00000000
 *  Val Acc 96.900, time 0.91
Epoch:17
LR: 0.001
 * Train Acc 96.550, Loss 0.070
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 1.06
Epoch:18
LR: 0.001
 * Train Acc 96.970, Loss 1.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.93
Epoch:19
LR: 0.001
 * Train Acc 97.160, Loss 0.057
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.250, time 1.03
Epoch:20
LR: 0.001
 * Train Acc 97.150, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.93
Epoch:21
LR: 0.001
 * Train Acc 97.320, Loss 0.052
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 97.290, Loss 0.095
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 97.270, Loss 0.046
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.94
Epoch:24
LR: 0.001
 * Train Acc 97.590, Loss 70.781
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 1.09
Epoch:25
LR: 0.001
 * Train Acc 97.630, Loss 0.133
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 0.95
Epoch:26
LR: 0.001
 * Train Acc 97.670, Loss 2.650
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 1.08
Epoch:27
LR: 0.001
 * Train Acc 97.740, Loss 0.052
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 96.850, time 1.01
Epoch:28
LR: 0.001
 * Train Acc 98.080, Loss 0.543
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.200, time 1.01
Epoch:29
LR: 0.001
 * Train Acc 98.130, Loss 0.034
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.000, time 0.72
Epoch:30
LR: 0.001
 * Train Acc 97.750, Loss 1.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 1.00
Epoch:31
LR: 0.001
 * Train Acc 98.110, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 0.95
Epoch:32
LR: 0.001
 * Train Acc 98.160, Loss 0.029
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.550, time 0.92
Epoch:33
LR: 0.001
 * Train Acc 98.010, Loss 0.030
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 1.00
Epoch:34
LR: 0.001
 * Train Acc 98.320, Loss 0.026
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 0.95
Epoch:35
LR: 0.001
 * Train Acc 98.260, Loss 0.027
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.650, time 0.94
Epoch:36
LR: 0.001
 * Train Acc 98.290, Loss 0.026
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.100, time 0.92
Epoch:37
LR: 0.001
 * Train Acc 98.190, Loss 0.050
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 0.92
Epoch:38
LR: 0.001
 * Train Acc 97.840, Loss 9645.296
 * robust loss: 20.842 robust error: 0.01000000
 *  Val Acc 97.450, time 0.87
Epoch:39
LR: 0.001
 * Train Acc 98.290, Loss 0.026
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 0.96
Epoch:40
LR: 0.001
 * Train Acc 98.360, Loss 0.022
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.97
Epoch:41
LR: 0.001
 * Train Acc 98.170, Loss 0.027
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 98.150, time 1.01
Epoch:42
LR: 0.001
 * Train Acc 98.480, Loss 0.039
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.050, time 0.99
Epoch:43
LR: 0.001
 * Train Acc 98.560, Loss 0.020
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.550, time 0.99
Epoch:44
LR: 0.001
 * Train Acc 98.660, Loss 0.019
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.850, time 0.96
Epoch:45
LR: 0.001
 * Train Acc 98.650, Loss 0.608
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 0.96
Epoch:46
LR: 0.001
 * Train Acc 98.610, Loss 0.018
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.300, time 1.00
Epoch:47
LR: 0.001
 * Train Acc 98.620, Loss 100.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 0.95
Epoch:48
LR: 0.001
 * Train Acc 98.450, Loss 1.051
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.200, time 0.96
Epoch:49
LR: 0.001
 * Train Acc 98.730, Loss 12.180
 * robust loss: 0.035 robust error: 0.00000000
 *  Val Acc 97.500, time 1.00
after batch eps: 2.500000000000002, kappa: 0.5
sum: 12.91854476928711 - mean: 0.014952019788324833 - std: 0.005003606900572777
 * min 0.00855604000389576, max: 0.028888529166579247
sum: 288.51519775390625 - mean: 0.03130590170621872 - std: 0.013455663807690144
 * min 0.011737463995814323, max: 0.07482033967971802
sum: 177.6579132080078 - mean: 0.009638559073209763 - std: 0.0030479237902909517
 * min 0.0033478683326393366, max: 0.0219416581094265
sum: 480.42901611328125 - mean: 0.01303247082978487 - std: 0.0031740046106278896
 * min 0.005048248451203108, max: 0.028320487588644028
sum: 2561.111572265625 - mean: 0.034737300127744675 - std: 0.010119779966771603
 * min 0.011978454887866974, max: 0.08343517035245895
sum: 5239.1826171875 - mean: 0.03553048148751259 - std: 0.008283205330371857
 * min 0.013649027794599533, max: 0.09245513379573822
sum: 6204.86181640625 - mean: 0.04207941144704819 - std: 0.008271940052509308
 * min 0.01401782687753439, max: 0.10086125135421753
sum: 111.1537857055664 - mean: 0.00013568576832767576 - std: 7.040940204205981e-07
 * min 0.00011702121264534071, max: 0.0001396798325004056
sum: 5.0 - mean: 0.009765625 - std: 0.005485313478857279
 * min 0.0028138880152255297, max: 0.03621800243854523
eps: tensor([0.1346, 0.2818, 0.0867, 0.1173, 0.3126, 0.3198, 0.3787, 0.4342, 0.4343],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 97.500, time 0.92
 * Lower 1 Val Acc 50.650, time 0.96
 * Upper 1 Val Acc 50.650, time 0.93
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 64.750, Loss 0.641
 * robust loss: 0.012 robust error: 0.01000000
 *  Val Acc 75.700, time 1.00
Epoch:1
LR: 0.001
 * Train Acc 76.090, Loss 0.495
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 79.800, time 1.00
Epoch:2
LR: 0.001
 * Train Acc 79.370, Loss 0.433
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.78
Epoch:3
LR: 0.001
 * Train Acc 80.490, Loss 0.415
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 0.89
Epoch:4
LR: 0.001
 * Train Acc 81.320, Loss 0.397
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.97
Epoch:5
LR: 0.001
 * Train Acc 81.600, Loss 0.386
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 1.02
Epoch:6
LR: 0.001
 * Train Acc 82.240, Loss 0.372
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.97
Epoch:7
LR: 0.001
 * Train Acc 82.050, Loss 0.364
 * robust loss: 0.577 robust error: 0.01000000
 *  Val Acc 80.650, time 0.80
Epoch:8
LR: 0.001
 * Train Acc 82.530, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 1.00
Epoch:9
LR: 0.001
 * Train Acc 82.960, Loss 0.337
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.97
Epoch:10
LR: 0.001
 * Train Acc 83.760, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.89
Epoch:11
LR: 0.001
 * Train Acc 83.600, Loss 0.544
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.67
Epoch:12
LR: 0.001
 * Train Acc 82.670, Loss 0.325
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.85
Epoch:13
LR: 0.001
 * Train Acc 83.870, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.71
Epoch:14
LR: 0.001
 * Train Acc 84.120, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.71
Epoch:15
LR: 0.001
 * Train Acc 84.460, Loss 0.289
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.77
Epoch:16
LR: 0.001
 * Train Acc 84.570, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.92
Epoch:17
LR: 0.001
 * Train Acc 84.360, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.93
Epoch:18
LR: 0.001
 * Train Acc 85.170, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 1.04
Epoch:19
LR: 0.001
 * Train Acc 85.020, Loss 0.263
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 1.10
Epoch:20
LR: 0.001
 * Train Acc 84.760, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.92
Epoch:21
LR: 0.001
 * Train Acc 83.460, Loss 0.271
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.92
Epoch:22
LR: 0.001
 * Train Acc 84.280, Loss 0.258
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 0.96
Epoch:23
LR: 0.001
 * Train Acc 84.930, Loss 0.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 1.03
Epoch:24
LR: 0.001
 * Train Acc 84.790, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 1.00
Epoch:25
LR: 0.001
 * Train Acc 85.070, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.700, time 0.96
Epoch:26
LR: 0.001
 * Train Acc 84.390, Loss 0.238
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 1.05
Epoch:27
LR: 0.001
 * Train Acc 84.440, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.94
Epoch:28
LR: 0.001
 * Train Acc 85.020, Loss 0.223
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.96
Epoch:29
LR: 0.001
 * Train Acc 84.970, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 1.07
Epoch:30
LR: 0.001
 * Train Acc 84.900, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.88
Epoch:31
LR: 0.001
 * Train Acc 85.120, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.93
Epoch:32
LR: 0.001
 * Train Acc 84.660, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 1.08
Epoch:33
LR: 0.001
 * Train Acc 85.080, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 0.99
Epoch:34
LR: 0.001
 * Train Acc 84.690, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.96
Epoch:35
LR: 0.001
 * Train Acc 84.580, Loss 0.611
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 1.04
Epoch:36
LR: 0.001
 * Train Acc 85.000, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.96
Epoch:37
LR: 0.001
 * Train Acc 84.560, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.91
Epoch:38
LR: 0.001
 * Train Acc 84.500, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.99
Epoch:39
LR: 0.001
 * Train Acc 84.860, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.76
Epoch:40
LR: 0.001
 * Train Acc 84.940, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.99
Epoch:41
LR: 0.001
 * Train Acc 84.740, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 0.92
Epoch:42
LR: 0.001
 * Train Acc 84.650, Loss 0.584
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.91
Epoch:43
LR: 0.001
 * Train Acc 83.960, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.400, time 0.83
Epoch:44
LR: 0.001
 * Train Acc 84.160, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.72
Epoch:45
LR: 0.001
 * Train Acc 84.470, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.900, time 0.96
Epoch:46
LR: 0.001
 * Train Acc 84.640, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.93
Epoch:47
LR: 0.001
 * Train Acc 83.950, Loss 0.224
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.96
Epoch:48
LR: 0.001
 * Train Acc 83.980, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.91
Epoch:49
LR: 0.001
 * Train Acc 85.030, Loss 0.176
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.95
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.16898250579834 - mean: 0.004825211130082607 - std: 0.0021831602789461613
 * min 0.0026505824644118547, max: 0.011216261424124241
sum: 110.70170593261719 - mean: 0.012011904269456863 - std: 0.006432157475501299
 * min 0.0034928484819829464, max: 0.036032937467098236
sum: 48.362098693847656 - mean: 0.0026238118298351765 - std: 0.0009922257158905268
 * min 0.0007548400317318738, max: 0.007595473434776068
sum: 128.74075317382812 - mean: 0.0034923164639621973 - std: 0.000960099627263844
 * min 0.0011753239668905735, max: 0.008507615886628628
sum: 765.7326049804688 - mean: 0.010385912843048573 - std: 0.0034281820990145206
 * min 0.0030122685711830854, max: 0.02720928005874157
sum: 1942.82763671875 - mean: 0.013175643049180508 - std: 0.003521091304719448
 * min 0.0043166582472622395, max: 0.03920552507042885
sum: 2351.394287109375 - mean: 0.015946412459015846 - std: 0.0035887081176042557
 * min 0.004419734701514244, max: 0.041617948561906815
sum: 43.237327575683594 - mean: 5.277993841446005e-05 - std: 3.2115545423039293e-07
 * min 4.412606358528137e-05, max: 5.462184708449058e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0022292817011475563
 * min 0.0008861537207849324, max: 0.014994119293987751
eps: tensor([0.0434, 0.1081, 0.0236, 0.0314, 0.0935, 0.1186, 0.1435, 0.1689, 0.1690],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.250, time 0.88
 * Lower 1 Val Acc 86.650, time 0.97
 * Upper 1 Val Acc 86.650, time 1.00
validation split name: 2
 *  Val Acc 82.800, time 0.94
 * Lower 1 Val Acc 71.700, time 0.96
 * Upper 1 Val Acc 71.700, time 0.95
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 79.510, Loss 0.443
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.98
Epoch:1
LR: 0.001
 * Train Acc 83.980, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.93
Epoch:2
LR: 0.001
 * Train Acc 83.640, Loss 0.361
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.95
Epoch:3
LR: 0.001
 * Train Acc 83.890, Loss 0.351
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.98
Epoch:4
LR: 0.001
 * Train Acc 84.440, Loss 0.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 0.96
Epoch:5
LR: 0.001
 * Train Acc 84.320, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.96
Epoch:6
LR: 0.001
 * Train Acc 84.490, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 0.95
Epoch:7
LR: 0.001
 * Train Acc 85.150, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.150, time 0.96
Epoch:8
LR: 0.001
 * Train Acc 84.490, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 1.04
Epoch:9
LR: 0.001
 * Train Acc 84.090, Loss 0.320
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 1.05
Epoch:10
LR: 0.001
 * Train Acc 84.820, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.93
Epoch:11
LR: 0.001
 * Train Acc 84.410, Loss 0.312
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 0.88
Epoch:12
LR: 0.001
 * Train Acc 84.910, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 0.82
Epoch:13
LR: 0.001
 * Train Acc 84.630, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.78
Epoch:14
LR: 0.001
 * Train Acc 84.950, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 1.01
Epoch:15
LR: 0.001
 * Train Acc 85.260, Loss 0.287
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 1.24
Epoch:16
LR: 0.001
 * Train Acc 85.100, Loss 0.294
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 1.06
Epoch:17
LR: 0.001
 * Train Acc 84.650, Loss 0.280
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.400, time 1.05
Epoch:18
LR: 0.001
 * Train Acc 84.370, Loss 0.278
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 1.09
Epoch:19
LR: 0.001
 * Train Acc 84.840, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 1.05
Epoch:20
LR: 0.001
 * Train Acc 84.880, Loss 0.268
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.350, time 1.07
Epoch:21
LR: 0.001
 * Train Acc 84.520, Loss 0.262
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.100, time 1.10
Epoch:22
LR: 0.001
 * Train Acc 84.650, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 1.09
Epoch:23
LR: 0.001
 * Train Acc 84.360, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.350, time 1.07
Epoch:24
LR: 0.001
 * Train Acc 84.530, Loss 0.250
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.150, time 1.18
Epoch:25
LR: 0.001
 * Train Acc 84.280, Loss 0.246
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 1.24
Epoch:26
LR: 0.001
 * Train Acc 84.270, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 1.05
Epoch:27
LR: 0.001
 * Train Acc 83.930, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 1.17
Epoch:28
LR: 0.001
 * Train Acc 84.580, Loss 0.231
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 0.84
Epoch:29
LR: 0.001
 * Train Acc 84.160, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.750, time 0.98
Epoch:30
LR: 0.001
 * Train Acc 84.060, Loss 0.227
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 0.94
Epoch:31
LR: 0.001
 * Train Acc 84.810, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 1.07
Epoch:32
LR: 0.001
 * Train Acc 83.860, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.98
Epoch:33
LR: 0.001
 * Train Acc 83.750, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 1.01
Epoch:34
LR: 0.001
 * Train Acc 83.940, Loss 0.209
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 1.00
Epoch:35
LR: 0.001
 * Train Acc 84.130, Loss 0.203
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 1.02
Epoch:36
LR: 0.001
 * Train Acc 84.440, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 1.12
Epoch:37
LR: 0.001
 * Train Acc 84.950, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 1.00
Epoch:38
LR: 0.001
 * Train Acc 84.080, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.91
Epoch:39
LR: 0.001
 * Train Acc 83.480, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 1.04
Epoch:40
LR: 0.001
 * Train Acc 83.790, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 1.03
Epoch:41
LR: 0.001
 * Train Acc 84.060, Loss 0.183
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 1.05
Epoch:42
LR: 0.001
 * Train Acc 83.510, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 1.06
Epoch:43
LR: 0.001
 * Train Acc 83.360, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.500, time 1.02
Epoch:44
LR: 0.001
 * Train Acc 83.670, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 1.10
Epoch:45
LR: 0.001
 * Train Acc 83.170, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 1.10
Epoch:46
LR: 0.001
 * Train Acc 83.030, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.98
Epoch:47
LR: 0.001
 * Train Acc 82.730, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 1.07
Epoch:48
LR: 0.001
 * Train Acc 83.300, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.70
Epoch:49
LR: 0.001
 * Train Acc 82.710, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 1.02
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.401548147201538 - mean: 0.0016221621772274375 - std: 0.0007949913269840181
 * min 0.0008699288591742516, max: 0.003945370204746723
sum: 36.09610366821289 - mean: 0.0039166780188679695 - std: 0.0022110408172011375
 * min 0.0010697399266064167, max: 0.012271197512745857
sum: 15.15700912475586 - mean: 0.0008223204058595002 - std: 0.0003217196208424866
 * min 0.00023343757493421435, max: 0.002526939148083329
sum: 40.72190475463867 - mean: 0.0011046524159610271 - std: 0.00030928279738873243
 * min 0.00035379923065193, max: 0.0027203690260648727
sum: 235.0686492919922 - mean: 0.0031883225310593843 - std: 0.0010725551983341575
 * min 0.0009096875437535346, max: 0.008907887153327465
sum: 660.205322265625 - mean: 0.004477303940802813 - std: 0.0012130201794207096
 * min 0.0014443877153098583, max: 0.013389826752245426
sum: 801.939697265625 - mean: 0.005438501480966806 - std: 0.0012436709366738796
 * min 0.0014706926885992289, max: 0.014571969397366047
sum: 14.700925827026367 - mean: 1.7945465515367687e-05 - std: 1.0987235299353415e-07
 * min 1.4996019672253169e-05, max: 1.857308961916715e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0007566687418147922
 * min 0.0002895612269639969, max: 0.005064811557531357
eps: tensor([0.0146, 0.0353, 0.0074, 0.0099, 0.0287, 0.0403, 0.0489, 0.0574, 0.0574],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.300, time 0.93
 * Lower 1 Val Acc 88.700, time 0.99
 * Upper 1 Val Acc 88.700, time 1.10
validation split name: 2
 *  Val Acc 75.500, time 1.03
 * Lower 1 Val Acc 76.600, time 0.95
 * Upper 1 Val Acc 76.600, time 1.10
validation split name: 3
 *  Val Acc 78.400, time 0.92
 * Lower 1 Val Acc 77.500, time 0.76
 * Upper 1 Val Acc 77.500, time 1.02
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 76.660, Loss 0.494
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.800, time 1.09
Epoch:1
LR: 0.001
 * Train Acc 79.140, Loss 0.450
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 1.00
Epoch:2
LR: 0.001
 * Train Acc 79.190, Loss 0.444
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 1.08
Epoch:3
LR: 0.001
 * Train Acc 78.800, Loss 0.446
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 1.05
Epoch:4
LR: 0.001
 * Train Acc 78.940, Loss 0.438
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 1.12
Epoch:5
LR: 0.001
 * Train Acc 78.900, Loss 0.435
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 1.00
Epoch:6
LR: 0.001
 * Train Acc 78.970, Loss 0.424
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.97
Epoch:7
LR: 0.001
 * Train Acc 78.680, Loss 0.424
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.150, time 0.91
Epoch:8
LR: 0.001
 * Train Acc 78.830, Loss 0.419
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.350, time 0.72
Epoch:9
LR: 0.001
 * Train Acc 78.650, Loss 0.414
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.450, time 1.08
Epoch:10
LR: 0.001
 * Train Acc 78.070, Loss 0.413
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.150, time 0.82
Epoch:11
LR: 0.001
 * Train Acc 77.480, Loss 0.408
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 1.16
Epoch:12
LR: 0.001
 * Train Acc 78.200, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.93
Epoch:13
LR: 0.001
 * Train Acc 77.730, Loss 0.403
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 0.98
Epoch:14
LR: 0.001
 * Train Acc 77.780, Loss 0.395
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.900, time 1.08
Epoch:15
LR: 0.001
 * Train Acc 77.210, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.600, time 1.08
Epoch:16
LR: 0.001
 * Train Acc 77.490, Loss 0.386
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.550, time 1.02
Epoch:17
LR: 0.001
 * Train Acc 76.510, Loss 0.398
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.850, time 1.16
Epoch:18
LR: 0.001
 * Train Acc 76.980, Loss 0.383
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.750, time 1.04
Epoch:19
LR: 0.001
 * Train Acc 76.390, Loss 0.378
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.96
Epoch:20
LR: 0.001
 * Train Acc 76.060, Loss 0.374
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.750, time 1.22
Epoch:21
LR: 0.001
 * Train Acc 75.820, Loss 0.374
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 0.74
Epoch:22
LR: 0.001
 * Train Acc 75.780, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.500, time 1.09
Epoch:23
LR: 0.001
 * Train Acc 75.600, Loss 0.363
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.950, time 0.96
Epoch:24
LR: 0.001
 * Train Acc 75.270, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 1.01
Epoch:25
LR: 0.001
 * Train Acc 74.960, Loss 0.358
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.350, time 1.07
Epoch:26
LR: 0.001
 * Train Acc 74.620, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.450, time 1.07
Epoch:27
LR: 0.001
 * Train Acc 74.220, Loss 0.349
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.750, time 1.05
Epoch:28
LR: 0.001
 * Train Acc 74.110, Loss 0.345
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.400, time 1.03
Epoch:29
LR: 0.001
 * Train Acc 74.040, Loss 0.343
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.250, time 0.94
Epoch:30
LR: 0.001
 * Train Acc 73.840, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.500, time 1.02
Epoch:31
LR: 0.001
 * Train Acc 73.660, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.400, time 1.00
Epoch:32
LR: 0.001
 * Train Acc 73.240, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.350, time 1.02
Epoch:33
LR: 0.001
 * Train Acc 73.140, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.000, time 0.84
Epoch:34
LR: 0.001
 * Train Acc 72.080, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.550, time 1.05
Epoch:35
LR: 0.001
 * Train Acc 72.190, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.250, time 1.00
Epoch:36
LR: 0.001
 * Train Acc 71.720, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.800, time 0.99
Epoch:37
LR: 0.001
 * Train Acc 71.560, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.200, time 1.02
Epoch:38
LR: 0.001
 * Train Acc 70.840, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.750, time 0.86
Epoch:39
LR: 0.001
 * Train Acc 71.400, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.650, time 1.01
Epoch:40
LR: 0.001
 * Train Acc 70.870, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.950, time 0.98
Epoch:41
LR: 0.001
 * Train Acc 70.040, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.050, time 0.96
Epoch:42
LR: 0.001
 * Train Acc 69.750, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.150, time 0.99
Epoch:43
LR: 0.001
 * Train Acc 69.060, Loss 0.302
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.600, time 1.01
Epoch:44
LR: 0.001
 * Train Acc 69.340, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.200, time 1.10
Epoch:45
LR: 0.001
 * Train Acc 68.940, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.550, time 1.06
Epoch:46
LR: 0.001
 * Train Acc 69.040, Loss 0.307
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.150, time 1.04
Epoch:47
LR: 0.001
 * Train Acc 67.920, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.100, time 1.38
Epoch:48
LR: 0.001
 * Train Acc 67.770, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 70.300, time 1.06
Epoch:49
LR: 0.001
 * Train Acc 66.620, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 69.250, time 1.07
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 0.9727553129196167 - mean: 0.0011258742306381464 - std: 0.0005812838207930326
 * min 0.0005831317976117134, max: 0.0028719757683575153
sum: 24.420665740966797 - mean: 0.0026498117949813604 - std: 0.0015559917083010077
 * min 0.0006821192800998688, max: 0.008713844232261181
sum: 9.931784629821777 - mean: 0.0005388337885960937 - std: 0.0002186533238273114
 * min 0.00014280360483098775, max: 0.001713297446258366
sum: 26.59235191345215 - mean: 0.0007213636999949813 - std: 0.0002055316581390798
 * min 0.00022273049398791045, max: 0.0018353404011577368
sum: 152.2103271484375 - mean: 0.0020644846372306347 - std: 0.0007022326462902129
 * min 0.0005642102914862335, max: 0.006003327667713165
sum: 435.2382507324219 - mean: 0.0029516483191400766 - std: 0.0008083065622486174
 * min 0.0009447011398151517, max: 0.009001939557492733
sum: 532.6534423828125 - mean: 0.0036122873425483704 - std: 0.0008367239497601986
 * min 0.0009622520883567631, max: 0.009849836118519306
sum: 9.856853485107422 - mean: 1.2032292033836711e-05 - std: 7.36956806690614e-08
 * min 1.0054245649371296e-05, max: 1.2453156159608625e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 0.0005083868163637817
 * min 0.00019421831530053169, max: 0.0033991015516221523
eps: tensor([0.0101, 0.0238, 0.0048, 0.0065, 0.0186, 0.0266, 0.0325, 0.0385, 0.0385],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 79.750, time 0.95
 * Lower 1 Val Acc 81.450, time 0.92
 * Upper 1 Val Acc 81.450, time 1.03
validation split name: 2
 *  Val Acc 73.000, time 1.08
 * Lower 1 Val Acc 73.850, time 0.93
 * Upper 1 Val Acc 73.850, time 0.89
validation split name: 3
 *  Val Acc 75.350, time 0.98
 * Lower 1 Val Acc 75.750, time 0.97
 * Upper 1 Val Acc 75.750, time 0.99
validation split name: 4
 *  Val Acc 69.250, time 0.82
 * Lower 1 Val Acc 68.950, time 0.83
 * Upper 1 Val Acc 68.950, time 0.84
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 83.390, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.94
Epoch:1
LR: 0.001
 * Train Acc 84.200, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 1.26
Epoch:2
LR: 0.001
 * Train Acc 84.880, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.96
Epoch:3
LR: 0.001
 * Train Acc 84.660, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.96
Epoch:4
LR: 0.001
 * Train Acc 84.500, Loss 0.332
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 1.12
Epoch:5
LR: 0.001
 * Train Acc 84.350, Loss 0.334
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 1.16
Epoch:6
LR: 0.001
 * Train Acc 84.520, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 1.18
Epoch:7
LR: 0.001
 * Train Acc 84.430, Loss 0.326
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 1.15
Epoch:8
LR: 0.001
 * Train Acc 84.410, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 1.00
Epoch:9
LR: 0.001
 * Train Acc 84.310, Loss 0.316
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 1.23
Epoch:10
LR: 0.001
 * Train Acc 84.700, Loss 0.313
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.600, time 1.12
Epoch:11
LR: 0.001
 * Train Acc 84.150, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 1.07
Epoch:12
LR: 0.001
 * Train Acc 84.570, Loss 0.304
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 1.12
Epoch:13
LR: 0.001
 * Train Acc 84.380, Loss 0.300
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 1.14
Epoch:14
LR: 0.001
 * Train Acc 84.320, Loss 0.299
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.99
Epoch:15
LR: 0.001
 * Train Acc 84.360, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.550, time 1.13
Epoch:16
LR: 0.001
 * Train Acc 83.860, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.97
Epoch:17
LR: 0.001
 * Train Acc 84.520, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.96
Epoch:18
LR: 0.001
 * Train Acc 84.340, Loss 0.278
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 1.14
Epoch:19
LR: 0.001
 * Train Acc 84.470, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 1.00
Epoch:20
LR: 0.001
 * Train Acc 84.010, Loss 0.273
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.93
Epoch:21
LR: 0.001
 * Train Acc 84.080, Loss 0.269
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 1.16
Epoch:22
LR: 0.001
 * Train Acc 83.520, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 1.12
Epoch:23
LR: 0.001
 * Train Acc 83.690, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.89
Epoch:24
LR: 0.001
 * Train Acc 84.050, Loss 0.257
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 82.500, time 1.06
Epoch:25
LR: 0.001
 * Train Acc 83.820, Loss 0.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 1.00
Epoch:26
LR: 0.001
 * Train Acc 83.730, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 1.09
Epoch:27
LR: 0.001
 * Train Acc 83.730, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.99
Epoch:28
LR: 0.001
 * Train Acc 83.700, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 1.06
Epoch:29
LR: 0.001
 * Train Acc 83.550, Loss 0.239
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 0.99
Epoch:30
LR: 0.001
 * Train Acc 83.230, Loss 0.233
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.200, time 0.84
Epoch:31
LR: 0.001
 * Train Acc 83.510, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 1.02
Epoch:32
LR: 0.001
 * Train Acc 82.940, Loss 0.228
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.93
Epoch:33
LR: 0.001
 * Train Acc 83.170, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.500, time 1.00
Epoch:34
LR: 0.001
 * Train Acc 83.200, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 1.00
Epoch:35
LR: 0.001
 * Train Acc 83.360, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 1.03
Epoch:36
LR: 0.001
 * Train Acc 82.810, Loss 0.211
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.500, time 1.10
Epoch:37
LR: 0.001
 * Train Acc 82.900, Loss 0.207
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 1.00
Epoch:38
LR: 0.001
 * Train Acc 82.420, Loss 0.203
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 1.07
Epoch:39
LR: 0.001
 * Train Acc 83.130, Loss 0.198
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 1.02
Epoch:40
LR: 0.001
 * Train Acc 82.950, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.87
Epoch:41
LR: 0.001
 * Train Acc 82.700, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.90
Epoch:42
LR: 0.001
 * Train Acc 82.530, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.89
Epoch:43
LR: 0.001
 * Train Acc 82.450, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.700, time 0.85
Epoch:44
LR: 0.001
 * Train Acc 82.510, Loss 0.199
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 1.00
Epoch:45
LR: 0.001
 * Train Acc 82.090, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.650, time 1.02
Epoch:46
LR: 0.001
 * Train Acc 82.450, Loss 0.198
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 1.09
Epoch:47
LR: 0.001
 * Train Acc 82.280, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.000, time 1.05
Epoch:48
LR: 0.001
 * Train Acc 82.000, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.87
Epoch:49
LR: 0.001
 * Train Acc 82.580, Loss 0.200
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.400, time 1.14
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.4681541919708252 - mean: 0.0005418451619334519 - std: 0.0003239665529690683
 * min 0.00023117894306778908, max: 0.0015450668288394809
sum: 11.145235061645508 - mean: 0.0012093354016542435 - std: 0.0008053428609855473
 * min 0.00024182027846109122, max: 0.004492976237088442
sum: 3.7524847984313965 - mean: 0.00020358532492537051 - std: 9.116745059145615e-05
 * min 4.355109194875695e-05, max: 0.0007118310895748436
sum: 10.074207305908203 - mean: 0.0002732803695835173 - std: 8.152696682373062e-05
 * min 6.990969268372282e-05, max: 0.0007383222109638155
sum: 60.0022087097168 - mean: 0.0008138320408761501 - std: 0.0002848832809831947
 * min 0.00020860326185356826, max: 0.00263234437443316
sum: 219.4823760986328 - mean: 0.0014884601114317775 - std: 0.0004180737887509167
 * min 0.00044995552161708474, max: 0.005075584631413221
sum: 283.3087158203125 - mean: 0.0019213102059438825 - std: 0.00046233899774961174
 * min 0.00047548802103847265, max: 0.0059638721868395805
sum: 5.366483688354492 - mean: 6.550883426825749e-06 - std: 4.0313558713478415e-08
 * min 5.470655651151901e-06, max: 6.780561761843273e-06
sum: 0.20000001788139343 - mean: 0.00039062503492459655 - std: 0.00030920738936401904
 * min 7.830958202248439e-05, max: 0.002079589990898967
eps: tensor([0.0049, 0.0109, 0.0018, 0.0025, 0.0073, 0.0134, 0.0173, 0.0210, 0.0210],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.550, time 1.21
 * Lower 1 Val Acc 88.200, time 1.17
 * Upper 1 Val Acc 88.200, time 1.13
validation split name: 2
 *  Val Acc 75.050, time 1.12
 * Lower 1 Val Acc 74.900, time 1.10
 * Upper 1 Val Acc 74.900, time 1.20
validation split name: 3
 *  Val Acc 67.800, time 0.86
 * Lower 1 Val Acc 67.500, time 1.02
 * Upper 1 Val Acc 67.500, time 1.09
validation split name: 4
 *  Val Acc 65.100, time 1.05
 * Lower 1 Val Acc 64.500, time 1.01
 * Upper 1 Val Acc 64.500, time 0.93
validation split name: 5
 *  Val Acc 81.400, time 0.95
 * Lower 1 Val Acc 81.900, time 0.96
 * Upper 1 Val Acc 81.900, time 1.01
Task 1 average acc: 97.5
Task 2 average acc: 84.525
Task 3 average acc: 80.06666666666668
Task 4 average acc: 74.3375
Task 5 average acc: 75.38
===Summary of experiment repeats: 9 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64 73.59 77.93 76.73 75.4  75.38  0.  ]
mean: 68.18199999999999 std: 22.774495296273855
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
IntervalCNN(
  (input): Conv2dInterval(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (c1): Sequential(
    (0): Conv2dInterval(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c2): Sequential(
    (0): Conv2dInterval(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (c3): Sequential(
    (0): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2dInterval(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): ReLU()
    (4): MaxPool2dInterval(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)
    (5): IntervalDropout()
  )
  (fc1): Sequential(
    (0): LinearInterval(in_features=3200, out_features=256, bias=False)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): LinearInterval(in_features=256, out_features=2, bias=False)
  )
)
#parameter of model: 2507465
Task order: ['1', '2', '3', '4', '5']
====================== 1 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 71.600, Loss 0.548
 * robust loss: 3.299 robust error: 0.03000000
 *  Val Acc 80.300, time 1.12
Epoch:1
LR: 0.001
 * Train Acc 80.730, Loss 0.417
 * robust loss: 0.601 robust error: 0.02000000
 *  Val Acc 85.700, time 1.12
Epoch:2
LR: 0.001
 * Train Acc 85.650, Loss 0.337
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 88.150, time 1.06
Epoch:3
LR: 0.001
 * Train Acc 86.850, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.700, time 1.01
Epoch:4
LR: 0.001
 * Train Acc 88.910, Loss 0.257
 * robust loss: 0.004 robust error: 0.00000000
 *  Val Acc 91.200, time 1.16
Epoch:5
LR: 0.001
 * Train Acc 90.040, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.750, time 1.05
Epoch:6
LR: 0.001
 * Train Acc 91.380, Loss 0.194
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 93.250, time 1.00
Epoch:7
LR: 0.001
 * Train Acc 92.460, Loss 0.184
 * robust loss: 0.055 robust error: 0.00000000
 *  Val Acc 93.500, time 0.96
Epoch:8
LR: 0.001
 * Train Acc 93.890, Loss 0.155
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.250, time 0.99
Epoch:9
LR: 0.001
 * Train Acc 94.940, Loss 0.262
 * robust loss: 0.021 robust error: 0.00000000
 *  Val Acc 96.450, time 0.97
Epoch:10
LR: 0.001
 * Train Acc 95.170, Loss 0.166
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 96.550, time 0.97
Epoch:11
LR: 0.001
 * Train Acc 95.670, Loss 0.102
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 95.850, time 1.17
Epoch:12
LR: 0.001
 * Train Acc 95.770, Loss 0.154
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.700, time 0.97
Epoch:13
LR: 0.001
 * Train Acc 95.850, Loss 0.347
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.250, time 1.09
Epoch:14
LR: 0.001
 * Train Acc 96.500, Loss 0.080
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.550, time 0.94
Epoch:15
LR: 0.001
 * Train Acc 96.500, Loss 0.190
 * robust loss: 0.022 robust error: 0.00000000
 *  Val Acc 97.300, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 96.690, Loss 0.069
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.550, time 1.00
Epoch:17
LR: 0.001
 * Train Acc 97.150, Loss 0.162
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.350, time 0.97
Epoch:18
LR: 0.001
 * Train Acc 97.280, Loss 0.058
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 94.950, time 1.10
Epoch:19
LR: 0.001
 * Train Acc 97.180, Loss 0.065
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.600, time 1.04
Epoch:20
LR: 0.001
 * Train Acc 97.490, Loss 1.008
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.700, time 1.14
Epoch:21
LR: 0.001
 * Train Acc 97.580, Loss 0.048
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.400, time 1.09
Epoch:22
LR: 0.001
 * Train Acc 97.590, Loss 3.006
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.850, time 1.04
Epoch:23
LR: 0.001
 * Train Acc 97.280, Loss 0.675
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.500, time 1.10
Epoch:24
LR: 0.001
 * Train Acc 97.410, Loss 0.049
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.250, time 1.06
Epoch:25
LR: 0.001
 * Train Acc 97.850, Loss 0.041
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.800, time 1.15
Epoch:26
LR: 0.001
 * Train Acc 97.980, Loss 0.078
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.400, time 0.99
Epoch:27
LR: 0.001
 * Train Acc 98.060, Loss 0.035
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.450, time 1.01
Epoch:28
LR: 0.001
 * Train Acc 98.030, Loss 0.034
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 97.600, time 1.04
Epoch:29
LR: 0.001
 * Train Acc 97.920, Loss 0.045
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.050, time 1.05
Epoch:30
LR: 0.001
 * Train Acc 97.970, Loss 52.645
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.500, time 1.03
Epoch:31
LR: 0.001
 * Train Acc 98.120, Loss 0.030
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.800, time 0.99
Epoch:32
LR: 0.001
 * Train Acc 98.290, Loss 0.049
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.850, time 1.05
Epoch:33
LR: 0.001
 * Train Acc 98.270, Loss 0.028
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.950, time 1.06
Epoch:34
LR: 0.001
 * Train Acc 98.250, Loss 0.027
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 96.850, time 1.11
Epoch:35
LR: 0.001
 * Train Acc 98.340, Loss 1.856
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.250, time 0.99
Epoch:36
LR: 0.001
 * Train Acc 98.350, Loss 1.567
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.750, time 0.93
Epoch:37
LR: 0.001
 * Train Acc 98.450, Loss 0.023
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.950, time 0.83
Epoch:38
LR: 0.001
 * Train Acc 98.440, Loss 0.053
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.300, time 0.79
Epoch:39
LR: 0.001
 * Train Acc 98.530, Loss 0.065
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.900, time 1.02
Epoch:40
LR: 0.001
 * Train Acc 98.330, Loss 1.116
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.150, time 1.18
Epoch:41
LR: 0.001
 * Train Acc 98.540, Loss 0.019
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.000, time 0.97
Epoch:42
LR: 0.001
 * Train Acc 98.710, Loss 0.024
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.750, time 1.08
Epoch:43
LR: 0.001
 * Train Acc 98.560, Loss 170.549
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 98.300, time 1.03
Epoch:44
LR: 0.001
 * Train Acc 98.470, Loss 117.956
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.250, time 1.07
Epoch:45
LR: 0.001
 * Train Acc 98.480, Loss 0.022
 * robust loss: 0.014 robust error: 0.00000000
 *  Val Acc 98.300, time 1.08
Epoch:46
LR: 0.001
 * Train Acc 98.580, Loss 3.536
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.100, time 1.15
Epoch:47
LR: 0.001
 * Train Acc 98.510, Loss 0.471
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 97.850, time 0.98
Epoch:48
LR: 0.001
 * Train Acc 98.520, Loss 2.341
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 97.100, time 1.10
Epoch:49
LR: 0.001
 * Train Acc 98.500, Loss 0.964
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 98.300, time 0.95
after batch eps: 2.500000000000002, kappa: 0.5
sum: 13.995895385742188 - mean: 0.016198953613638878 - std: 0.004755652975291014
 * min 0.010068592615425587, max: 0.030234457924962044
sum: 294.62567138671875 - mean: 0.03196893259882927 - std: 0.013874810189008713
 * min 0.011420703493058681, max: 0.07570840418338776
sum: 185.2935028076172 - mean: 0.0100528160110116 - std: 0.0035542501136660576
 * min 0.0037187894340604544, max: 0.022188354283571243
sum: 500.49713134765625 - mean: 0.013576854020357132 - std: 0.004092313814908266
 * min 0.004367793910205364, max: 0.030167147517204285
sum: 2397.10302734375 - mean: 0.03251279145479202 - std: 0.010787743143737316
 * min 0.010684721171855927, max: 0.08409488946199417
sum: 4999.4912109375 - mean: 0.033904969692230225 - std: 0.007572379428893328
 * min 0.013776668347418308, max: 0.0805123820900917
sum: 6139.4384765625 - mean: 0.04163573309779167 - std: 0.007696225773543119
 * min 0.015969276428222656, max: 0.09351177513599396
sum: 112.78721618652344 - mean: 0.00013767970085609704 - std: 9.666213145465008e-07
 * min 0.00010832442785613239, max: 0.00014302782074082643
sum: 5.0 - mean: 0.009765625 - std: 0.004105227999389172
 * min 0.003275447990745306, max: 0.03133528679609299
eps: tensor([0.1458, 0.2877, 0.0905, 0.1222, 0.2926, 0.3051, 0.3747, 0.4406, 0.4408],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 98.300, time 0.98
 * Lower 1 Val Acc 56.800, time 1.03
 * Upper 1 Val Acc 56.800, time 0.98
====================== 2 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 64.770, Loss 0.642
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.200, time 0.97
Epoch:1
LR: 0.001
 * Train Acc 76.060, Loss 0.492
 * robust loss: 0.009 robust error: 0.01000000
 *  Val Acc 76.150, time 1.13
Epoch:2
LR: 0.001
 * Train Acc 78.500, Loss 0.442
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.050, time 1.00
Epoch:3
LR: 0.001
 * Train Acc 80.530, Loss 0.409
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 1.03
Epoch:4
LR: 0.001
 * Train Acc 81.750, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.80
Epoch:5
LR: 0.001
 * Train Acc 82.100, Loss 0.373
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.97
Epoch:6
LR: 0.001
 * Train Acc 82.320, Loss 0.361
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.800, time 0.93
Epoch:7
LR: 0.001
 * Train Acc 82.970, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.78
Epoch:8
LR: 0.001
 * Train Acc 82.810, Loss 0.340
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.97
Epoch:9
LR: 0.001
 * Train Acc 83.480, Loss 0.725
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 1.02
Epoch:10
LR: 0.001
 * Train Acc 83.680, Loss 0.426
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.250, time 0.70
Epoch:11
LR: 0.001
 * Train Acc 83.770, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.650, time 0.96
Epoch:12
LR: 0.001
 * Train Acc 83.850, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 0.97
Epoch:13
LR: 0.001
 * Train Acc 84.900, Loss 0.294
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.99
Epoch:14
LR: 0.001
 * Train Acc 84.470, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.200, time 0.95
Epoch:15
LR: 0.001
 * Train Acc 84.090, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 0.95
Epoch:16
LR: 0.001
 * Train Acc 84.300, Loss 0.286
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.81
Epoch:17
LR: 0.001
 * Train Acc 84.450, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.500, time 0.95
Epoch:18
LR: 0.001
 * Train Acc 84.900, Loss 0.267
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 0.97
Epoch:19
LR: 0.001
 * Train Acc 84.390, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 1.02
Epoch:20
LR: 0.001
 * Train Acc 85.060, Loss 0.260
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 0.95
Epoch:21
LR: 0.001
 * Train Acc 85.110, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.450, time 0.99
Epoch:22
LR: 0.001
 * Train Acc 84.670, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.94
Epoch:23
LR: 0.001
 * Train Acc 84.880, Loss 0.249
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.800, time 0.97
Epoch:24
LR: 0.001
 * Train Acc 84.420, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 0.87
Epoch:25
LR: 0.001
 * Train Acc 85.100, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.97
Epoch:26
LR: 0.001
 * Train Acc 84.750, Loss 0.229
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.96
Epoch:27
LR: 0.001
 * Train Acc 85.150, Loss 0.226
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 85.150, time 1.02
Epoch:28
LR: 0.001
 * Train Acc 85.090, Loss 0.218
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 1.02
Epoch:29
LR: 0.001
 * Train Acc 84.810, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.850, time 1.06
Epoch:30
LR: 0.001
 * Train Acc 84.820, Loss 0.215
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 1.00
Epoch:31
LR: 0.001
 * Train Acc 84.580, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.98
Epoch:32
LR: 0.001
 * Train Acc 84.970, Loss 0.206
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.99
Epoch:33
LR: 0.001
 * Train Acc 84.910, Loss 0.201
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.150, time 0.94
Epoch:34
LR: 0.001
 * Train Acc 85.190, Loss 0.196
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.350, time 0.96
Epoch:35
LR: 0.001
 * Train Acc 84.770, Loss 0.192
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.97
Epoch:36
LR: 0.001
 * Train Acc 85.260, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.89
Epoch:37
LR: 0.001
 * Train Acc 85.100, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.600, time 0.90
Epoch:38
LR: 0.001
 * Train Acc 84.410, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.450, time 0.79
Epoch:39
LR: 0.001
 * Train Acc 85.010, Loss 0.174
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.450, time 1.04
Epoch:40
LR: 0.001
 * Train Acc 84.850, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.650, time 1.13
Epoch:41
LR: 0.001
 * Train Acc 84.680, Loss 0.177
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.600, time 0.76
Epoch:42
LR: 0.001
 * Train Acc 84.920, Loss 0.173
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.100, time 0.97
Epoch:43
LR: 0.001
 * Train Acc 84.840, Loss 0.175
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.86
Epoch:44
LR: 0.001
 * Train Acc 84.400, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.850, time 1.21
Epoch:45
LR: 0.001
 * Train Acc 83.330, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.250, time 0.97
Epoch:46
LR: 0.001
 * Train Acc 84.080, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.050, time 1.10
Epoch:47
LR: 0.001
 * Train Acc 84.140, Loss 0.180
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.400, time 1.08
Epoch:48
LR: 0.001
 * Train Acc 84.380, Loss 0.179
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 1.05
Epoch:49
LR: 0.001
 * Train Acc 84.000, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.98
after batch eps: 0.8999999999999344, kappa: 0.5
sum: 4.77854585647583 - mean: 0.005530724301934242 - std: 0.0021514121908694506
 * min 0.0030079251155257225, max: 0.012286443263292313
sum: 115.45352172851562 - mean: 0.012527508661150932 - std: 0.006469720508903265
 * min 0.0037474974524229765, max: 0.03365422040224075
sum: 55.30216598510742 - mean: 0.0030003346037119627 - std: 0.001186246401630342
 * min 0.0008511861669830978, max: 0.0075787948444485664
sum: 153.16412353515625 - mean: 0.004154842812567949 - std: 0.0013358286814764142
 * min 0.0012246569385752082, max: 0.010508515872061253
sum: 805.5277709960938 - mean: 0.01092567015439272 - std: 0.0038561674300581217
 * min 0.003323977580294013, max: 0.03344416990876198
sum: 1786.6607666015625 - mean: 0.012116569094359875 - std: 0.002895807381719351
 * min 0.0044102449901402, max: 0.0314515121281147
sum: 2235.2421875 - mean: 0.015158706344664097 - std: 0.0029945746064186096
 * min 0.0053668939508497715, max: 0.035627059638500214
sum: 42.135963439941406 - mean: 5.143550151842646e-05 - std: 3.7867565083615773e-07
 * min 4.046716640004888e-05, max: 5.3435727750184014e-05
sum: 1.7999999523162842 - mean: 0.0035156249068677425 - std: 0.0015794088831171393
 * min 0.00109888706356287, max: 0.012049701064825058
eps: tensor([0.0498, 0.1127, 0.0270, 0.0374, 0.0983, 0.1090, 0.1364, 0.1646, 0.1647],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 89.000, time 0.97
 * Lower 1 Val Acc 76.750, time 0.93
 * Upper 1 Val Acc 76.750, time 1.02
validation split name: 2
 *  Val Acc 83.750, time 1.03
 * Lower 1 Val Acc 64.850, time 1.06
 * Upper 1 Val Acc 64.850, time 0.97
====================== 3 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 80.150, Loss 0.438
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.000, time 1.05
Epoch:1
LR: 0.001
 * Train Acc 83.880, Loss 0.371
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.700, time 0.80
Epoch:2
LR: 0.001
 * Train Acc 84.910, Loss 0.349
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 1.02
Epoch:3
LR: 0.001
 * Train Acc 84.340, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.900, time 0.99
Epoch:4
LR: 0.001
 * Train Acc 84.400, Loss 0.337
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 0.96
Epoch:5
LR: 0.001
 * Train Acc 84.920, Loss 0.335
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 0.79
Epoch:6
LR: 0.001
 * Train Acc 85.650, Loss 0.319
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.600, time 1.07
Epoch:7
LR: 0.001
 * Train Acc 84.930, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.100, time 0.96
Epoch:8
LR: 0.001
 * Train Acc 84.710, Loss 0.321
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 87.000, time 1.08
Epoch:9
LR: 0.001
 * Train Acc 85.200, Loss 0.308
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 1.00
Epoch:10
LR: 0.001
 * Train Acc 84.970, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 1.00
Epoch:11
LR: 0.001
 * Train Acc 85.520, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 1.00
Epoch:12
LR: 0.001
 * Train Acc 85.610, Loss 0.292
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.350, time 1.05
Epoch:13
LR: 0.001
 * Train Acc 85.690, Loss 0.283
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.300, time 0.88
Epoch:14
LR: 0.001
 * Train Acc 85.240, Loss 0.288
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.550, time 1.11
Epoch:15
LR: 0.001
 * Train Acc 85.920, Loss 0.279
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.950, time 1.01
Epoch:16
LR: 0.001
 * Train Acc 85.530, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 1.13
Epoch:17
LR: 0.001
 * Train Acc 85.670, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.200, time 1.13
Epoch:18
LR: 0.001
 * Train Acc 85.030, Loss 0.269
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.250, time 1.04
Epoch:19
LR: 0.001
 * Train Acc 85.150, Loss 0.265
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.400, time 0.83
Epoch:20
LR: 0.001
 * Train Acc 85.630, Loss 0.253
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.900, time 1.12
Epoch:21
LR: 0.001
 * Train Acc 85.190, Loss 0.252
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 1.03
Epoch:22
LR: 0.001
 * Train Acc 84.950, Loss 0.254
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 1.00
Epoch:23
LR: 0.001
 * Train Acc 85.500, Loss 0.243
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 1.03
Epoch:24
LR: 0.001
 * Train Acc 85.030, Loss 0.242
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.550, time 0.99
Epoch:25
LR: 0.001
 * Train Acc 85.070, Loss 0.241
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 1.02
Epoch:26
LR: 0.001
 * Train Acc 85.270, Loss 0.236
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.800, time 1.05
Epoch:27
LR: 0.001
 * Train Acc 85.090, Loss 0.228
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.450, time 1.03
Epoch:28
LR: 0.001
 * Train Acc 85.380, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 1.09
Epoch:29
LR: 0.001
 * Train Acc 85.060, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.750, time 1.03
Epoch:30
LR: 0.001
 * Train Acc 85.060, Loss 0.638
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.950, time 0.98
Epoch:31
LR: 0.001
 * Train Acc 85.170, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 0.87
Epoch:32
LR: 0.001
 * Train Acc 85.550, Loss 0.395
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.200, time 0.74
Epoch:33
LR: 0.001
 * Train Acc 84.730, Loss 0.208
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.250, time 1.02
Epoch:34
LR: 0.001
 * Train Acc 85.010, Loss 0.204
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 1.24
Epoch:35
LR: 0.001
 * Train Acc 84.550, Loss 0.202
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.050, time 1.09
Epoch:36
LR: 0.001
 * Train Acc 84.750, Loss 0.197
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 86.050, time 1.12
Epoch:37
LR: 0.001
 * Train Acc 84.730, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.900, time 1.17
Epoch:38
LR: 0.001
 * Train Acc 84.670, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.300, time 1.00
Epoch:39
LR: 0.001
 * Train Acc 84.140, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.850, time 0.98
Epoch:40
LR: 0.001
 * Train Acc 83.980, Loss 0.182
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 86.150, time 1.16
Epoch:41
LR: 0.001
 * Train Acc 84.410, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.90
Epoch:42
LR: 0.001
 * Train Acc 83.960, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.300, time 1.10
Epoch:43
LR: 0.001
 * Train Acc 83.700, Loss 0.191
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 1.00
Epoch:44
LR: 0.001
 * Train Acc 84.460, Loss 0.185
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.200, time 1.20
Epoch:45
LR: 0.001
 * Train Acc 84.610, Loss 0.182
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.500, time 1.07
Epoch:46
LR: 0.001
 * Train Acc 84.270, Loss 0.181
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.100, time 1.15
Epoch:47
LR: 0.001
 * Train Acc 83.960, Loss 0.188
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.99
Epoch:48
LR: 0.001
 * Train Acc 83.940, Loss 0.187
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 84.300, time 1.00
Epoch:49
LR: 0.001
 * Train Acc 84.300, Loss 0.186
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 85.000, time 1.03
after batch eps: 0.3000000000000167, kappa: 0.5
sum: 1.6907594203948975 - mean: 0.0019568975549191236 - std: 0.0008974045049399137
 * min 0.0010412982665002346, max: 0.004820797126740217
sum: 38.88325881958008 - mean: 0.00421910360455513 - std: 0.002408218802884221
 * min 0.0011203738395124674, max: 0.012818872928619385
sum: 17.334611892700195 - mean: 0.0009404628654010594 - std: 0.0003976298321504146
 * min 0.00023964107094798237, max: 0.002450989093631506
sum: 47.52193832397461 - mean: 0.0012891150545328856 - std: 0.0004267987096682191
 * min 0.0003528311208356172, max: 0.0033247536048293114
sum: 253.24671936035156 - mean: 0.0034348785411566496 - std: 0.0012372133787721395
 * min 0.0009678936330601573, max: 0.01100813690572977
sum: 584.66845703125 - mean: 0.003965036943554878 - std: 0.0009606486419215798
 * min 0.001428638817742467, max: 0.010594222694635391
sum: 739.0038452148438 - mean: 0.005011690780520439 - std: 0.0010054853046312928
 * min 0.0017575311940163374, max: 0.01183643564581871
sum: 14.414215087890625 - mean: 1.7595477402210236e-05 - std: 1.302585843632187e-07
 * min 1.3766187294095289e-05, max: 1.8290189473191276e-05
sum: 0.6000000238418579 - mean: 0.0011718750465661287 - std: 0.0005271901609376073
 * min 0.000368179171346128, max: 0.0040152911096811295
eps: tensor([0.0176, 0.0380, 0.0085, 0.0116, 0.0309, 0.0357, 0.0451, 0.0563, 0.0563],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 86.200, time 1.03
 * Lower 1 Val Acc 84.550, time 1.08
 * Upper 1 Val Acc 84.550, time 1.04
validation split name: 2
 *  Val Acc 77.450, time 1.07
 * Lower 1 Val Acc 74.000, time 0.96
 * Upper 1 Val Acc 74.000, time 1.11
validation split name: 3
 *  Val Acc 85.000, time 0.71
 * Lower 1 Val Acc 81.850, time 0.72
 * Upper 1 Val Acc 81.850, time 0.74
====================== 4 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 76.780, Loss 0.490
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.300, time 1.10
Epoch:1
LR: 0.001
 * Train Acc 78.750, Loss 0.450
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.000, time 1.01
Epoch:2
LR: 0.001
 * Train Acc 79.310, Loss 0.435
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.84
Epoch:3
LR: 0.001
 * Train Acc 79.250, Loss 0.437
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 1.09
Epoch:4
LR: 0.001
 * Train Acc 79.170, Loss 0.428
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 1.05
Epoch:5
LR: 0.001
 * Train Acc 78.990, Loss 0.424
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.250, time 0.89
Epoch:6
LR: 0.001
 * Train Acc 78.890, Loss 0.424
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.95
Epoch:7
LR: 0.001
 * Train Acc 79.140, Loss 0.417
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 1.05
Epoch:8
LR: 0.001
 * Train Acc 78.570, Loss 0.420
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 1.16
Epoch:9
LR: 0.001
 * Train Acc 78.030, Loss 0.411
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.850, time 0.94
Epoch:10
LR: 0.001
 * Train Acc 78.110, Loss 0.411
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.97
Epoch:11
LR: 0.001
 * Train Acc 77.500, Loss 0.413
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 1.05
Epoch:12
LR: 0.001
 * Train Acc 77.430, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.650, time 1.01
Epoch:13
LR: 0.001
 * Train Acc 77.610, Loss 0.402
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 1.10
Epoch:14
LR: 0.001
 * Train Acc 77.440, Loss 0.393
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.800, time 1.10
Epoch:15
LR: 0.001
 * Train Acc 77.650, Loss 0.389
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.100, time 1.03
Epoch:16
LR: 0.001
 * Train Acc 77.440, Loss 0.388
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.000, time 1.02
Epoch:17
LR: 0.001
 * Train Acc 76.820, Loss 0.382
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.400, time 1.09
Epoch:18
LR: 0.001
 * Train Acc 77.160, Loss 0.377
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.550, time 0.96
Epoch:19
LR: 0.001
 * Train Acc 76.430, Loss 0.376
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.300, time 0.94
Epoch:20
LR: 0.001
 * Train Acc 76.320, Loss 0.372
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.400, time 0.95
Epoch:21
LR: 0.001
 * Train Acc 75.770, Loss 0.366
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.700, time 1.00
Epoch:22
LR: 0.001
 * Train Acc 75.730, Loss 0.364
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.150, time 1.11
Epoch:23
LR: 0.001
 * Train Acc 75.800, Loss 0.362
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 78.350, time 1.11
Epoch:24
LR: 0.001
 * Train Acc 75.200, Loss 0.361
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.850, time 0.91
Epoch:25
LR: 0.001
 * Train Acc 75.150, Loss 0.356
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 77.400, time 0.86
Epoch:26
LR: 0.001
 * Train Acc 75.300, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.400, time 0.78
Epoch:27
LR: 0.001
 * Train Acc 74.130, Loss 0.350
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.000, time 0.85
Epoch:28
LR: 0.001
 * Train Acc 73.900, Loss 0.343
 * robust loss: 0.007 robust error: 0.00000000
 *  Val Acc 75.750, time 0.84
Epoch:29
LR: 0.001
 * Train Acc 73.340, Loss 0.338
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 76.600, time 0.81
Epoch:30
LR: 0.001
 * Train Acc 74.340, Loss 0.331
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.400, time 0.73
Epoch:31
LR: 0.001
 * Train Acc 73.410, Loss 0.329
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.100, time 0.69
Epoch:32
LR: 0.001
 * Train Acc 72.940, Loss 0.327
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.700, time 0.75
Epoch:33
LR: 0.001
 * Train Acc 72.940, Loss 0.323
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.800, time 0.73
Epoch:34
LR: 0.001
 * Train Acc 72.360, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 75.050, time 0.71
Epoch:35
LR: 0.001
 * Train Acc 72.810, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.400, time 0.72
Epoch:36
LR: 0.001
 * Train Acc 72.320, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.950, time 0.69
Epoch:37
LR: 0.001
 * Train Acc 71.280, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 74.250, time 0.68
Epoch:38
LR: 0.001
 * Train Acc 71.070, Loss 0.303
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.750, time 0.72
Epoch:39
LR: 0.001
 * Train Acc 70.800, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.450, time 0.72
Epoch:40
LR: 0.001
 * Train Acc 70.640, Loss 0.293
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.650, time 0.72
Epoch:41
LR: 0.001
 * Train Acc 70.370, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.300, time 0.75
Epoch:42
LR: 0.001
 * Train Acc 70.380, Loss 0.296
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 73.450, time 0.74
Epoch:43
LR: 0.001
 * Train Acc 69.640, Loss 0.298
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.900, time 0.70
Epoch:44
LR: 0.001
 * Train Acc 69.610, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.500, time 0.71
Epoch:45
LR: 0.001
 * Train Acc 69.400, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 72.150, time 0.70
Epoch:46
LR: 0.001
 * Train Acc 68.980, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.400, time 0.73
Epoch:47
LR: 0.001
 * Train Acc 67.910, Loss 0.310
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.800, time 0.70
Epoch:48
LR: 0.001
 * Train Acc 69.150, Loss 0.306
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.400, time 0.74
Epoch:49
LR: 0.001
 * Train Acc 68.250, Loss 0.311
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 71.550, time 0.75
after batch eps: 0.20000000000001855, kappa: 0.5
sum: 1.1079535484313965 - mean: 0.0012823536526411772 - std: 0.0006618001498281956
 * min 0.0006162088247947395, max: 0.0034326075110584497
sum: 24.626216888427734 - mean: 0.0026721155736595392 - std: 0.0016417764127254486
 * min 0.0006446393090300262, max: 0.008739870972931385
sum: 10.255158424377441 - mean: 0.0005563779850490391 - std: 0.0002448958402965218
 * min 0.00012916816922370344, max: 0.001499778707511723
sum: 28.498361587524414 - mean: 0.0007730675279162824 - std: 0.0002597261918708682
 * min 0.0002017337246797979, max: 0.002077592071145773
sum: 161.15658569335938 - mean: 0.002185826189815998 - std: 0.0007979162037372589
 * min 0.0005968870827928185, max: 0.007491707801818848
sum: 398.91778564453125 - mean: 0.0027053344529122114 - std: 0.000664832885377109
 * min 0.0009522594627924263, max: 0.007533949799835682
sum: 507.82611083984375 - mean: 0.003443916328251362 - std: 0.0007050597341731191
 * min 0.0011731432750821114, max: 0.008339062333106995
sum: 9.907827377319336 - mean: 1.2094516023353208e-05 - std: 8.982672028423622e-08
 * min 9.45169995247852e-06, max: 1.2572260857268702e-05
sum: 0.4000000059604645 - mean: 0.0007812500116415322 - std: 0.0003748666204046458
 * min 0.00023063004482537508, max: 0.0028412973042577505
eps: tensor([0.0115, 0.0240, 0.0050, 0.0070, 0.0197, 0.0243, 0.0310, 0.0387, 0.0387],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 81.650, time 0.73
 * Lower 1 Val Acc 79.350, time 0.69
 * Upper 1 Val Acc 79.350, time 0.75
validation split name: 2
 *  Val Acc 76.550, time 0.68
 * Lower 1 Val Acc 74.650, time 0.70
 * Upper 1 Val Acc 74.650, time 0.72
validation split name: 3
 *  Val Acc 81.900, time 0.72
 * Lower 1 Val Acc 81.850, time 0.71
 * Upper 1 Val Acc 81.850, time 0.72
validation split name: 4
 *  Val Acc 71.550, time 0.68
 * Lower 1 Val Acc 69.400, time 0.72
 * Upper 1 Val Acc 69.400, time 0.70
====================== 5 =======================
before batch eps: 0, kappa: 1
Epoch:0
LR: 0.001
 * Train Acc 81.750, Loss 0.404
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.750, time 0.68
Epoch:1
LR: 0.001
 * Train Acc 82.950, Loss 0.379
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.70
Epoch:2
LR: 0.001
 * Train Acc 83.450, Loss 0.367
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.69
Epoch:3
LR: 0.001
 * Train Acc 83.350, Loss 0.368
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.67
Epoch:4
LR: 0.001
 * Train Acc 82.830, Loss 0.366
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.000, time 0.73
Epoch:5
LR: 0.001
 * Train Acc 83.380, Loss 0.357
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.800, time 0.73
Epoch:6
LR: 0.001
 * Train Acc 83.160, Loss 0.353
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.71
Epoch:7
LR: 0.001
 * Train Acc 82.700, Loss 0.354
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.71
Epoch:8
LR: 0.001
 * Train Acc 82.340, Loss 0.352
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.750, time 0.70
Epoch:9
LR: 0.001
 * Train Acc 82.570, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.150, time 0.68
Epoch:10
LR: 0.001
 * Train Acc 83.020, Loss 0.339
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.71
Epoch:11
LR: 0.001
 * Train Acc 82.250, Loss 0.344
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.950, time 0.68
Epoch:12
LR: 0.001
 * Train Acc 82.290, Loss 0.335
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.72
Epoch:13
LR: 0.001
 * Train Acc 83.100, Loss 0.322
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 83.250, time 0.70
Epoch:14
LR: 0.001
 * Train Acc 82.470, Loss 0.324
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.850, time 0.70
Epoch:15
LR: 0.001
 * Train Acc 82.140, Loss 0.318
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.950, time 0.71
Epoch:16
LR: 0.001
 * Train Acc 82.660, Loss 0.314
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.350, time 0.70
Epoch:17
LR: 0.001
 * Train Acc 82.370, Loss 0.309
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.550, time 0.68
Epoch:18
LR: 0.001
 * Train Acc 82.720, Loss 0.305
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.900, time 0.68
Epoch:19
LR: 0.001
 * Train Acc 82.590, Loss 0.301
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.150, time 0.68
Epoch:20
LR: 0.001
 * Train Acc 82.490, Loss 0.295
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.850, time 0.67
Epoch:21
LR: 0.001
 * Train Acc 82.580, Loss 0.291
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.350, time 0.76
Epoch:22
LR: 0.001
 * Train Acc 81.900, Loss 0.294
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.69
Epoch:23
LR: 0.001
 * Train Acc 82.070, Loss 0.284
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.70
Epoch:24
LR: 0.001
 * Train Acc 82.070, Loss 0.281
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.550, time 0.76
Epoch:25
LR: 0.001
 * Train Acc 82.070, Loss 0.277
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.950, time 0.74
Epoch:26
LR: 0.001
 * Train Acc 81.550, Loss 0.275
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.700, time 0.71
Epoch:27
LR: 0.001
 * Train Acc 81.820, Loss 0.270
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.300, time 0.71
Epoch:28
LR: 0.001
 * Train Acc 81.270, Loss 0.266
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.69
Epoch:29
LR: 0.001
 * Train Acc 81.630, Loss 0.258
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.050, time 0.74
Epoch:30
LR: 0.001
 * Train Acc 81.220, Loss 0.259
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.650, time 0.71
Epoch:31
LR: 0.001
 * Train Acc 81.180, Loss 0.255
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.450, time 0.70
Epoch:32
LR: 0.001
 * Train Acc 80.800, Loss 0.247
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.650, time 0.71
Epoch:33
LR: 0.001
 * Train Acc 80.740, Loss 0.245
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 82.200, time 0.72
Epoch:34
LR: 0.001
 * Train Acc 80.790, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.700, time 0.73
Epoch:35
LR: 0.001
 * Train Acc 81.140, Loss 0.237
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.600, time 0.91
Epoch:36
LR: 0.001
 * Train Acc 81.150, Loss 0.230
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.850, time 0.71
Epoch:37
LR: 0.001
 * Train Acc 81.190, Loss 0.225
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.700, time 0.72
Epoch:38
LR: 0.001
 * Train Acc 80.880, Loss 0.221
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.77
Epoch:39
LR: 0.001
 * Train Acc 80.530, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 81.100, time 0.76
Epoch:40
LR: 0.001
 * Train Acc 80.670, Loss 0.212
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.950, time 0.77
Epoch:41
LR: 0.001
 * Train Acc 80.600, Loss 0.213
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.77
Epoch:42
LR: 0.001
 * Train Acc 80.470, Loss 0.216
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.400, time 0.75
Epoch:43
LR: 0.001
 * Train Acc 80.440, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.500, time 0.79
Epoch:44
LR: 0.001
 * Train Acc 79.960, Loss 0.219
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.600, time 0.73
Epoch:45
LR: 0.001
 * Train Acc 80.150, Loss 0.217
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.050, time 0.77
Epoch:46
LR: 0.001
 * Train Acc 79.900, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.550, time 0.88
Epoch:47
LR: 0.001
 * Train Acc 79.890, Loss 0.220
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 79.000, time 0.74
Epoch:48
LR: 0.001
 * Train Acc 79.770, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.650, time 0.74
Epoch:49
LR: 0.001
 * Train Acc 79.690, Loss 0.222
 * robust loss: 0.000 robust error: 0.00000000
 *  Val Acc 80.200, time 0.78
after batch eps: 0.10000000000000928, kappa: 0.5
sum: 0.5554006099700928 - mean: 0.0006428248016163707 - std: 0.0003326713340356946
 * min 0.00030851762858219445, max: 0.0017243176698684692
sum: 12.33902359008789 - mean: 0.0013388697989284992 - std: 0.0008247360237874091
 * min 0.00032185137388296425, max: 0.004382431972771883
sum: 5.134726524353027 - mean: 0.0002785767428576946 - std: 0.00012282657553441823
 * min 6.453079549828544e-05, max: 0.0007513230084441602
sum: 14.265294075012207 - mean: 0.0003869708743877709 - std: 0.00013005881919525564
 * min 0.00010097833728650585, max: 0.0010400559986010194
sum: 79.93159484863281 - mean: 0.0010841416660696268 - std: 0.0003958029847126454
 * min 0.0002960482961498201, max: 0.003716327017173171
sum: 199.50418090820312 - mean: 0.0013529743300750852 - std: 0.00033250570413656533
 * min 0.0004762308089993894, max: 0.0037680636160075665
sum: 253.99517822265625 - mean: 0.001722515095025301 - std: 0.0003526484069880098
 * min 0.0005867302534170449, max: 0.004171089734882116
sum: 4.95693302154541 - mean: 6.050943284208188e-06 - std: 4.49407444591543e-08
 * min 4.728729891212424e-06, max: 6.28996167506557e-06
sum: 0.20000001788139343 - mean: 0.00039062503492459655 - std: 0.00018743165128398687
 * min 0.00011531623749760911, max: 0.0014206317719072104
eps: tensor([0.0058, 0.0120, 0.0025, 0.0035, 0.0098, 0.0122, 0.0155, 0.0194, 0.0194],
       device='cuda:0', grad_fn=<DivBackward0>)
validation split name: 1
 *  Val Acc 87.450, time 0.76
 * Lower 1 Val Acc 87.100, time 0.74
 * Upper 1 Val Acc 87.100, time 0.75
validation split name: 2
 *  Val Acc 77.200, time 0.75
 * Lower 1 Val Acc 76.800, time 0.71
 * Upper 1 Val Acc 76.800, time 0.75
validation split name: 3
 *  Val Acc 74.000, time 0.75
 * Lower 1 Val Acc 74.500, time 0.75
 * Upper 1 Val Acc 74.500, time 0.73
validation split name: 4
 *  Val Acc 66.500, time 0.73
 * Lower 1 Val Acc 67.150, time 0.74
 * Upper 1 Val Acc 67.150, time 0.78
validation split name: 5
 *  Val Acc 80.200, time 0.77
 * Lower 1 Val Acc 79.400, time 0.76
 * Upper 1 Val Acc 79.400, time 0.74
Task 1 average acc: 98.3
Task 2 average acc: 86.375
Task 3 average acc: 82.88333333333334
Task 4 average acc: 77.9125
Task 5 average acc: 77.07
===Summary of experiment repeats: 10 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [76.66 72.85 76.64 76.64 73.59 77.93 76.73 75.4  75.38 77.07]
mean: 75.88899999999998 std: 1.5168813401185985
reg_coef: 0.0 mean: 75.88899999999998 std: 1.5168813401185985
* kappa decrease from 1 to 0.5 in [40.0, 40.0, 40.0, 40.0, 40.0] epoch
* eps increase by [2.5, 0.9, 0.3, 0.2, 0.1] every [50.0, 50.0, 50.0, 50.0, 50.0] epoch
* maximal eps: [0.0, 0.0, 0.0, 0.0, 0.0]
* tasks were trained [50, 50, 50, 50, 50] epoch with clipping
